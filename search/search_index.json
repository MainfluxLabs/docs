{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Overview # About # What is Mainflux? # Mainflux is modern, scalable, secure open source and patent-free IoT cloud platform written in Go. It accepts user and thing connections over various network protocols (i.e. HTTP, MQTT, WebSocket, CoAP), thus making a seamless bridge between them. It is used as the IoT middleware for building complex IoT solutions. Features # Protocol bridging (i.e. HTTP, MQTT, WebSocket, CoAP) Device management and provisioning Fine-grained access control Platform logging and instrumentation support Container-based deployment using Docker Contributing to Mainflux # Thank you for your interest in Mainflux and the desire to contribute! Take a look at our open issues . The good-first-issue label is specifically for issues that are great for getting started. Checkout the contribution guide to learn more about our style and conventions. Make your changes compatible to our workflow. License # Apache-2.0","title":"Overview"},{"location":"#overview","text":"","title":"Overview"},{"location":"#about","text":"","title":"About"},{"location":"#what-is-mainflux","text":"Mainflux is modern, scalable, secure open source and patent-free IoT cloud platform written in Go. It accepts user and thing connections over various network protocols (i.e. HTTP, MQTT, WebSocket, CoAP), thus making a seamless bridge between them. It is used as the IoT middleware for building complex IoT solutions.","title":"What is Mainflux?"},{"location":"#features","text":"Protocol bridging (i.e. HTTP, MQTT, WebSocket, CoAP) Device management and provisioning Fine-grained access control Platform logging and instrumentation support Container-based deployment using Docker","title":"Features"},{"location":"#contributing-to-mainflux","text":"Thank you for your interest in Mainflux and the desire to contribute! Take a look at our open issues . The good-first-issue label is specifically for issues that are great for getting started. Checkout the contribution guide to learn more about our style and conventions. Make your changes compatible to our workflow.","title":"Contributing to Mainflux"},{"location":"#license","text":"Apache-2.0","title":"License"},{"location":"api/","text":"API # Reference # API reference in the Swagger UI can be found at: https://api.mainflux.io Users # Create User # To start working with the Mainflux system, you need to create a user account. Must-have: e-mail and password (password must contain at least 8 characters) curl -s -S -i -X POST -H \"Content-Type: application/json\" http://localhost/users -d '{\"email\":\"<user_email>\", \"password\":\"<user_password>\"}' Response: HTTP/1.1 201 Created Server: nginx/1.16.0 Date: Wed, 10 Mar 2021 15:06:45 GMT Content-Type: application/json Content-Length: 0 Connection: keep-alive Location: /users/d782b42b-e317-4cd7-9dd0-4e2ea0f349c8 Strict-Transport-Security: max-age=63072000; includeSubdomains X-Frame-Options: DENY X-Content-Type-Options: nosniff Access-Control-Allow-Origin: * Access-Control-Allow-Methods: * Access-Control-Allow-Headers: * Create Token # To log in to the Mainflux system, you need to create a user_token . Must-have: registered e-mail and password curl -s -S -i -X POST -H \"Content-Type: application/json\" http://localhost/tokens -d '{\"email\":\"<user_email>\", \"password\":\"<user_password>\"}' Response: HTTP/1.1 201 Created Server: nginx/1.16.0 Date: Wed, 10 Mar 2021 15:07:18 GMT Content-Type: application/json Content-Length: 281 Connection: keep-alive Strict-Transport-Security: max-age=63072000; includeSubdomains X-Frame-Options: DENY X-Content-Type-Options: nosniff Access-Control-Allow-Origin: * Access-Control-Allow-Methods: * Access-Control-Allow-Headers: * {\"token\":\"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJleHAiOjE2MTU0MjQ4MzgsImlhdCI6MTYxNTM4ODgzOCwiaXNzIjoibWFpbmZsdXguYXV0aCIsInN1YiI6InRlc3RAZW1haWwuY29tIiwiaXNzdWVyX2lkIjoiZDc4MmI0MmItZTMxNy00Y2Q3LTlkZDAtNGUyZWEwZjM0OWM4IiwidHlwZSI6MH0.TAQxV6TImKw06RsK0J11rOHiWPvexEOA4BNZnhLhtxs\"} Get User # You can always check the user entity that is logged in by entering the user ID and user_token . Must-have: user_id and user_token curl -s -S -i -X GET -H \"Authorization: Bearer <user_token>\" http://localhost/users/<user_id> Response: HTTP/1.1 200 OK Server: nginx/1.16.0 Date: Wed, 10 Mar 2021 15:09:47 GMT Content-Type: application/json Content-Length: 85 Connection: keep-alive Strict-Transport-Security: max-age=63072000; includeSubdomains X-Frame-Options: DENY X-Content-Type-Options: nosniff Access-Control-Allow-Origin: * Access-Control-Allow-Methods: * Access-Control-Allow-Headers: * {\"id\":\"d782b42b-e317-4cd7-9dd0-4e2ea0f349c8\",\"email\":\"test@email.com\"} Get All Users # You can get all users in the database by calling the this function Must-have: user_token curl -s -S -i -X GET -H \"Authorization: Bearer <user_token>\" http://localhost/users Response: HTTP/1.1 200 OK Server: nginx/1.16.0 Date: Wed, 10 Mar 2021 15:11:28 GMT Content-Type: application/json Content-Length: 217 Connection: keep-alive Strict-Transport-Security: max-age=63072000; includeSubdomains X-Frame-Options: DENY X-Content-Type-Options: nosniff Access-Control-Allow-Origin: * Access-Control-Allow-Methods: * Access-Control-Allow-Headers: * {\"total\":2,\"offset\":0,\"limit\":10,\"Users\":[{\"id\":\"4bf4a13a-e9c3-4207-aa11-fe569986c301\",\"email\":\"admin@example.com\"},{\"id\":\"d782b42b-e317-4cd7-9dd0-4e2ea0f349c8\",\"email\":\"test@email.com\"}]} Update User # Updating user's metadata Must-have: user_token curl -s -S -i -X PUT -H \"Content-Type: application/json\" -H \"Authorization: Bearer <user_token>\" http://localhost/users -d '{\"metadata\":{\"foo\":\"bar\"}}' Response: HTTP/1.1 200 OK Server: nginx/1.16.0 Date: Wed, 10 Mar 2021 15:15:31 GMT Content-Type: application/json Content-Length: 0 Connection: keep-alive Strict-Transport-Security: max-age=63072000; includeSubdomains X-Frame-Options: DENY X-Content-Type-Options: nosniff Access-Control-Allow-Origin: * Access-Control-Allow-Methods: * Access-Control-Allow-Headers: * Change Password # Changing the user password can be done by calling the update password function Must-have: user_token , old_password and password ( new_password ) curl -s -S -i -X PATCH -H \"Content-Type: application/json\" -H \"Authorization: Bearer <user_token>\" http://localhost/password -d '{\"old_password\":\"<old_password>\", \"password\":\"<new_password>\"}' Response: HTTP/1.1 201 Created Server: nginx/1.16.0 Date: Wed, 10 Mar 2021 15:17:36 GMT Content-Type: application/json Content-Length: 11 Connection: keep-alive Strict-Transport-Security: max-age=63072000; includeSubdomains X-Frame-Options: DENY X-Content-Type-Options: nosniff Access-Control-Allow-Origin: * Access-Control-Allow-Methods: * Access-Control-Allow-Headers: * Things # Create Thing # To create a thing, you need the thing and a user_token Must-have: user_token curl -s -S -i -X POST -H \"Content-Type: application/json\" -H \"Authorization: Bearer <user_token>\" http://localhost/things -d '{\"name\": \"<thing_name>\"}' Response: HTTP/1.1 201 Created Server: nginx/1.20.0 Date: Wed, 12 Jan 2022 14:20:05 GMT Content-Type: application/json Content-Length: 0 Connection: keep-alive Location: /things/647216d6-2f02-4358-9752-afffbf12a642 Warning-Deprecated: This endpoint will be depreciated in v1.0.0. It will be replaced with the bulk endpoint currently found at /things/bulk. Access-Control-Expose-Headers: Location Create Thing with External ID # It is often the case that the user will want to integrate the existing solutions, e.g. an asset management system, with the Mainflux platform. To simplify the integration between the systems and avoid artificial cross-platform reference, such as special fields in Mainflux Things metadata, it is possible to set Mainflux Thing ID with an existing unique ID while create the Thing. This way, the user can set the existing ID as the Thing ID of a newly created Thing to keep reference between Thing and the asset that Thing represents. There are two limitations - the existing ID have to be in UUID V4 format and it has to be unique in the Mainflux domain. To create a thing with an external ID, you need provide the UUID v4 format ID together with thing name, and other fields as well as a user_token Must-have: user_token curl -s -S -i -X POST -H \"Content-Type: application/json\" -H \"Authorization: Bearer <user_token>\" http://localhost/things/bulk -d '[{\"id\": \"<xxxxxxxx-xxxx-xxxx-xxxxxxxxxxxxxxxx>\",\"name\": \"<thing_name>\"}]' Response: HTTP/1.1 201 Created Server: nginx/1.16.0 Date: Wed, 10 Mar 2021 15:18:37 GMT Content-Type: application/json Content-Length: 119 Connection: keep-alive Access-Control-Expose-Headers: Location {\"things\":[{\"id\":\"<xxxxxxxx-xxxx-xxxx-xxxxxxxxxxxxxxxx>\",\"name\":\"thing_name\",\"key\":\"659aa6ca-1781-4a69-9a20-689ddb235506\"}]} Create Things # You can create multiple things at once by entering a series of things structures and a user_token Must-have: user_token and at least two things curl -s -S -i -X POST -H \"Content-Type: application/json\" -H \"Authorization: Bearer <user_token>\" http://localhost/things/bulk -d '[{\"name\": \"<thing_name_1>\"}, {\"name\": \"<thing_name_2>\"}]' Response: HTTP/1.1 201 Created Server: nginx/1.16.0 Date: Wed, 10 Mar 2021 15:19:48 GMT Content-Type: application/json Content-Length: 227 Connection: keep-alive Access-Control-Expose-Headers: Location {\"things\":[{\"id\":\"4328f3e4-4c67-40b3-9491-0ab782c48d50\",\"name\":\"thing_name_1\",\"key\":\"828c6985-c2d6-419e-a124-ba99147b9920\"},{\"id\":\"38aa33fe-39e5-4ee3-97ba-4227cfac63f6\",\"name\":\"thing_name_2\",\"key\":\"f73e7342-06c1-499a-9584-35de495aa338\"}]} Create Things with external ID # The same as creating a Thing with external ID the user can create multiple things at once by providing UUID v4 format unique ID in a series of things together with a user_token Must-have: user_token and at least two things curl -s -S -i -X POST -H \"Content-Type: application/json\" -H \"Authorization: Bearer <user_token>\" http://localhost/things/bulk -d '[{\"id\": \"<xxxxxxxx-xxxx-xxxx-xxxxxxxxxxxxxxx1>\",\"name\": \"<thing_name_1>\"},{\"id\": \"<xxxxxxxx-xxxx-xxxx-xxxxxxxxxxxxxxx2>\",\"name\": \"<thing_name_2>\"}]' Response: HTTP/1.1 201 Created Server: nginx/1.16.0 Date: Wed, 10 Mar 2021 15:19:48 GMT Content-Type: application/json Content-Length: 227 Connection: keep-alive Access-Control-Expose-Headers: Location {\"things\":[{\"id\":\"<xxxxxxxx-xxxx-xxxx-xxxxxxxxxxxxxxx1>\",\"name\":\"thing_name_1\",\"key\":\"828c6985-c2d6-419e-a124-ba99147b9920\"},{\"id\":\"<xxxxxxxx-xxxx-xxxx-xxxxxxxxxxxxxxx2>\",\"name\":\"thing_name_2\",\"key\":\"f73e7342-06c1-499a-9584-35de495aa338\"}]} Get Thing # You can get thing entity by entering the thing ID and user_token Must-have: user_token and thing_id curl -s -S -i -X GET -H \"Authorization: Bearer <user_token>\" http://localhost/things/<thing_id> Response: HTTP/1.1 200 OK Server: nginx/1.16.0 Date: Wed, 10 Mar 2021 15:20:52 GMT Content-Type: application/json Content-Length: 106 Connection: keep-alive Access-Control-Expose-Headers: Location {\"id\":\"64140f0b-6448-41cf-967e-1bbcc703c332\",\"name\":\"thing_name\",\"key\":\"659aa6ca-1781-4a69-9a20-689ddb235506\"} Get All Things # Get all things, list requests accepts limit and offset query parameters Must-have: user_token curl -s -S -i -X GET -H \"Authorization: Bearer <user_token>\" http://localhost/things Response: HTTP/1.1 200 OK Server: nginx/1.16.0 Date: Wed, 10 Mar 2021 15:21:49 GMT Content-Type: application/json Content-Length: 391 Connection: keep-alive Access-Control-Expose-Headers: Location {\"total\":3,\"offset\":0,\"limit\":10,\"order\":\"\",\"direction\":\"\",\"things\":[{\"id\":\"64140f0b-6448-41cf-967e-1bbcc703c332\",\"name\":\"thing_name\",\"key\":\"659aa6ca-1781-4a69-9a20-689ddb235506\"},{\"id\":\"4328f3e4-4c67-40b3-9491-0ab782c48d50\",\"name\":\"thing_name_1\",\"key\":\"828c6985-c2d6-419e-a124-ba99147b9920\"},{\"id\":\"38aa33fe-39e5-4ee3-97ba-4227cfac63f6\",\"name\":\"thing_name_2\",\"key\":\"f73e7342-06c1-499a-9584-35de495aa338\"}]} Update Thing # Updating a thing entity Must-have: user_token and thing_id curl -s -S -i -X PUT -H \"Content-Type: application/json\" -H \"Authorization: Bearer <user_token>\" http://localhost/things/<thing_id> -d '{\"name\": \"<thing_name>\"}' Response: HTTP/1.1 200 OK Server: nginx/1.16.0 Date: Wed, 10 Mar 2021 15:23:36 GMT Content-Type: application/json Content-Length: 0 Connection: keep-alive Access-Control-Expose-Headers: Location Delete Thing # To delete a thing you need a thing_id and a user_token Must-have: user_token and thing_id curl -s -S -i -X DELETE -H \"Content-Type: application/json\" -H \"Authorization: Bearer <user_token>\" http://localhost/things/<thing_id> Response: HTTP/1.1 204 No Content Server: nginx/1.16.0 Date: Wed, 10 Mar 2021 15:24:44 GMT Content-Type: application/json Connection: keep-alive Access-Control-Expose-Headers: Location Channels # Create Channel # To create a channel, you need a user_token Must-have: user_token curl -s -S -i -X POST -H \"Content-Type: application/json\" -H \"Authorization: Bearer <user_token>\" http://localhost/channels -d '{\"name\": \"<channel_name>\"}' Response: HTTP/1.1 201 Created Server: nginx/1.16.0 Date: Wed, 10 Mar 2021 15:26:51 GMT Content-Type: application/json Content-Length: 0 Connection: keep-alive Location: /channels/db4b7428-e278-4fe3-b85a-d65554d6abe9 Warning-Deprecated: This endpoint will be depreciated in v1.0.0. It will be replaced with the bulk endpoint currently found at /channels/bulk. Access-Control-Expose-Headers: Location Create Channel with external ID # Channel is a group of things that could represent a special category in existing systems, e.g. a building level channel could represent the level of a smarting building system. For helping to keep the reference, it is possible to set an existing ID while creating the Mainflux channel. There are two limitations - the existing ID has to be in UUID V4 format and it has to be unique in the Mainflux domain. To create a channel with external ID, the user needs provide a UUID v4 format unique ID, and a user_token Must-have: user_token curl -s -S -i -X POST -H \"Content-Type: application/json\" -H \"Authorization: Bearer <user_token>\" http://localhost/channels -d '{\"id\": \"<xxxxxxxx-xxxx-xxxx-xxxxxxxxxxxxxxxx>\",\"name\": \"<channel_name>\"}' Response: HTTP/1.1 201 Created Server: nginx/1.16.0 Date: Wed, 10 Mar 2021 15:26:51 GMT Content-Type: application/json Content-Length: 0 Connection: keep-alive Location: /channels/db4b7428-e278-4fe3-b85a-d65554d6abe9 Warning-Deprecated: This endpoint will be depreciated in v1.0.0. It will be replaced with the bulk endpoint currently found at /channels/bulk. Access-Control-Expose-Headers: Location Create Channels # The same as creating a channel with external ID the user can create multiple channels at once by providing UUID v4 format unique ID in a series of channels together with a user_token Must-have: user_token and at least 2 channels curl -s -S -i -X POST -H \"Content-Type: application/json\" -H \"Authorization: Bearer <user_token>\" http://localhost/channels/bulk -d '[{\"name\": \"<channel_name_1>\"}, {\"name\": \"<channel_name_2>\"}]' Response: HTTP/1.1 201 Created Server: nginx/1.16.0 Date: Wed, 10 Mar 2021 15:28:10 GMT Content-Type: application/json Content-Length: 143 Connection: keep-alive Access-Control-Expose-Headers: Location {\"channels\":[{\"id\":\"b8073d41-01dc-46ad-bb26-cfecc596c6c1\",\"name\":\"channel_name_1\"},{\"id\":\"2200527a-f590-4fe5-b9d6-892fc6f825c3\",\"name\":\"channel_name_2\"}]} Create Channels with external ID # As with things, you can create multiple channels with external ID at once Must-have: user_token and at least 2 channels curl -s -S -i -X POST -H \"Content-Type: application/json\" -H \"Authorization: Bearer <user_token>\" http://localhost/channels/bulk -d '[{\"id\": \"<xxxxxxxx-xxxx-xxxx-xxxxxxxxxxxxxxx1>\",\"name\": \"<channel_name_1>\"}, {\"id\": \"<xxxxxxxx-xxxx-xxxx-xxxxxxxxxxxxxxx2>\",\"name\": \"<channel_name_2>\"}]' Response: HTTP/1.1 201 Created Server: nginx/1.16.0 Date: Wed, 10 Mar 2021 15:28:10 GMT Content-Type: application/json Content-Length: 143 Connection: keep-alive Access-Control-Expose-Headers: Location {\"channels\":[{\"id\":\"<xxxxxxxx-xxxx-xxxx-xxxxxxxxxxxxxxx1>\",\"name\":\"channel_name_1\"},{\"id\":\"<xxxxxxxx-xxxx-xxxx-xxxxxxxxxxxxxxx2>\",\"name\":\"channel_name_2\"}]} Get Channel # Get a channel entity for a logged in user Must-have: user_token and channel_id curl -s -S -i -X GET -H \"Authorization: Bearer <user_token>\" http://localhost/channels/<channel_id> Response: HTTP/1.1 200 OK Server: nginx/1.16.0 Date: Wed, 10 Mar 2021 15:29:49 GMT Content-Type: application/json Content-Length: 63 Connection: keep-alive Access-Control-Expose-Headers: Location {\"id\":\"db4b7428-e278-4fe3-b85a-d65554d6abe9\",\"name\":\"channel_name\"} Get Channels # Get all channels, list requests accepts limit and offset query parameters Must-have: user_token curl -s -S -i -X GET -H \"Authorization: Bearer <user_token>\" http://localhost/channels Response: HTTP/1.1 200 OK Server: nginx/1.16.0 Date: Wed, 10 Mar 2021 15:30:34 GMT Content-Type: application/json Content-Length: 264 Connection: keep-alive Access-Control-Expose-Headers: Location {\"total\":3,\"offset\":0,\"limit\":10,\"order\":\"\",\"direction\":\"\",\"channels\":[{\"id\":\"db4b7428-e278-4fe3-b85a-d65554d6abe9\",\"name\":\"channel_name\"},{\"id\":\"b8073d41-01dc-46ad-bb26-cfecc596c6c1\",\"name\":\"channel_name_1\"},{\"id\":\"2200527a-f590-4fe5-b9d6-892fc6f825c3\",\"name\":\"channel_name_2\"}]} Update Channel # Update channel entity Must-have: user_token and channel_id curl -s -S -i -X PUT -H \"Content-Type: application/json\" -H \"Authorization: Bearer <user_token>\" http://localhost/channels/<channel_id> -d '{\"name\": \"<channel_name>\"}' Response: HTTP/1.1 200 OK Server: nginx/1.16.0 Date: Wed, 10 Mar 2021 15:32:08 GMT Content-Type: application/json Content-Length: 0 Connection: keep-alive Access-Control-Expose-Headers: Location Delete Channel # Delete a channel entity Must-have: user_token and channel_id curl -s -S -i -X DELETE -H \"Content-Type: application/json\" -H \"Authorization: Bearer <user_token>\" http://localhost/channels/<channel_id> Response: HTTP/1.1 204 No Content Server: nginx/1.16.0 Date: Wed, 10 Mar 2021 15:33:21 GMT Content-Type: application/json Connection: keep-alive Access-Control-Expose-Headers: Location Connect # Connect things to channels Must-have: user_token , channel_id and thing_id curl -s -S -i -X POST -H \"Content-Type: application/json\" -H \"Authorization: Bearer <user_token>\" http://localhost/connect -d '{\"channel_ids\": [\"<channel_id>\"], \"thing_ids\": [\"<thing_id>\"]}' Response: HTTP/1.1 200 OK Server: nginx/1.16.0 Date: Wed, 10 Mar 2021 15:36:32 GMT Content-Type: application/json Content-Length: 0 Connection: keep-alive Access-Control-Expose-Headers: Location Connect thing to channel Must-have: user_token , channel_id and thing_id curl -s -S -i -X PUT -H \"Content-Type: application/json\" -H \"Authorization: Bearer <user_token>\" http://localhost/channels/<channel_id>/things/<thing_id> Response: HTTP/1.1 200 OK Server: nginx/1.20.0 Date: Fri, 21 Jan 2022 15:20:47 GMT Content-Type: application/json Content-Length: 0 Connection: keep-alive Warning-Deprecated: This endpoint will be depreciated in v1.0.0. It will be replaced with the bulk endpoint found at /connect. Access-Control-Expose-Headers: Location Disconnect # Disconnect things from channels specified by lists of IDs. Must-have: user_token , channel_ids and thing_ids curl -s -S -i -X PUT -H \"Content-Type: application/json\" -H \"Authorization: Bearer <user_token>\" http://localhost:<service_port>/disconnect -d '{\"thing_ids\": [\"<thing_id_1>\", \"<thing_id_2>\"], \"channel_ids\": [\"<channel_id_1>\", \"<channel_id_2>\"]}' Response: HTTP/1.1 200 OK Content-Type: application/json Date: Sun, 11 Jul 2021 17:23:39 GMT Content-Length: 0 Disconnect thing from the channel Must-have: user_token , channel_id and thing_id curl -s -S -i -X DELETE -H \"Content-Type: application/json\" -H \"Authorization: Bearer <user_token>\" http://localhost/channels/<channel_id>/things/<thing_id> Response: HTTP/1.1 204 No Content Server: nginx/1.16.0 Date: Wed, 10 Mar 2021 15:38:14 GMT Content-Type: application/json Connection: keep-alive Access-Control-Expose-Headers: Location Access by Key # Checks if thing has access to a channel Must-have: channel_id and thing_key curl -s -S -i -X POST -H \"Content-Type: application/json\" http://localhost/identify/channels/<channel_id>/access-by-key -d '{\"token\": \"<thing_key>\"}' Response: HTTP/1.1 200 OK Server: nginx/1.16.0 Date: Mon, 22 Mar 2021 13:10:53 GMT Content-Type: application/json Content-Length: 46 Connection: keep-alive Access-Control-Expose-Headers: Location {\"id\":\"d69d0098-072b-41bf-8c6e-ce4dbb12d333\"} Access by ID # Checks if thing has access to a channel Must-have: channel_id and thing_id curl -s -S -i -X POST -H \"Content-Type: application/json\" http://localhost/identify/channels/<channel_id>/access-by-id -d '{\"thing_id\": \"<thing_id>\"}' Response: HTTP/1.1 200 OK Server: nginx/1.16.0 Date: Mon, 22 Mar 2021 15:02:02 GMT Content-Type: application/json Content-Length: 0 Connection: keep-alive Access-Control-Expose-Headers: Location Identify # Validates thing's key and returns it's ID if key is valid Must-have: thing_key curl -s -S -i -X POST -H \"Content-Type: application/json\" http://localhost/identify -d '{\"token\": \"<thing_key>\"}' Response: HTTP/1.1 200 OK Server: nginx/1.16.0 Date: Mon, 22 Mar 2021 15:04:41 GMT Content-Type: application/json Content-Length: 46 Connection: keep-alive Access-Control-Expose-Headers: Location {\"id\":\"d69d0098-072b-41bf-8c6e-ce4dbb12d333\"} Messages # Send Messages # Sends message via HTTP protocol Must-have: thing_key and channel_id curl -s -S -i -X POST -H \"Content-Type: application/json\" -H \"Authorization: Thing <thing_key>\" http://localhost/http/channels/<channel_id>/messages -d '[{\"bn\":\"some-base-name:\",\"bt\":1.276020076001e+09,\"bu\":\"A\",\"bver\":5,\"n\":\"voltage\",\"u\":\"V\",\"v\":120.1}, {\"n\":\"current\",\"t\":-5,\"v\":1.2}, {\"n\":\"current\",\"t\":-4,\"v\":1.3}]' Response: HTTP/1.1 202 Accepted Server: nginx/1.16.0 Date: Wed, 10 Mar 2021 16:53:54 GMT Content-Length: 0 Connection: keep-alive Read Messages # Reads messages from database for a given channel Must-have: thing_key and channel_id curl -s -S -i -H \"Authorization: Thing <thing_key>\" http://localhost:<service_port>/channels/<channel_id>/messages?offset=0&limit=5 Response: HTTP/1.1 200 OK Content-Type: application/json Date: Wed, 10 Mar 2021 16:54:58 GMT Content-Length: 660 {\"offset\":0,\"limit\":10,\"format\":\"messages\",\"total\":3,\"messages\":[{\"channel_name\":\"1a0cde06-8e5c-4f07-aac5-95aff4a19ea0\",\"publisher\":\"33eb28c3-4ca2-45c3-b1c5-d5d049c6c24e\",\"protocol\":\"http\",\"name\":\"some-base-name:voltage\",\"unit\":\"V\",\"time\":1276020076.001,\"value\":120.1},{\"channel_name\":\"1a0cde06-8e5c-4f07-aac5-95aff4a19ea0\",\"publisher\":\"33eb28c3-4ca2-45c3-b1c5-d5d049c6c24e\",\"protocol\":\"http\",\"name\":\"some-base-name:current\",\"unit\":\"A\",\"time\":1276020072.001,\"value\":1.3},{\"channel_name\":\"1a0cde06-8e5c-4f07-aac5-95aff4a19ea0\",\"publisher\":\"33eb28c3-4ca2-45c3-b1c5-d5d049c6c24e\",\"protocol\":\"http\",\"name\":\"some-base-name:current\",\"unit\":\"A\",\"time\":1276020071.001,\"value\":1.2}]} Groups # Create group # To create a group, you need the group name and a user_token Must-have: user_token curl -s -S -i -X POST -H \"Content-Type: application/json\" -H \"Authorization: Bearer <user_token>\" http://localhost/groups -d '{\"name\": \"<group_name>\", \"parent_id\": \"<previous_group_id>\", \"description\": \"<group_description>\", \"metadata\": {}}' Response: HTTP/1.1 201 Created Server: nginx/1.16.0 Date: Wed, 10 Mar 2021 16:58:09 GMT Content-Type: application/json Content-Length: 0 Connection: keep-alive Location: /groups/01F0EH61SA7C7NDKWYCXVG7PWD Access-Control-Expose-Headers: Location Members # Get list of ID's from group Must-have: user_token and group_id curl -s -S -i -X GET -H 'Content-Type: application/json' -H \"Authorization: Bearer <user_token>\" http://localhost/groups/<group_id>/members Response: HTTP/1.1 200 OK Server: nginx/1.16.0 Date: Tue, 23 Mar 2021 09:18:10 GMT Content-Type: application/json Content-Length: 116 Connection: keep-alive Access-Control-Expose-Headers: Location {\"limit\":10,\"total\":0,\"level\":0,\"name\":\"\",\"Members\":[{\"ID\":\"d782b42b-e317-4cd7-9dd0-4e2ea0f349c8\",\"Type\":\"users\"}]} Assign # Assign user, thing or channel to a group Must-have: user_token , group_id , member_id and member_type curl -s -S -i -X POST -H \"Content-Type: application/json\" -H \"Authorization: Bearer <user_token>\" http://localhost/groups/<group_id>/members -d '{\"members\":[\"<user_id>\" | \"<thing_id_>\" | \"<channel_id_>\"], \"type\":[\"users\" | \"things\" | \"channels\"]}' Response: HTTP/1.1 200 OK Server: nginx/1.16.0 Date: Wed, 10 Mar 2021 17:04:41 GMT Content-Type: application/json Content-Length: 0 Connection: keep-alive Access-Control-Expose-Headers: Location Unassign # Unassign user, thing or channel from group Must-have: user_token , group_id , member_id and member_type curl -s -S -i -X DELETE -H \"Content-Type: application/json\" -H \"Authorization: Bearer <user_token>\" http://localhost/groups/<group_id>/members -d '{\"members\":[\"<user_id>\" | \"<thing_id_>\" | \"<channel_id_>\"], \"type\":[\"users\" | \"things\" | \"channels\"]}' Response: HTTP/1.1 204 No Content Server: nginx/1.16.0 Date: Wed, 10 Mar 2021 17:13:06 GMT Content-Type: application/json Connection: keep-alive Access-Control-Expose-Headers: Location Get group # Get a group entity for a logged in user Must-have: user_token and group_id curl -s -S -i -X GET -H \"Content-Type: application/json\" -H \"Authorization: Bearer <user_token>\" http://localhost/groups/<group_id> Response: HTTP/1.1 200 OK Server: nginx/1.16.0 Date: Wed, 10 Mar 2021 17:06:48 GMT Content-Type: application/json Content-Length: 201 Connection: keep-alive Access-Control-Expose-Headers: Location {\"id\":\"01F0EH61SA7C7NDKWYCXVG7PWD\",\"name\":\"group_name\",\"owner_id\":\"d782b42b-e317-4cd7-9dd0-4e2ea0f349c8\",\"level\":0,\"path\":\"\",\"created_at\":\"2021-03-10T16:58:09.579Z\",\"updated_at\":\"2021-03-10T16:58:09.579Z\"} Get groups # Get all groups, list requests accepts limit and offset query parameters Must-have: user_token curl -s -S -i -X GET -H \"Content-Type: application/json\" -H \"Authorization: Bearer <user_token>\" http://localhost/groups Response: HTTP/1.1 200 OK Server: nginx/1.16.0 Date: Wed, 10 Mar 2021 17:09:28 GMT Content-Type: application/json Content-Length: 496 Connection: keep-alive Access-Control-Expose-Headers: Location {\"total\":2,\"level\":0,\"name\":\"\",\"groups\":[{\"id\":\"01F0EH61SA7C7NDKWYCXVG7PWD\",\"name\":\"group_name\",\"owner_id\":\"d782b42b-e317-4cd7-9dd0-4e2ea0f349c8\",\"level\":1,\"path\":\"01F0EH61SA7C7NDKWYCXVG7PWD\",\"created_at\":\"2021-03-10T16:58:09.579Z\",\"updated_at\":\"2021-03-10T16:58:09.579Z\"},{\"id\":\"01F0EHQTP2HQ7JTWZNMVJ0JJCN\",\"name\":\"group_name_1\",\"owner_id\":\"d782b42b-e317-4cd7-9dd0-4e2ea0f349c8\",\"level\":1,\"path\":\"01F0EHQTP2HQ7JTWZNMVJ0JJCN\",\"created_at\":\"2021-03-10T17:07:52.13Z\",\"updated_at\":\"2021-03-10T17:07:52.13Z\"}]} Update group # Update group entity Must-have: user_token and group_id curl -s -S -i -X PUT -H \"Content-Type: application/json\" -H \"Authorization: Bearer <user_token>\" http://localhost/groups/<group_id> -d '{\"name\": \"<group_name>\"}' Response: HTTP/1.1 200 OK Server: nginx/1.16.0 Date: Wed, 10 Mar 2021 17:11:51 GMT Content-Type: application/json Content-Length: 0 Connection: keep-alive Access-Control-Expose-Headers: Location Delete group # Delete a group entity Must-have: user_token and group_id curl -s -S -i -X DELETE -H \"Content-Type: application/json\" -H \"Authorization: Bearer <user_token>\" http://localhost/groups/<group_id> Response: HTTP/1.1 204 No Content Server: nginx/1.16.0 Date: Wed, 10 Mar 2021 17:14:13 GMT Content-Type: application/json Connection: keep-alive Access-Control-Expose-Headers: Location Share User Group with Things Group # Adds access rights on thing groups to the user group. Must-have: user_token , user_group_id and <thing_group_id> . curl -s -S -i -X POST http://localhost/groups/<user_group_id>/share -d '{\"thing_group_id\": \"<thing_group_id>\"}' -H 'Content-Type: application/json' -H \"Authorization: Bearer <user_token>\" Each user from the group identified by user_group_id will have read , write , and delete policies on the things grouped by thing_group_id . Therefore, they will be able to do operations defined under Things Policies section . Policies # Add policies # The admin can add custom policies. Only policies defined on Predefined Policies section are allowed. Must-have: admin_token, object, subjects_ids and policies curl -isSX POST http://localhost/policies -d '{\"subjects\": [\"<subject_id1>\",...\"<subject_idN>\"], \"object\": \"<object>\", \"policies\": [\"<action_1>, ...\"<action_N>\"]}' -H \"Authorization: Bearer <admin_token>\" -H 'Content-Type: application/json' admin_token must belong to the admin. Response: HTTP/1.1 201 Created Content-Type: application/json Date: Wed, 03 Nov 2021 13:00:14 GMT Content-Length: 3 {} Delete policies # The admin can delete policies. Only policies defined on Predefined Policies section are allowed. Must-have: admin_token, object, subjects_ids and policies curl -isSX PUT http://localhost/policies -d '{\"subjects\": [\"<subject_id1>\",...\"<subject_idN>\"], \"object\": \"<object>\", \"policies\": [\"<action_1>, ...\"<action_N>\"]}' -H \"Authorization: Bearer <admin_token>\" -H 'Content-Type: application/json' admin_token must belong to the admin. Response: HTTP/1.1 204 No Content Content-Type: application/json Date: Wed, 03 Nov 2021 13:00:05 GMT API Key # Issue API Key # Generates a new API key. Then new API key will be uniquely identified by its ID. Duration is expressed in seconds. Must-have: user_token curl -isSX POST http://localhost/keys -H \"Content-Type: application/json\" -H \"Authorization: Bearer <user_token>\" -d '{\"type\":2, \"duration\":10000}' Response: HTTP/1.1 201 Created Server: nginx/1.20.0 Date: Sun, 19 Dec 2021 17:39:44 GMT Content-Type: application/json Content-Length: 476 Connection: keep-alive Access-Control-Expose-Headers: Location {\"id\":\"4d62fb1e-085e-435c-a0c5-5255febfa35b\",\"value\":\"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJleHAiOjE2NDAwMzU1ODQsImp0aSI6IjRkNjJmYjFlLTA4NWUtNDM1Yy1hMGM1LTUyNTVmZWJmYTM1YiIsImlhdCI6MTYzOTkzNTU4NCwiaXNzIjoibWFpbmZsdXguYXV0aCIsInN1YiI6ImZscDFAZW1haWwuY29tIiwiaXNzdWVyX2lkIjoiYzkzY2FmYjMtYjNhNy00ZTdmLWE0NzAtMTVjMTRkOGVkMWUwIiwidHlwZSI6Mn0.RnvjhygEPPWFDEUKtfk5okzVhZzOcO0azr8gd5vby5M\",\"issued_at\":\"2021-12-19T17:39:44.175088349Z\",\"expires_at\":\"2021-12-20T21:26:24.175088349Z\"} Get API key details # Must-have: 'user_token' and 'key_id' curl -isSX GET http://localhost/keys/<key_id> -H 'Content-Type: application/json' -H 'Authorization: Bearer <user_token>' Response: HTTP/1.1 200 OK Server: nginx/1.20.0 Date: Sun, 19 Dec 2021 17:43:30 GMT Content-Type: application/json Content-Length: 218 Connection: keep-alive Access-Control-Expose-Headers: Location {\"id\":\"f630f594-d967-4c54-85ef-af58efe8e8ed\",\"issuer_id\":\"c93cafb3-b3a7-4e7f-a470-15c14d8ed1e0\",\"subject\":\"test@email.com\",\"type\":2,\"issued_at\":\"2021-12-19T17:42:40.884521Z\",\"expires_at\":\"2021-12-20T21:29:20.884521Z\"} Revoke API key identified by the given ID # Must-have: 'user_token' and 'key_id' curl -isSX DELETE http://localhost/keys/<key_id> -H 'Content-Type: application/json' -H 'Authorization: Bearer <user_token>' Response: HTTP/1.1 204 No Content Server: nginx/1.20.0 Date: Sun, 19 Dec 2021 17:47:11 GMT Content-Type: application/json Connection: keep-alive Access-Control-Expose-Headers: Location","title":"API"},{"location":"api/#api","text":"","title":"API"},{"location":"api/#reference","text":"API reference in the Swagger UI can be found at: https://api.mainflux.io","title":"Reference"},{"location":"api/#users","text":"","title":"Users"},{"location":"api/#create-user","text":"To start working with the Mainflux system, you need to create a user account. Must-have: e-mail and password (password must contain at least 8 characters) curl -s -S -i -X POST -H \"Content-Type: application/json\" http://localhost/users -d '{\"email\":\"<user_email>\", \"password\":\"<user_password>\"}' Response: HTTP/1.1 201 Created Server: nginx/1.16.0 Date: Wed, 10 Mar 2021 15:06:45 GMT Content-Type: application/json Content-Length: 0 Connection: keep-alive Location: /users/d782b42b-e317-4cd7-9dd0-4e2ea0f349c8 Strict-Transport-Security: max-age=63072000; includeSubdomains X-Frame-Options: DENY X-Content-Type-Options: nosniff Access-Control-Allow-Origin: * Access-Control-Allow-Methods: * Access-Control-Allow-Headers: *","title":"Create User"},{"location":"api/#create-token","text":"To log in to the Mainflux system, you need to create a user_token . Must-have: registered e-mail and password curl -s -S -i -X POST -H \"Content-Type: application/json\" http://localhost/tokens -d '{\"email\":\"<user_email>\", \"password\":\"<user_password>\"}' Response: HTTP/1.1 201 Created Server: nginx/1.16.0 Date: Wed, 10 Mar 2021 15:07:18 GMT Content-Type: application/json Content-Length: 281 Connection: keep-alive Strict-Transport-Security: max-age=63072000; includeSubdomains X-Frame-Options: DENY X-Content-Type-Options: nosniff Access-Control-Allow-Origin: * Access-Control-Allow-Methods: * Access-Control-Allow-Headers: * {\"token\":\"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJleHAiOjE2MTU0MjQ4MzgsImlhdCI6MTYxNTM4ODgzOCwiaXNzIjoibWFpbmZsdXguYXV0aCIsInN1YiI6InRlc3RAZW1haWwuY29tIiwiaXNzdWVyX2lkIjoiZDc4MmI0MmItZTMxNy00Y2Q3LTlkZDAtNGUyZWEwZjM0OWM4IiwidHlwZSI6MH0.TAQxV6TImKw06RsK0J11rOHiWPvexEOA4BNZnhLhtxs\"}","title":"Create Token"},{"location":"api/#get-user","text":"You can always check the user entity that is logged in by entering the user ID and user_token . Must-have: user_id and user_token curl -s -S -i -X GET -H \"Authorization: Bearer <user_token>\" http://localhost/users/<user_id> Response: HTTP/1.1 200 OK Server: nginx/1.16.0 Date: Wed, 10 Mar 2021 15:09:47 GMT Content-Type: application/json Content-Length: 85 Connection: keep-alive Strict-Transport-Security: max-age=63072000; includeSubdomains X-Frame-Options: DENY X-Content-Type-Options: nosniff Access-Control-Allow-Origin: * Access-Control-Allow-Methods: * Access-Control-Allow-Headers: * {\"id\":\"d782b42b-e317-4cd7-9dd0-4e2ea0f349c8\",\"email\":\"test@email.com\"}","title":"Get User"},{"location":"api/#get-all-users","text":"You can get all users in the database by calling the this function Must-have: user_token curl -s -S -i -X GET -H \"Authorization: Bearer <user_token>\" http://localhost/users Response: HTTP/1.1 200 OK Server: nginx/1.16.0 Date: Wed, 10 Mar 2021 15:11:28 GMT Content-Type: application/json Content-Length: 217 Connection: keep-alive Strict-Transport-Security: max-age=63072000; includeSubdomains X-Frame-Options: DENY X-Content-Type-Options: nosniff Access-Control-Allow-Origin: * Access-Control-Allow-Methods: * Access-Control-Allow-Headers: * {\"total\":2,\"offset\":0,\"limit\":10,\"Users\":[{\"id\":\"4bf4a13a-e9c3-4207-aa11-fe569986c301\",\"email\":\"admin@example.com\"},{\"id\":\"d782b42b-e317-4cd7-9dd0-4e2ea0f349c8\",\"email\":\"test@email.com\"}]}","title":"Get All Users"},{"location":"api/#update-user","text":"Updating user's metadata Must-have: user_token curl -s -S -i -X PUT -H \"Content-Type: application/json\" -H \"Authorization: Bearer <user_token>\" http://localhost/users -d '{\"metadata\":{\"foo\":\"bar\"}}' Response: HTTP/1.1 200 OK Server: nginx/1.16.0 Date: Wed, 10 Mar 2021 15:15:31 GMT Content-Type: application/json Content-Length: 0 Connection: keep-alive Strict-Transport-Security: max-age=63072000; includeSubdomains X-Frame-Options: DENY X-Content-Type-Options: nosniff Access-Control-Allow-Origin: * Access-Control-Allow-Methods: * Access-Control-Allow-Headers: *","title":"Update User"},{"location":"api/#change-password","text":"Changing the user password can be done by calling the update password function Must-have: user_token , old_password and password ( new_password ) curl -s -S -i -X PATCH -H \"Content-Type: application/json\" -H \"Authorization: Bearer <user_token>\" http://localhost/password -d '{\"old_password\":\"<old_password>\", \"password\":\"<new_password>\"}' Response: HTTP/1.1 201 Created Server: nginx/1.16.0 Date: Wed, 10 Mar 2021 15:17:36 GMT Content-Type: application/json Content-Length: 11 Connection: keep-alive Strict-Transport-Security: max-age=63072000; includeSubdomains X-Frame-Options: DENY X-Content-Type-Options: nosniff Access-Control-Allow-Origin: * Access-Control-Allow-Methods: * Access-Control-Allow-Headers: *","title":"Change Password"},{"location":"api/#things","text":"","title":"Things"},{"location":"api/#create-thing","text":"To create a thing, you need the thing and a user_token Must-have: user_token curl -s -S -i -X POST -H \"Content-Type: application/json\" -H \"Authorization: Bearer <user_token>\" http://localhost/things -d '{\"name\": \"<thing_name>\"}' Response: HTTP/1.1 201 Created Server: nginx/1.20.0 Date: Wed, 12 Jan 2022 14:20:05 GMT Content-Type: application/json Content-Length: 0 Connection: keep-alive Location: /things/647216d6-2f02-4358-9752-afffbf12a642 Warning-Deprecated: This endpoint will be depreciated in v1.0.0. It will be replaced with the bulk endpoint currently found at /things/bulk. Access-Control-Expose-Headers: Location","title":"Create Thing"},{"location":"api/#create-thing-with-external-id","text":"It is often the case that the user will want to integrate the existing solutions, e.g. an asset management system, with the Mainflux platform. To simplify the integration between the systems and avoid artificial cross-platform reference, such as special fields in Mainflux Things metadata, it is possible to set Mainflux Thing ID with an existing unique ID while create the Thing. This way, the user can set the existing ID as the Thing ID of a newly created Thing to keep reference between Thing and the asset that Thing represents. There are two limitations - the existing ID have to be in UUID V4 format and it has to be unique in the Mainflux domain. To create a thing with an external ID, you need provide the UUID v4 format ID together with thing name, and other fields as well as a user_token Must-have: user_token curl -s -S -i -X POST -H \"Content-Type: application/json\" -H \"Authorization: Bearer <user_token>\" http://localhost/things/bulk -d '[{\"id\": \"<xxxxxxxx-xxxx-xxxx-xxxxxxxxxxxxxxxx>\",\"name\": \"<thing_name>\"}]' Response: HTTP/1.1 201 Created Server: nginx/1.16.0 Date: Wed, 10 Mar 2021 15:18:37 GMT Content-Type: application/json Content-Length: 119 Connection: keep-alive Access-Control-Expose-Headers: Location {\"things\":[{\"id\":\"<xxxxxxxx-xxxx-xxxx-xxxxxxxxxxxxxxxx>\",\"name\":\"thing_name\",\"key\":\"659aa6ca-1781-4a69-9a20-689ddb235506\"}]}","title":"Create Thing with External ID"},{"location":"api/#create-things","text":"You can create multiple things at once by entering a series of things structures and a user_token Must-have: user_token and at least two things curl -s -S -i -X POST -H \"Content-Type: application/json\" -H \"Authorization: Bearer <user_token>\" http://localhost/things/bulk -d '[{\"name\": \"<thing_name_1>\"}, {\"name\": \"<thing_name_2>\"}]' Response: HTTP/1.1 201 Created Server: nginx/1.16.0 Date: Wed, 10 Mar 2021 15:19:48 GMT Content-Type: application/json Content-Length: 227 Connection: keep-alive Access-Control-Expose-Headers: Location {\"things\":[{\"id\":\"4328f3e4-4c67-40b3-9491-0ab782c48d50\",\"name\":\"thing_name_1\",\"key\":\"828c6985-c2d6-419e-a124-ba99147b9920\"},{\"id\":\"38aa33fe-39e5-4ee3-97ba-4227cfac63f6\",\"name\":\"thing_name_2\",\"key\":\"f73e7342-06c1-499a-9584-35de495aa338\"}]}","title":"Create Things"},{"location":"api/#create-things-with-external-id","text":"The same as creating a Thing with external ID the user can create multiple things at once by providing UUID v4 format unique ID in a series of things together with a user_token Must-have: user_token and at least two things curl -s -S -i -X POST -H \"Content-Type: application/json\" -H \"Authorization: Bearer <user_token>\" http://localhost/things/bulk -d '[{\"id\": \"<xxxxxxxx-xxxx-xxxx-xxxxxxxxxxxxxxx1>\",\"name\": \"<thing_name_1>\"},{\"id\": \"<xxxxxxxx-xxxx-xxxx-xxxxxxxxxxxxxxx2>\",\"name\": \"<thing_name_2>\"}]' Response: HTTP/1.1 201 Created Server: nginx/1.16.0 Date: Wed, 10 Mar 2021 15:19:48 GMT Content-Type: application/json Content-Length: 227 Connection: keep-alive Access-Control-Expose-Headers: Location {\"things\":[{\"id\":\"<xxxxxxxx-xxxx-xxxx-xxxxxxxxxxxxxxx1>\",\"name\":\"thing_name_1\",\"key\":\"828c6985-c2d6-419e-a124-ba99147b9920\"},{\"id\":\"<xxxxxxxx-xxxx-xxxx-xxxxxxxxxxxxxxx2>\",\"name\":\"thing_name_2\",\"key\":\"f73e7342-06c1-499a-9584-35de495aa338\"}]}","title":"Create Things with external ID"},{"location":"api/#get-thing","text":"You can get thing entity by entering the thing ID and user_token Must-have: user_token and thing_id curl -s -S -i -X GET -H \"Authorization: Bearer <user_token>\" http://localhost/things/<thing_id> Response: HTTP/1.1 200 OK Server: nginx/1.16.0 Date: Wed, 10 Mar 2021 15:20:52 GMT Content-Type: application/json Content-Length: 106 Connection: keep-alive Access-Control-Expose-Headers: Location {\"id\":\"64140f0b-6448-41cf-967e-1bbcc703c332\",\"name\":\"thing_name\",\"key\":\"659aa6ca-1781-4a69-9a20-689ddb235506\"}","title":"Get Thing"},{"location":"api/#get-all-things","text":"Get all things, list requests accepts limit and offset query parameters Must-have: user_token curl -s -S -i -X GET -H \"Authorization: Bearer <user_token>\" http://localhost/things Response: HTTP/1.1 200 OK Server: nginx/1.16.0 Date: Wed, 10 Mar 2021 15:21:49 GMT Content-Type: application/json Content-Length: 391 Connection: keep-alive Access-Control-Expose-Headers: Location {\"total\":3,\"offset\":0,\"limit\":10,\"order\":\"\",\"direction\":\"\",\"things\":[{\"id\":\"64140f0b-6448-41cf-967e-1bbcc703c332\",\"name\":\"thing_name\",\"key\":\"659aa6ca-1781-4a69-9a20-689ddb235506\"},{\"id\":\"4328f3e4-4c67-40b3-9491-0ab782c48d50\",\"name\":\"thing_name_1\",\"key\":\"828c6985-c2d6-419e-a124-ba99147b9920\"},{\"id\":\"38aa33fe-39e5-4ee3-97ba-4227cfac63f6\",\"name\":\"thing_name_2\",\"key\":\"f73e7342-06c1-499a-9584-35de495aa338\"}]}","title":"Get All Things"},{"location":"api/#update-thing","text":"Updating a thing entity Must-have: user_token and thing_id curl -s -S -i -X PUT -H \"Content-Type: application/json\" -H \"Authorization: Bearer <user_token>\" http://localhost/things/<thing_id> -d '{\"name\": \"<thing_name>\"}' Response: HTTP/1.1 200 OK Server: nginx/1.16.0 Date: Wed, 10 Mar 2021 15:23:36 GMT Content-Type: application/json Content-Length: 0 Connection: keep-alive Access-Control-Expose-Headers: Location","title":"Update Thing"},{"location":"api/#delete-thing","text":"To delete a thing you need a thing_id and a user_token Must-have: user_token and thing_id curl -s -S -i -X DELETE -H \"Content-Type: application/json\" -H \"Authorization: Bearer <user_token>\" http://localhost/things/<thing_id> Response: HTTP/1.1 204 No Content Server: nginx/1.16.0 Date: Wed, 10 Mar 2021 15:24:44 GMT Content-Type: application/json Connection: keep-alive Access-Control-Expose-Headers: Location","title":"Delete Thing"},{"location":"api/#channels","text":"","title":"Channels"},{"location":"api/#create-channel","text":"To create a channel, you need a user_token Must-have: user_token curl -s -S -i -X POST -H \"Content-Type: application/json\" -H \"Authorization: Bearer <user_token>\" http://localhost/channels -d '{\"name\": \"<channel_name>\"}' Response: HTTP/1.1 201 Created Server: nginx/1.16.0 Date: Wed, 10 Mar 2021 15:26:51 GMT Content-Type: application/json Content-Length: 0 Connection: keep-alive Location: /channels/db4b7428-e278-4fe3-b85a-d65554d6abe9 Warning-Deprecated: This endpoint will be depreciated in v1.0.0. It will be replaced with the bulk endpoint currently found at /channels/bulk. Access-Control-Expose-Headers: Location","title":"Create Channel"},{"location":"api/#create-channel-with-external-id","text":"Channel is a group of things that could represent a special category in existing systems, e.g. a building level channel could represent the level of a smarting building system. For helping to keep the reference, it is possible to set an existing ID while creating the Mainflux channel. There are two limitations - the existing ID has to be in UUID V4 format and it has to be unique in the Mainflux domain. To create a channel with external ID, the user needs provide a UUID v4 format unique ID, and a user_token Must-have: user_token curl -s -S -i -X POST -H \"Content-Type: application/json\" -H \"Authorization: Bearer <user_token>\" http://localhost/channels -d '{\"id\": \"<xxxxxxxx-xxxx-xxxx-xxxxxxxxxxxxxxxx>\",\"name\": \"<channel_name>\"}' Response: HTTP/1.1 201 Created Server: nginx/1.16.0 Date: Wed, 10 Mar 2021 15:26:51 GMT Content-Type: application/json Content-Length: 0 Connection: keep-alive Location: /channels/db4b7428-e278-4fe3-b85a-d65554d6abe9 Warning-Deprecated: This endpoint will be depreciated in v1.0.0. It will be replaced with the bulk endpoint currently found at /channels/bulk. Access-Control-Expose-Headers: Location","title":"Create Channel with external ID"},{"location":"api/#create-channels","text":"The same as creating a channel with external ID the user can create multiple channels at once by providing UUID v4 format unique ID in a series of channels together with a user_token Must-have: user_token and at least 2 channels curl -s -S -i -X POST -H \"Content-Type: application/json\" -H \"Authorization: Bearer <user_token>\" http://localhost/channels/bulk -d '[{\"name\": \"<channel_name_1>\"}, {\"name\": \"<channel_name_2>\"}]' Response: HTTP/1.1 201 Created Server: nginx/1.16.0 Date: Wed, 10 Mar 2021 15:28:10 GMT Content-Type: application/json Content-Length: 143 Connection: keep-alive Access-Control-Expose-Headers: Location {\"channels\":[{\"id\":\"b8073d41-01dc-46ad-bb26-cfecc596c6c1\",\"name\":\"channel_name_1\"},{\"id\":\"2200527a-f590-4fe5-b9d6-892fc6f825c3\",\"name\":\"channel_name_2\"}]}","title":"Create Channels"},{"location":"api/#create-channels-with-external-id","text":"As with things, you can create multiple channels with external ID at once Must-have: user_token and at least 2 channels curl -s -S -i -X POST -H \"Content-Type: application/json\" -H \"Authorization: Bearer <user_token>\" http://localhost/channels/bulk -d '[{\"id\": \"<xxxxxxxx-xxxx-xxxx-xxxxxxxxxxxxxxx1>\",\"name\": \"<channel_name_1>\"}, {\"id\": \"<xxxxxxxx-xxxx-xxxx-xxxxxxxxxxxxxxx2>\",\"name\": \"<channel_name_2>\"}]' Response: HTTP/1.1 201 Created Server: nginx/1.16.0 Date: Wed, 10 Mar 2021 15:28:10 GMT Content-Type: application/json Content-Length: 143 Connection: keep-alive Access-Control-Expose-Headers: Location {\"channels\":[{\"id\":\"<xxxxxxxx-xxxx-xxxx-xxxxxxxxxxxxxxx1>\",\"name\":\"channel_name_1\"},{\"id\":\"<xxxxxxxx-xxxx-xxxx-xxxxxxxxxxxxxxx2>\",\"name\":\"channel_name_2\"}]}","title":"Create Channels with external ID"},{"location":"api/#get-channel","text":"Get a channel entity for a logged in user Must-have: user_token and channel_id curl -s -S -i -X GET -H \"Authorization: Bearer <user_token>\" http://localhost/channels/<channel_id> Response: HTTP/1.1 200 OK Server: nginx/1.16.0 Date: Wed, 10 Mar 2021 15:29:49 GMT Content-Type: application/json Content-Length: 63 Connection: keep-alive Access-Control-Expose-Headers: Location {\"id\":\"db4b7428-e278-4fe3-b85a-d65554d6abe9\",\"name\":\"channel_name\"}","title":"Get Channel"},{"location":"api/#get-channels","text":"Get all channels, list requests accepts limit and offset query parameters Must-have: user_token curl -s -S -i -X GET -H \"Authorization: Bearer <user_token>\" http://localhost/channels Response: HTTP/1.1 200 OK Server: nginx/1.16.0 Date: Wed, 10 Mar 2021 15:30:34 GMT Content-Type: application/json Content-Length: 264 Connection: keep-alive Access-Control-Expose-Headers: Location {\"total\":3,\"offset\":0,\"limit\":10,\"order\":\"\",\"direction\":\"\",\"channels\":[{\"id\":\"db4b7428-e278-4fe3-b85a-d65554d6abe9\",\"name\":\"channel_name\"},{\"id\":\"b8073d41-01dc-46ad-bb26-cfecc596c6c1\",\"name\":\"channel_name_1\"},{\"id\":\"2200527a-f590-4fe5-b9d6-892fc6f825c3\",\"name\":\"channel_name_2\"}]}","title":"Get Channels"},{"location":"api/#update-channel","text":"Update channel entity Must-have: user_token and channel_id curl -s -S -i -X PUT -H \"Content-Type: application/json\" -H \"Authorization: Bearer <user_token>\" http://localhost/channels/<channel_id> -d '{\"name\": \"<channel_name>\"}' Response: HTTP/1.1 200 OK Server: nginx/1.16.0 Date: Wed, 10 Mar 2021 15:32:08 GMT Content-Type: application/json Content-Length: 0 Connection: keep-alive Access-Control-Expose-Headers: Location","title":"Update Channel"},{"location":"api/#delete-channel","text":"Delete a channel entity Must-have: user_token and channel_id curl -s -S -i -X DELETE -H \"Content-Type: application/json\" -H \"Authorization: Bearer <user_token>\" http://localhost/channels/<channel_id> Response: HTTP/1.1 204 No Content Server: nginx/1.16.0 Date: Wed, 10 Mar 2021 15:33:21 GMT Content-Type: application/json Connection: keep-alive Access-Control-Expose-Headers: Location","title":"Delete Channel"},{"location":"api/#connect","text":"Connect things to channels Must-have: user_token , channel_id and thing_id curl -s -S -i -X POST -H \"Content-Type: application/json\" -H \"Authorization: Bearer <user_token>\" http://localhost/connect -d '{\"channel_ids\": [\"<channel_id>\"], \"thing_ids\": [\"<thing_id>\"]}' Response: HTTP/1.1 200 OK Server: nginx/1.16.0 Date: Wed, 10 Mar 2021 15:36:32 GMT Content-Type: application/json Content-Length: 0 Connection: keep-alive Access-Control-Expose-Headers: Location Connect thing to channel Must-have: user_token , channel_id and thing_id curl -s -S -i -X PUT -H \"Content-Type: application/json\" -H \"Authorization: Bearer <user_token>\" http://localhost/channels/<channel_id>/things/<thing_id> Response: HTTP/1.1 200 OK Server: nginx/1.20.0 Date: Fri, 21 Jan 2022 15:20:47 GMT Content-Type: application/json Content-Length: 0 Connection: keep-alive Warning-Deprecated: This endpoint will be depreciated in v1.0.0. It will be replaced with the bulk endpoint found at /connect. Access-Control-Expose-Headers: Location","title":"Connect"},{"location":"api/#disconnect","text":"Disconnect things from channels specified by lists of IDs. Must-have: user_token , channel_ids and thing_ids curl -s -S -i -X PUT -H \"Content-Type: application/json\" -H \"Authorization: Bearer <user_token>\" http://localhost:<service_port>/disconnect -d '{\"thing_ids\": [\"<thing_id_1>\", \"<thing_id_2>\"], \"channel_ids\": [\"<channel_id_1>\", \"<channel_id_2>\"]}' Response: HTTP/1.1 200 OK Content-Type: application/json Date: Sun, 11 Jul 2021 17:23:39 GMT Content-Length: 0 Disconnect thing from the channel Must-have: user_token , channel_id and thing_id curl -s -S -i -X DELETE -H \"Content-Type: application/json\" -H \"Authorization: Bearer <user_token>\" http://localhost/channels/<channel_id>/things/<thing_id> Response: HTTP/1.1 204 No Content Server: nginx/1.16.0 Date: Wed, 10 Mar 2021 15:38:14 GMT Content-Type: application/json Connection: keep-alive Access-Control-Expose-Headers: Location","title":"Disconnect"},{"location":"api/#access-by-key","text":"Checks if thing has access to a channel Must-have: channel_id and thing_key curl -s -S -i -X POST -H \"Content-Type: application/json\" http://localhost/identify/channels/<channel_id>/access-by-key -d '{\"token\": \"<thing_key>\"}' Response: HTTP/1.1 200 OK Server: nginx/1.16.0 Date: Mon, 22 Mar 2021 13:10:53 GMT Content-Type: application/json Content-Length: 46 Connection: keep-alive Access-Control-Expose-Headers: Location {\"id\":\"d69d0098-072b-41bf-8c6e-ce4dbb12d333\"}","title":"Access by Key"},{"location":"api/#access-by-id","text":"Checks if thing has access to a channel Must-have: channel_id and thing_id curl -s -S -i -X POST -H \"Content-Type: application/json\" http://localhost/identify/channels/<channel_id>/access-by-id -d '{\"thing_id\": \"<thing_id>\"}' Response: HTTP/1.1 200 OK Server: nginx/1.16.0 Date: Mon, 22 Mar 2021 15:02:02 GMT Content-Type: application/json Content-Length: 0 Connection: keep-alive Access-Control-Expose-Headers: Location","title":"Access by ID"},{"location":"api/#identify","text":"Validates thing's key and returns it's ID if key is valid Must-have: thing_key curl -s -S -i -X POST -H \"Content-Type: application/json\" http://localhost/identify -d '{\"token\": \"<thing_key>\"}' Response: HTTP/1.1 200 OK Server: nginx/1.16.0 Date: Mon, 22 Mar 2021 15:04:41 GMT Content-Type: application/json Content-Length: 46 Connection: keep-alive Access-Control-Expose-Headers: Location {\"id\":\"d69d0098-072b-41bf-8c6e-ce4dbb12d333\"}","title":"Identify"},{"location":"api/#messages","text":"","title":"Messages"},{"location":"api/#send-messages","text":"Sends message via HTTP protocol Must-have: thing_key and channel_id curl -s -S -i -X POST -H \"Content-Type: application/json\" -H \"Authorization: Thing <thing_key>\" http://localhost/http/channels/<channel_id>/messages -d '[{\"bn\":\"some-base-name:\",\"bt\":1.276020076001e+09,\"bu\":\"A\",\"bver\":5,\"n\":\"voltage\",\"u\":\"V\",\"v\":120.1}, {\"n\":\"current\",\"t\":-5,\"v\":1.2}, {\"n\":\"current\",\"t\":-4,\"v\":1.3}]' Response: HTTP/1.1 202 Accepted Server: nginx/1.16.0 Date: Wed, 10 Mar 2021 16:53:54 GMT Content-Length: 0 Connection: keep-alive","title":"Send Messages"},{"location":"api/#read-messages","text":"Reads messages from database for a given channel Must-have: thing_key and channel_id curl -s -S -i -H \"Authorization: Thing <thing_key>\" http://localhost:<service_port>/channels/<channel_id>/messages?offset=0&limit=5 Response: HTTP/1.1 200 OK Content-Type: application/json Date: Wed, 10 Mar 2021 16:54:58 GMT Content-Length: 660 {\"offset\":0,\"limit\":10,\"format\":\"messages\",\"total\":3,\"messages\":[{\"channel_name\":\"1a0cde06-8e5c-4f07-aac5-95aff4a19ea0\",\"publisher\":\"33eb28c3-4ca2-45c3-b1c5-d5d049c6c24e\",\"protocol\":\"http\",\"name\":\"some-base-name:voltage\",\"unit\":\"V\",\"time\":1276020076.001,\"value\":120.1},{\"channel_name\":\"1a0cde06-8e5c-4f07-aac5-95aff4a19ea0\",\"publisher\":\"33eb28c3-4ca2-45c3-b1c5-d5d049c6c24e\",\"protocol\":\"http\",\"name\":\"some-base-name:current\",\"unit\":\"A\",\"time\":1276020072.001,\"value\":1.3},{\"channel_name\":\"1a0cde06-8e5c-4f07-aac5-95aff4a19ea0\",\"publisher\":\"33eb28c3-4ca2-45c3-b1c5-d5d049c6c24e\",\"protocol\":\"http\",\"name\":\"some-base-name:current\",\"unit\":\"A\",\"time\":1276020071.001,\"value\":1.2}]}","title":"Read Messages"},{"location":"api/#groups","text":"","title":"Groups"},{"location":"api/#create-group","text":"To create a group, you need the group name and a user_token Must-have: user_token curl -s -S -i -X POST -H \"Content-Type: application/json\" -H \"Authorization: Bearer <user_token>\" http://localhost/groups -d '{\"name\": \"<group_name>\", \"parent_id\": \"<previous_group_id>\", \"description\": \"<group_description>\", \"metadata\": {}}' Response: HTTP/1.1 201 Created Server: nginx/1.16.0 Date: Wed, 10 Mar 2021 16:58:09 GMT Content-Type: application/json Content-Length: 0 Connection: keep-alive Location: /groups/01F0EH61SA7C7NDKWYCXVG7PWD Access-Control-Expose-Headers: Location","title":"Create group"},{"location":"api/#members","text":"Get list of ID's from group Must-have: user_token and group_id curl -s -S -i -X GET -H 'Content-Type: application/json' -H \"Authorization: Bearer <user_token>\" http://localhost/groups/<group_id>/members Response: HTTP/1.1 200 OK Server: nginx/1.16.0 Date: Tue, 23 Mar 2021 09:18:10 GMT Content-Type: application/json Content-Length: 116 Connection: keep-alive Access-Control-Expose-Headers: Location {\"limit\":10,\"total\":0,\"level\":0,\"name\":\"\",\"Members\":[{\"ID\":\"d782b42b-e317-4cd7-9dd0-4e2ea0f349c8\",\"Type\":\"users\"}]}","title":"Members"},{"location":"api/#assign","text":"Assign user, thing or channel to a group Must-have: user_token , group_id , member_id and member_type curl -s -S -i -X POST -H \"Content-Type: application/json\" -H \"Authorization: Bearer <user_token>\" http://localhost/groups/<group_id>/members -d '{\"members\":[\"<user_id>\" | \"<thing_id_>\" | \"<channel_id_>\"], \"type\":[\"users\" | \"things\" | \"channels\"]}' Response: HTTP/1.1 200 OK Server: nginx/1.16.0 Date: Wed, 10 Mar 2021 17:04:41 GMT Content-Type: application/json Content-Length: 0 Connection: keep-alive Access-Control-Expose-Headers: Location","title":"Assign"},{"location":"api/#unassign","text":"Unassign user, thing or channel from group Must-have: user_token , group_id , member_id and member_type curl -s -S -i -X DELETE -H \"Content-Type: application/json\" -H \"Authorization: Bearer <user_token>\" http://localhost/groups/<group_id>/members -d '{\"members\":[\"<user_id>\" | \"<thing_id_>\" | \"<channel_id_>\"], \"type\":[\"users\" | \"things\" | \"channels\"]}' Response: HTTP/1.1 204 No Content Server: nginx/1.16.0 Date: Wed, 10 Mar 2021 17:13:06 GMT Content-Type: application/json Connection: keep-alive Access-Control-Expose-Headers: Location","title":"Unassign"},{"location":"api/#get-group","text":"Get a group entity for a logged in user Must-have: user_token and group_id curl -s -S -i -X GET -H \"Content-Type: application/json\" -H \"Authorization: Bearer <user_token>\" http://localhost/groups/<group_id> Response: HTTP/1.1 200 OK Server: nginx/1.16.0 Date: Wed, 10 Mar 2021 17:06:48 GMT Content-Type: application/json Content-Length: 201 Connection: keep-alive Access-Control-Expose-Headers: Location {\"id\":\"01F0EH61SA7C7NDKWYCXVG7PWD\",\"name\":\"group_name\",\"owner_id\":\"d782b42b-e317-4cd7-9dd0-4e2ea0f349c8\",\"level\":0,\"path\":\"\",\"created_at\":\"2021-03-10T16:58:09.579Z\",\"updated_at\":\"2021-03-10T16:58:09.579Z\"}","title":"Get group"},{"location":"api/#get-groups","text":"Get all groups, list requests accepts limit and offset query parameters Must-have: user_token curl -s -S -i -X GET -H \"Content-Type: application/json\" -H \"Authorization: Bearer <user_token>\" http://localhost/groups Response: HTTP/1.1 200 OK Server: nginx/1.16.0 Date: Wed, 10 Mar 2021 17:09:28 GMT Content-Type: application/json Content-Length: 496 Connection: keep-alive Access-Control-Expose-Headers: Location {\"total\":2,\"level\":0,\"name\":\"\",\"groups\":[{\"id\":\"01F0EH61SA7C7NDKWYCXVG7PWD\",\"name\":\"group_name\",\"owner_id\":\"d782b42b-e317-4cd7-9dd0-4e2ea0f349c8\",\"level\":1,\"path\":\"01F0EH61SA7C7NDKWYCXVG7PWD\",\"created_at\":\"2021-03-10T16:58:09.579Z\",\"updated_at\":\"2021-03-10T16:58:09.579Z\"},{\"id\":\"01F0EHQTP2HQ7JTWZNMVJ0JJCN\",\"name\":\"group_name_1\",\"owner_id\":\"d782b42b-e317-4cd7-9dd0-4e2ea0f349c8\",\"level\":1,\"path\":\"01F0EHQTP2HQ7JTWZNMVJ0JJCN\",\"created_at\":\"2021-03-10T17:07:52.13Z\",\"updated_at\":\"2021-03-10T17:07:52.13Z\"}]}","title":"Get groups"},{"location":"api/#update-group","text":"Update group entity Must-have: user_token and group_id curl -s -S -i -X PUT -H \"Content-Type: application/json\" -H \"Authorization: Bearer <user_token>\" http://localhost/groups/<group_id> -d '{\"name\": \"<group_name>\"}' Response: HTTP/1.1 200 OK Server: nginx/1.16.0 Date: Wed, 10 Mar 2021 17:11:51 GMT Content-Type: application/json Content-Length: 0 Connection: keep-alive Access-Control-Expose-Headers: Location","title":"Update group"},{"location":"api/#delete-group","text":"Delete a group entity Must-have: user_token and group_id curl -s -S -i -X DELETE -H \"Content-Type: application/json\" -H \"Authorization: Bearer <user_token>\" http://localhost/groups/<group_id> Response: HTTP/1.1 204 No Content Server: nginx/1.16.0 Date: Wed, 10 Mar 2021 17:14:13 GMT Content-Type: application/json Connection: keep-alive Access-Control-Expose-Headers: Location","title":"Delete group"},{"location":"api/#share-user-group-with-things-group","text":"Adds access rights on thing groups to the user group. Must-have: user_token , user_group_id and <thing_group_id> . curl -s -S -i -X POST http://localhost/groups/<user_group_id>/share -d '{\"thing_group_id\": \"<thing_group_id>\"}' -H 'Content-Type: application/json' -H \"Authorization: Bearer <user_token>\" Each user from the group identified by user_group_id will have read , write , and delete policies on the things grouped by thing_group_id . Therefore, they will be able to do operations defined under Things Policies section .","title":"Share User Group with Things Group"},{"location":"api/#policies","text":"","title":"Policies"},{"location":"api/#add-policies","text":"The admin can add custom policies. Only policies defined on Predefined Policies section are allowed. Must-have: admin_token, object, subjects_ids and policies curl -isSX POST http://localhost/policies -d '{\"subjects\": [\"<subject_id1>\",...\"<subject_idN>\"], \"object\": \"<object>\", \"policies\": [\"<action_1>, ...\"<action_N>\"]}' -H \"Authorization: Bearer <admin_token>\" -H 'Content-Type: application/json' admin_token must belong to the admin. Response: HTTP/1.1 201 Created Content-Type: application/json Date: Wed, 03 Nov 2021 13:00:14 GMT Content-Length: 3 {}","title":"Add policies"},{"location":"api/#delete-policies","text":"The admin can delete policies. Only policies defined on Predefined Policies section are allowed. Must-have: admin_token, object, subjects_ids and policies curl -isSX PUT http://localhost/policies -d '{\"subjects\": [\"<subject_id1>\",...\"<subject_idN>\"], \"object\": \"<object>\", \"policies\": [\"<action_1>, ...\"<action_N>\"]}' -H \"Authorization: Bearer <admin_token>\" -H 'Content-Type: application/json' admin_token must belong to the admin. Response: HTTP/1.1 204 No Content Content-Type: application/json Date: Wed, 03 Nov 2021 13:00:05 GMT","title":"Delete policies"},{"location":"api/#api-key","text":"","title":"API Key"},{"location":"api/#issue-api-key","text":"Generates a new API key. Then new API key will be uniquely identified by its ID. Duration is expressed in seconds. Must-have: user_token curl -isSX POST http://localhost/keys -H \"Content-Type: application/json\" -H \"Authorization: Bearer <user_token>\" -d '{\"type\":2, \"duration\":10000}' Response: HTTP/1.1 201 Created Server: nginx/1.20.0 Date: Sun, 19 Dec 2021 17:39:44 GMT Content-Type: application/json Content-Length: 476 Connection: keep-alive Access-Control-Expose-Headers: Location {\"id\":\"4d62fb1e-085e-435c-a0c5-5255febfa35b\",\"value\":\"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJleHAiOjE2NDAwMzU1ODQsImp0aSI6IjRkNjJmYjFlLTA4NWUtNDM1Yy1hMGM1LTUyNTVmZWJmYTM1YiIsImlhdCI6MTYzOTkzNTU4NCwiaXNzIjoibWFpbmZsdXguYXV0aCIsInN1YiI6ImZscDFAZW1haWwuY29tIiwiaXNzdWVyX2lkIjoiYzkzY2FmYjMtYjNhNy00ZTdmLWE0NzAtMTVjMTRkOGVkMWUwIiwidHlwZSI6Mn0.RnvjhygEPPWFDEUKtfk5okzVhZzOcO0azr8gd5vby5M\",\"issued_at\":\"2021-12-19T17:39:44.175088349Z\",\"expires_at\":\"2021-12-20T21:26:24.175088349Z\"}","title":"Issue API Key"},{"location":"api/#get-api-key-details","text":"Must-have: 'user_token' and 'key_id' curl -isSX GET http://localhost/keys/<key_id> -H 'Content-Type: application/json' -H 'Authorization: Bearer <user_token>' Response: HTTP/1.1 200 OK Server: nginx/1.20.0 Date: Sun, 19 Dec 2021 17:43:30 GMT Content-Type: application/json Content-Length: 218 Connection: keep-alive Access-Control-Expose-Headers: Location {\"id\":\"f630f594-d967-4c54-85ef-af58efe8e8ed\",\"issuer_id\":\"c93cafb3-b3a7-4e7f-a470-15c14d8ed1e0\",\"subject\":\"test@email.com\",\"type\":2,\"issued_at\":\"2021-12-19T17:42:40.884521Z\",\"expires_at\":\"2021-12-20T21:29:20.884521Z\"}","title":"Get API key details"},{"location":"api/#revoke-api-key-identified-by-the-given-id","text":"Must-have: 'user_token' and 'key_id' curl -isSX DELETE http://localhost/keys/<key_id> -H 'Content-Type: application/json' -H 'Authorization: Bearer <user_token>' Response: HTTP/1.1 204 No Content Server: nginx/1.20.0 Date: Sun, 19 Dec 2021 17:47:11 GMT Content-Type: application/json Connection: keep-alive Access-Control-Expose-Headers: Location","title":"Revoke API key identified by the given ID"},{"location":"architecture/","text":"Architecture # Components # Mainflux IoT platform is comprised of the following services: Service Description users Manages platform's users and auth concerns things Manages platform's things, channels and access policies http-adapter Provides an HTTP interface for accessing communication channels mqtt-adapter Provides an MQTT and MQTT over WS interface for accessing communication channels coap-adapter Provides a CoAP interface for accessing communication channels opcua-adapter Provides an OPC-UA interface for accessing communication channels lora-adapter Provides a LoRa Server forwarder for accessing communication channels mainflux-cli Command line interface Domain Model # The platform is built around 3 main entities: users , things and channels . User represents the real (human) user of the system. It is represented via its e-mail and password, which he uses as platform access credentials in order to obtain an access token. Once logged into the system, user can manage his resources (i.e. things and channels) in CRUD fashion and define access control policies by connecting them. Thing represents devices (or applications) connected to Mainflux that uses the platform for message exchange with other \"things\". Channel represents a communication channel. It serves as message topic that can be consumed by all of the things connected to it. Messaging # Mainflux uses NATS as its messaging backbone, due to its lightweight and performant nature. You can treat its subjects as physical representation of Mainflux channels, where subject name is constructed using channel unique identifier. In general, there is no constrained put on content that is being exchanged through channels. However, in order to be post-processed and normalized, messages should be formatted using SenML . Edge # Mainflux platform can be run on the edge as well. Deploying Mainflux on a gateway makes it able to collect, store and analyze data, organize and authenticate devices. To connect Mainflux instances running on a gateway with Mainflux in a cloud we can use two gateway services developed for that purpose: Agent Export Unified IoT Platform # Running Mainflux on gateway moves computation from cloud towards the edge thus decentralizing IoT system. Since we can deploy same Mainflux code on gateway and in the cloud there are many benefits but the biggest one is easy deployment and adoption - once the engineers understand how to deploy and maintain the platform, they will have the same known work across the whole edge-fog-cloud continuum. Same set of tools can be used, same patches and bug fixes can be applied. The whole system is much easier to reason about, and the maintenance is much easier and less costly.","title":"Architecture"},{"location":"architecture/#architecture","text":"","title":"Architecture"},{"location":"architecture/#components","text":"Mainflux IoT platform is comprised of the following services: Service Description users Manages platform's users and auth concerns things Manages platform's things, channels and access policies http-adapter Provides an HTTP interface for accessing communication channels mqtt-adapter Provides an MQTT and MQTT over WS interface for accessing communication channels coap-adapter Provides a CoAP interface for accessing communication channels opcua-adapter Provides an OPC-UA interface for accessing communication channels lora-adapter Provides a LoRa Server forwarder for accessing communication channels mainflux-cli Command line interface","title":"Components"},{"location":"architecture/#domain-model","text":"The platform is built around 3 main entities: users , things and channels . User represents the real (human) user of the system. It is represented via its e-mail and password, which he uses as platform access credentials in order to obtain an access token. Once logged into the system, user can manage his resources (i.e. things and channels) in CRUD fashion and define access control policies by connecting them. Thing represents devices (or applications) connected to Mainflux that uses the platform for message exchange with other \"things\". Channel represents a communication channel. It serves as message topic that can be consumed by all of the things connected to it.","title":"Domain Model"},{"location":"architecture/#messaging","text":"Mainflux uses NATS as its messaging backbone, due to its lightweight and performant nature. You can treat its subjects as physical representation of Mainflux channels, where subject name is constructed using channel unique identifier. In general, there is no constrained put on content that is being exchanged through channels. However, in order to be post-processed and normalized, messages should be formatted using SenML .","title":"Messaging"},{"location":"architecture/#edge","text":"Mainflux platform can be run on the edge as well. Deploying Mainflux on a gateway makes it able to collect, store and analyze data, organize and authenticate devices. To connect Mainflux instances running on a gateway with Mainflux in a cloud we can use two gateway services developed for that purpose: Agent Export","title":"Edge"},{"location":"architecture/#unified-iot-platform","text":"Running Mainflux on gateway moves computation from cloud towards the edge thus decentralizing IoT system. Since we can deploy same Mainflux code on gateway and in the cloud there are many benefits but the biggest one is easy deployment and adoption - once the engineers understand how to deploy and maintain the platform, they will have the same known work across the whole edge-fog-cloud continuum. Same set of tools can be used, same patches and bug fixes can be applied. The whole system is much easier to reason about, and the maintenance is much easier and less costly.","title":"Unified IoT Platform"},{"location":"authentication/","text":"Authentication # User authentication # For user authentication Mainflux uses Authentication keys. There are three types of authentication keys: User key - keys issued to the user upon login request API key - keys issued upon the user request Recovery key - password recovery key Authentication keys are represented and distributed by the corresponding JWT . User keys are issued when user logs in. Each user request (other than registration and login) contains user key that is used to authenticate the user. API keys are similar to the User keys. The main difference is that API keys have configurable expiration time. If no time is set, the key will never expire. API keys are the only key type that can be revoked . This also means that, despite being used as a JWT, it requires a query to the database to validate the API key. The user with API key can perform all the same actions as the user with login key (can act on behalf of the user for Thing, Channel, or user profile management), except issuing new API keys . Recovery key is the password recovery key. It's short-lived token used for password recovery process. The following actions are supported: create (all key types) verify (all key types) obtain (API keys only; secret is never obtained) revoke (API keys only) Authentication with Mainflux keys # By default, Mainflux uses Mainflux Thing keys for authentication. The Thing key is a secret key that's generated at the Thing creation. In order to authenticate, the Thing needs to send its key with the message. The way the key is passed depends on the protocol used to send a message and differs from adapter to adapter. For more details on how this key is passed around, please check out messaging section . This is the default Mainflux authentication mechanism and this method is used if the composition is started using the following command: docker-compose -f docker/docker-compose.yml up Mutual TLS Authentication with X.509 Certificates # In most of the cases, HTTPS, MQTTS or secure CoAP are secure enough. However, sometimes you might need an even more secure connection. Mainflux supports mutual TLS authentication ( mTLS ) based on X.509 certificates . By default, the TLS protocol only proves the identity of the server to the client using the X.509 certificate and the authentication of the client to the server is left to the application layer. TLS also offers client-to-server authentication using client-side X.509 authentication. This is called two-way or mutual authentication. Mainflux currently supports mTLS over HTTP, MQTT and MQTT over WS protocols. In order to run Docker composition with mTLS turned on, you can execute the following command from the project root: AUTH=x509 docker-compose -f docker/docker-compose.yml up -d Mutual authentication includes client-side certificates. Certificates can be generated using the simple script provided here . In order to create a valid certificate, you need to create Mainflux thing using the process described in the provisioning section . After that, you need to fetch created thing key. Thing key will be used to create x.509 certificate for the corresponding thing. To create a certificate, execute the following commands: cd docker/ssl make ca CN=<common_name> O=<organization> OU=<organizational_unit> emailAddress=<email_address> make server_cert CN=<common_name> O=<organization> OU=<organizational_unit> emailAddress=<email_address> make thing_cert THING_KEY=<thing_key> CRT_FILE_NAME=<cert_name> O=<organization> OU=<organizational_unit> emailAddress=<email_address> These commands use OpenSSL tool, so please make sure that you have it installed and set up before running these commands. The default values for Makefile variables are CRT_LOCATION = certs THING_KEY = d7cc2964-a48b-4a6e-871a-08da28e7883d O = Mainflux OU = mainflux EA = info@mainflux.com CN = localhost CRT_FILE_NAME = thing Normally, in order to get things running, you will need to specify only THING_KEY . The other variables are not mandatory and the termination should work with the default values. Command make ca will generate a self-signed certificate that will later be used as a CA to sign other generated certificates. CA will expire in 3 years. Command make server_cert will generate and sign (with previously created CA) server cert, which will expire after 1000 days. This cert is used as a Mainflux server-side certificate in usual TLS flow to establish HTTPS or MQTTS connection. Command make thing_cert will finally generate and sign a client-side certificate and private key for the thing. In this example <thing_key> represents key of the thing and <cert_name> represents the name of the certificate and key file which will be saved in docker/ssl/certs directory. Generated Certificate will expire after 2 years. The key must be stored in the x.509 certificate CN field. This script is created for testing purposes and is not meant to be used in production. We strongly recommend avoiding self-signed certificates and using a certificate management tool such as Vault for the production. Once you have created CA and server-side cert, you can spin the composition using: AUTH=x509 docker-compose -f docker/docker-compose.yml up -d Then, you can create user and provision things and channels. Now, in order to send a message from the specific thing to the channel, you need to connect thing to the channel and generate corresponding client certificate using aforementioned commands. To publish a message to the channel, thing should send following request: HTTPS # curl -s -S -i --cacert docker/ssl/certs/ca.crt --cert docker/ssl/certs/<thing_cert_name>.crt --key docker/ssl/certs/<thing_cert_key>.key -X POST -H \"Content-Type: application/senml+json\" https://localhost/http/channels/<channel_id>/messages -d '[{\"bn\":\"some-base-name:\",\"bt\":1.276020076001e+09, \"bu\":\"A\",\"bver\":5, \"n\":\"voltage\",\"u\":\"V\",\"v\":120.1}, {\"n\":\"current\",\"t\":-5,\"v\":1.2}, {\"n\":\"current\",\"t\":-4,\"v\":1.3}]' MQTTS # Publish # mosquitto_pub -u <thing_id> -P <thing_key> -t channels/<channel_id>/messages -h localhost -p 8883 --cafile docker/ssl/certs/ca.crt --cert docker/ssl/certs/<thing_cert_name>.crt --key docker/ssl/certs/<thing_cert_key>.key -m '[{\"bn\":\"some-base-name:\",\"bt\":1.276020076001e+09, \"bu\":\"A\",\"bver\":5, \"n\":\"voltage\",\"u\":\"V\",\"v\":120.1}, {\"n\":\"current\",\"t\":-5,\"v\":1.2}, {\"n\":\"current\",\"t\":-4,\"v\":1.3}]' Subscribe # mosquitto_sub -u <thing_id> -P <thing_key> --cafile docker/ssl/certs/ca.crt --cert docker/ssl/certs/<thing_cert_name>.crt --key docker/ssl/certs/<thing_cert_key>.key -t channels/<channel_id>/messages -h localhost -p 8883","title":"Authentication"},{"location":"authentication/#authentication","text":"","title":"Authentication"},{"location":"authentication/#user-authentication","text":"For user authentication Mainflux uses Authentication keys. There are three types of authentication keys: User key - keys issued to the user upon login request API key - keys issued upon the user request Recovery key - password recovery key Authentication keys are represented and distributed by the corresponding JWT . User keys are issued when user logs in. Each user request (other than registration and login) contains user key that is used to authenticate the user. API keys are similar to the User keys. The main difference is that API keys have configurable expiration time. If no time is set, the key will never expire. API keys are the only key type that can be revoked . This also means that, despite being used as a JWT, it requires a query to the database to validate the API key. The user with API key can perform all the same actions as the user with login key (can act on behalf of the user for Thing, Channel, or user profile management), except issuing new API keys . Recovery key is the password recovery key. It's short-lived token used for password recovery process. The following actions are supported: create (all key types) verify (all key types) obtain (API keys only; secret is never obtained) revoke (API keys only)","title":"User authentication"},{"location":"authentication/#authentication-with-mainflux-keys","text":"By default, Mainflux uses Mainflux Thing keys for authentication. The Thing key is a secret key that's generated at the Thing creation. In order to authenticate, the Thing needs to send its key with the message. The way the key is passed depends on the protocol used to send a message and differs from adapter to adapter. For more details on how this key is passed around, please check out messaging section . This is the default Mainflux authentication mechanism and this method is used if the composition is started using the following command: docker-compose -f docker/docker-compose.yml up","title":"Authentication with Mainflux keys"},{"location":"authentication/#mutual-tls-authentication-with-x509-certificates","text":"In most of the cases, HTTPS, MQTTS or secure CoAP are secure enough. However, sometimes you might need an even more secure connection. Mainflux supports mutual TLS authentication ( mTLS ) based on X.509 certificates . By default, the TLS protocol only proves the identity of the server to the client using the X.509 certificate and the authentication of the client to the server is left to the application layer. TLS also offers client-to-server authentication using client-side X.509 authentication. This is called two-way or mutual authentication. Mainflux currently supports mTLS over HTTP, MQTT and MQTT over WS protocols. In order to run Docker composition with mTLS turned on, you can execute the following command from the project root: AUTH=x509 docker-compose -f docker/docker-compose.yml up -d Mutual authentication includes client-side certificates. Certificates can be generated using the simple script provided here . In order to create a valid certificate, you need to create Mainflux thing using the process described in the provisioning section . After that, you need to fetch created thing key. Thing key will be used to create x.509 certificate for the corresponding thing. To create a certificate, execute the following commands: cd docker/ssl make ca CN=<common_name> O=<organization> OU=<organizational_unit> emailAddress=<email_address> make server_cert CN=<common_name> O=<organization> OU=<organizational_unit> emailAddress=<email_address> make thing_cert THING_KEY=<thing_key> CRT_FILE_NAME=<cert_name> O=<organization> OU=<organizational_unit> emailAddress=<email_address> These commands use OpenSSL tool, so please make sure that you have it installed and set up before running these commands. The default values for Makefile variables are CRT_LOCATION = certs THING_KEY = d7cc2964-a48b-4a6e-871a-08da28e7883d O = Mainflux OU = mainflux EA = info@mainflux.com CN = localhost CRT_FILE_NAME = thing Normally, in order to get things running, you will need to specify only THING_KEY . The other variables are not mandatory and the termination should work with the default values. Command make ca will generate a self-signed certificate that will later be used as a CA to sign other generated certificates. CA will expire in 3 years. Command make server_cert will generate and sign (with previously created CA) server cert, which will expire after 1000 days. This cert is used as a Mainflux server-side certificate in usual TLS flow to establish HTTPS or MQTTS connection. Command make thing_cert will finally generate and sign a client-side certificate and private key for the thing. In this example <thing_key> represents key of the thing and <cert_name> represents the name of the certificate and key file which will be saved in docker/ssl/certs directory. Generated Certificate will expire after 2 years. The key must be stored in the x.509 certificate CN field. This script is created for testing purposes and is not meant to be used in production. We strongly recommend avoiding self-signed certificates and using a certificate management tool such as Vault for the production. Once you have created CA and server-side cert, you can spin the composition using: AUTH=x509 docker-compose -f docker/docker-compose.yml up -d Then, you can create user and provision things and channels. Now, in order to send a message from the specific thing to the channel, you need to connect thing to the channel and generate corresponding client certificate using aforementioned commands. To publish a message to the channel, thing should send following request:","title":"Mutual TLS Authentication with X.509 Certificates"},{"location":"authentication/#https","text":"curl -s -S -i --cacert docker/ssl/certs/ca.crt --cert docker/ssl/certs/<thing_cert_name>.crt --key docker/ssl/certs/<thing_cert_key>.key -X POST -H \"Content-Type: application/senml+json\" https://localhost/http/channels/<channel_id>/messages -d '[{\"bn\":\"some-base-name:\",\"bt\":1.276020076001e+09, \"bu\":\"A\",\"bver\":5, \"n\":\"voltage\",\"u\":\"V\",\"v\":120.1}, {\"n\":\"current\",\"t\":-5,\"v\":1.2}, {\"n\":\"current\",\"t\":-4,\"v\":1.3}]'","title":"HTTPS"},{"location":"authentication/#mqtts","text":"","title":"MQTTS"},{"location":"authentication/#publish","text":"mosquitto_pub -u <thing_id> -P <thing_key> -t channels/<channel_id>/messages -h localhost -p 8883 --cafile docker/ssl/certs/ca.crt --cert docker/ssl/certs/<thing_cert_name>.crt --key docker/ssl/certs/<thing_cert_key>.key -m '[{\"bn\":\"some-base-name:\",\"bt\":1.276020076001e+09, \"bu\":\"A\",\"bver\":5, \"n\":\"voltage\",\"u\":\"V\",\"v\":120.1}, {\"n\":\"current\",\"t\":-5,\"v\":1.2}, {\"n\":\"current\",\"t\":-4,\"v\":1.3}]'","title":"Publish"},{"location":"authentication/#subscribe","text":"mosquitto_sub -u <thing_id> -P <thing_key> --cafile docker/ssl/certs/ca.crt --cert docker/ssl/certs/<thing_cert_name>.crt --key docker/ssl/certs/<thing_cert_key>.key -t channels/<channel_id>/messages -h localhost -p 8883","title":"Subscribe"},{"location":"authorization/","text":"Authorization # Policies # Mainflux uses policies to control permissions on entities: users , things , and groups . Under the hood, Mainflux uses ORY Keto that is an open-source implementation of \"Zanzibar: Google's Consistent, Global Authorization System\" . Policies define permissions for the entities. For example, which user has access to a specific thing . Such policies have three main components: subject , object , and relation . To put it briefly: Subject : As the name suggests, it is the subject that will have the policy such as users . Mainflux uses entity UUID on behalf of the real entities. Object : Objects are Mainflux entities (e.g. thing or group ) represented by their UUID. Relation : This is the action that the subject wants to do on the object. For more conceptual details, you can refer official ORY Keto documentation All three components create a single policy. For example, let's assume we have a following policy: \"user_id_123\" has \"read\" relation on \"thing_id_123\" . This policy means that subject (a user with ID: user_id_123 ) has a relation ( read ) on the object (a thing with ID: thing_id_123 ). Based upon this example, If the user wants to view a Thing , Mainflux first identifies the user with Authentication Keys and checks the policy as: User with ID: `user_id_123` has `read` relation on the thing with ID: `thing_id_123`. If the user has no such policy, the operation will be denied; otherwise, the operation will be allowed. In this case, since the user user_id_123 has the policy, the read operation on the thing thing_id_123 will be allowed for the user with ID user_id_123 . On the other hand, requests coming from other users (who have a different ID than user_id_123 ) will be denied. In order to check whether a user has the policy or not, Mainflux makes a gRPC call to Keto API, then Keto handles the checking existence of the policy. All policies are stored in the Keto Database. The database responsible for storing all policies is deployed along with the Mainflux, as a standalone PostgreSQL database container. Predefined Policies # Mainflux comes with predefined policies. Users service related policies # By default, Mainflux allows anybody to create a user. If you disable this default policy, only admin is able to create a user. This default policy can be disabled through an environment variable called MF_USERS_ALLOW_SELF_REGISTER in deployment time. MF_USERS_ALLOW_SELF_REGISTER is a boolean. Therefore, it expects \"true\" or \"false\" . If you assign \"false\" to this environment variable, only admin can create a user. Mainflux creates a special policy to enable this feature as follows: user#create@* . This policy dictates that subject * has create relation on the object users . Here, Mainflux uses a special * subject to represent all users. If this policy is defined, everybody can create new users. All users are a member of the users . To be more precise, once the new user is created, the policy service creates the following policy: users#member@<user_id> indicating that the subject <user_id > has member relation on the object users . The admin has a special policy indicating that the user is admin. This policy is the following: <admin_id> has member relation on the object authorities . Things service related policies # There are 3 policies regarding Things : read , write and delete . When a user creates a thing, the user will have read , write and delete policies on the Thing . In order to view a thing, you need read policy on that thing. In order to update and share the thing, you need a write policy on that thing. In order to remove a thing, you need a delete policy on that thing. Group entity related policies # Once the user creates a new group, the user will have a member policy on the group. If you assign a new User member to your group, the new user will have a member policy on this particular group. If you assign a new Thing member to your group, whatever has member policy on that group will have read , write and delete policies on the Things defined in the Group. Mainflux allows users to assign access rights of the Things group with the Users group. Thus, each member of the User group can access Things defined in the Thing group. In order to do so, the Policy service adds members of the User group as a member of the Thing Group. Therefore, the Users group members have read , write and delete policy on the Things defined in the Thing Group. Summary of the Defined Policies # member : Identifies registered user's role such as admin . Also, it indicates memberships on the Group entity. read , write and delete : Controls access control for the Things. create : Mainflux uses special create policy to allow everybody to create new users. If you want to enable this feature through the HTTP, you need to make following request: curl -isSX POST http://localhost/policies -d '{\"subjects\":[\"*\"],\"policies\": [\"create\"], \"object\": \"user\"}' -H \"Authorization: Bearer <admin_token>\" -H 'Content-Type: application/json' Add Policies # You can add policies as well through an HTTP endpoint. Only admin can use this endpoint. Therefore, you need an authentication token for the admin. Caveat: Only policies defined under Summary of the Defined Policies are allowed. Other policies are not allowed. For example, you can add member policy but not custom-member policy because custom-member policy is not defined on the system. curl -isSX POST http://localhost/policies -d '{\"subjects\": [\"<subject_id1>\",...\"<subject_idN>\"], \"object\": \"<object>\", \"policies\": [\"<action_1>, ...\"<action_N>\"]}' -H \"Authorization: Bearer <admin_token>\" -H 'Content-Type: application/json' Delete Policies # The admin can delete policies. Only policies defined on Predefined Policies section are allowed. Must-have: admin_token, object, subjects_ids and policies curl -isSX PUT http://localhost/policies -d '{\"subjects\": [\"<subject_id1>\",...\"<subject_idN>\"], \"object\": \"<object>\", \"policies\": [\"<action_1>, ...\"<action_N>\"]}' -H \"Authorization: Bearer <admin_token>\" -H 'Content-Type: application/json' admin_token must belong to the admin. Response: HTTP/1.1 204 No Content Content-Type: application/json Date: Wed, 03 Nov 2021 13:00:05 GMT If you delete policies, the policy will be removed from the policy storage. Further authorization checks related to that policy will fail. For example, let's assume user1 has read policy on the thing thing-123 . If you delete this policy as: curl -isSX PUT http://localhost/policies -d '{\"subjects\": [\"<user1_id>\"], \"object\": \"thing-123\", \"policies\": [\"read\"]}' -H \"Authorization: Bearer <admin_token>\" -H 'Content-Type: application/json' user1 will not be able to view the thing-123 anymore because the policy which allows user1 to view thing-123 is deleted by the admin. Example usage of adding a policy # Suppose we are using the Mainflux version that doesn't have a policies feature yet. Once you migrate a new version of the Mainflux including the Policy feature, your users will face a lack of authorization. For example, there is a user created before the Policy feature. This user is authenticated by <user_token >. Although the following operation is valid, the user will have an authorization error. mainflux-cli things create '{\"name\":\"user-thing\"}' <user_token> error: failed to create entity: 403 Forbidden The reason is that the user has not enough policy to create a new Thing after migration. In order to create a new thing, the user has to have a member relation on the users key. So that, Mainflux understands that the requester user is authorized to create new Things. The easiest solution for this problem is adding policies for the users through the HTTP endpoint. As described above, the user needs a member relation on the users . curl -isSX POST http://localhost/policies -d '{\"subjects\":[\"<user_id>\"],\"policies\": [\"member\"], \"object\": \"users\"}' -H \"Authorization: Bearer <admin_token> \" -H 'Content-Type: application/json' So what this request does is add new policies for the subject defined in the subjects field of the request body. Henceforth, the subject (here <user_id> ) will have a member relation on the object users . This policy allows the user to create new Things. Please, keep in mind that this endpoint requires you to use <admin_token> , not any token. So, the token must belong to the admin. Example usage of sharing a Thing # Let's assume, we have two users (called user1 and user2 ) registered on the system who have user_id_1 and user_id_2 as their ID respectively. Let's create a thing with the following command: mainflux-cli things create '{\"name\":\"user1-thing\"}' <user1_token> created: a1109d52-6281-410e-93ae-38ba7daa9381 This command creates a thing called \"user1-thing\" with ID = a1109d52-6281-410e-93ae-38ba7daa9381 . Mainflux identifies the user1 by using the <user1_token> . After identifying the requester as user1 , the Policy service adds read , write and delete policies to user1 on \"user1-thing\" . If user2 wants to view the \"user1-thing\" , the request will be denied. mainflux-cli things get a1109d52-6281-410e-93ae-38ba7daa9381 <user2_token> error: failed to fetch entity : 403 Forbidden After identifying the requester as user2 , the Policy service checks that Is user2 allowed to view the \"user1-thing\"? Since user2 has no such policy ( read policy on \"user1-thing\" ), the Policy service denies this request. Now, user1 wants to share the \"user1-thing\" with user2 . user1 can achieve this via HTTP endpoint for sharing things as follows: curl -isSX POST http://localhost/things/a1109d52-6281-410e-93ae-38ba7daa9381/share -d '{\"user_ids\":[\"<user2_id>]\", \"policies\": [\"read\", \"delete\"]}' -H \"Authorization: Bearer <user1_token>\" -H 'Content-Type: application/json' HTTP/1.1 200 OK Server: nginx/1.20.0 Date: Thu, 09 Sep 2021 11:36:10 GMT Content-Type: application/json Content-Length: 3 Connection: keep-alive Access-Control-Expose-Headers: Location {} Note: Since sharing a thing requires a write policy on the thing, user2 cannot assign a new policy for \"user1-thing\" by itself. Now, user2 has read and delete policies on \"user1-thing\" which allows user2 to view and delete \"user1-thing\" . However, user2 cannot update the \"user1-thing\" because user2 has no write policy on \"user1-thing\" . Let's try again viewing the \"user1-thing\" as user2 : mainflux-cli things get a1109d52-6281-410e-93ae-38ba7daa9381 <user2_token> { \"id\": \"a1109d52-6281-410e-93ae-38ba7daa9381\", \"key\": \"6c9c2146-de49-460d-8f0d-adce4ad37500\", \"name\": \"user1-thing\" } As we expected, the operation is successfully done. The policy server checked that Is user2 allowed to view \"user1-thing\"? Since user2 has a read policy on \"user1-thing\" , the Policy server allows this request. Example usage of Groups # In this scenario, there will be two users called user1@example.com and user2@example.com . user1@example.com will create one Thing called thing-test . Then, the Group entity will be utilized to store all of the created entities ( user1@example.com , user2@example.com , and thing-test ). At the end of this scenario, we will verify that although user2@example.com has no ownership of thing-test , user2@example.com can access the thing-test because they are in the same group. Let's start with creating users: - Create user1@example.com curl -s -S -i -X POST -H \"Content-Type: application/json\" http://localhost/users -d '{\"email\":\"user1@example.com\", \"password\":\"12345678\"}' Create user2@example.com curl -s -S -i -X POST -H \"Content-Type: application/json\" http://localhost/users -d '{\"email\":\"user2@example.com\", \"password\":\"12345678\"}' Now, let's create a Thing called thing-test owned by user1@example.com . Prior to creating it, first, obtain a token for user1@example.com as follows: curl -s -S -i -X POST -H \"Content-Type: application/json\" http://localhost/tokens -d '{\"email\":\"user1@example.com\", \"password\":\"12345678\"}' It is convenient to store the generated token because the token will be required in further steps repeatedly. export USER1TOKEN=<USER1TOKEN> And create a Thing called thing-test curl -s -S -i -X POST -H \"Content-Type: application/json\" -H \"Authorization: Bearer $USER1TOKEN\" http://localhost/things/bulk -d '[{\"name\": \"thing-test\"}]' Note: We will need the ID of newly created Thing in further steps. Again, it is better to store it. If user2@example.com tries to view thing-test , the operation will be denied by policy service because user2@example.com has no policies related to reading thing-test . curl -s -S -i -X GET -H \"Authorization: Bearer $USER2TOKEN\" http://localhost/things/<thing_id> HTTP/1.1 403 Forbidden Server: nginx/1.20.0 Date: Fri, 05 Nov 2021 06:03:42 GMT Content-Type: application/json Content-Length: 60 Connection: keep-alive {\"error\":\"failed to perform authorization over the entity\"} It is time to create a new Group and put all entities into that Group. Mainflux provides HTTP API for Groups like other entities. We will utilize this HTTP API for Group operations. For more details about Groups, please see Groups documentation . curl -s -S -i -X POST -H \"Content-Type: application/json\" -H \"Authorization: Bearer $USER1TOKEN\" http://localhost/groups -d '{\"name\": \"my_group\"}' HTTP/1.1 201 Created Server: nginx/1.20.0 Date: Fri, 05 Nov 2021 06:10:32 GMT Content-Type: application/json Content-Length: 0 Connection: keep-alive Location: /groups/01FKQBGQEP71DG9C99J37YBJD7 Access-Control-Expose-Headers: Location The POST /groups API creates a new group. In our case, it creates a group called my_group . Since user1@example.com 's token is used, the policy service creates a policy to indicate that user1@example.com is member of my_group . In the Location response header , you can see the ID of the my_group . For the response above, the location is Location: /groups/01FKQBGQEP71DG9C99J37YBJD7 . Therefore, the group ID of my_group is 01FKQBGQEP71DG9C99J37YBJD7 . We will need this ID while assigning new members to my_group . The group my_group includes just a member that is user1@example.com , yet. In order to add new members, we will use POST /groups/<group_id>/members . While assigning entities, you will need the ID of the entities respectively. Let's start with assigning thing-test to my_group . curl -s -S -i -X POST -H \"Content-Type: application/json\" -H \"Authorization: Bearer $USER1TOKEN\" http://localhost/groups/<group_id>/members -d '{\"members\":[\"<thing_id>\"], \"type\":\"things\"}' The crucial point here is that since we are assigning a Thing to the Group, the \"type\" field of the request body must be things . Now, assign user2@example.com to my_group . curl -s -S -i -X POST -H \"Content-Type: application/json\" -H \"Authorization: Bearer $USER1TOKEN\" http://localhost/groups/$g/members -d '{\"members\": \"c0fb3fdb-ecfa-407a-bd11-93884d70baf7\"], \"type\":\"users\"}' Again, please be careful about the \"type\" field of the request body. Since we are assigning the user, the type is users . Under the hood, the Policy service creates member policies for each entity respectively. Also, each user member will have access to Things defined in the Group. That's why the type field is crucial. Okay, let's check whether user2@example.com is capable to view the my-thing . Previously, the Policy service denied that request from user2@example.com . Try again: curl -s -S -i -X GET -H \"Authorization: Bearer $USER2TOKEN\" http://localhost/things/<thing_id> HTTP/1.1 200 OK Server: nginx/1.20.0 Date: Fri, 05 Nov 2021 06:36:03 GMT Content-Type: application/json Content-Length: 111 Connection: keep-alive Access-Control-Expose-Headers: Location {\"id\":\"7d551538-834d-4398-bd4d-38940dd4bfa9\",\"name\":\"thing-test\",\"key\":\"4305f78d-399b-4cc4-ad42-3fd5bac09715\"} Successful as we expected. Since user2@example.com and my-thing reside in the same group, user2@example.com can access the my-thing through Group policies. If you unassign user2@example.com, the user cannot access my-thing . In order to test it, you can unassign the user2@example.com as follows: curl -s -S -i -X DELETE -H \"Content-Type: application/json\" -H \"Authorization: Bearer $USER1TOKEN\" http://localhost/groups/$g/members -d '{\"members\": \"c0fb3fdb-ecfa-407a-bd11-93884d70baf7\"], \"type\":\"users\"}' Since user2@example.com is not a member of the my_group anymore, the Policy service denies incoming request related to viewing the my-thing from user2@example.com . curl -s -S -i -X GET -H \"Authorization: Bearer $USER2TOKEN\" http://localhost/things/$th HTTP/1.1 403 Forbidden Server: nginx/1.20.0 Date: Fri, 05 Nov 2021 06:39:47 GMT Content-Type: application/json Content-Length: 60 Connection: keep-alive {\"error\":\"failed to perform authorization over the entity\"} Example usage of sharing entities via Group # Mainflux allows you to group entities (e.g., Users and Things ) through Group object in auth service. You can find more details about usage of the Group at Groups documentation. In this example, we will demonstrate how you can share access of the Users group to the Things group. So, each member of the Users group will have access to each Thing assigned to the Things group. We are going to start from a clean Mainflux setup and follow these steps: Create a new user and multiple Things, Create a Thing and User group, and assign members to groups, Share access of the groups First of all, obtain a token for the default admin. You can use any user but for the simplicity of the document, the default admin will be used. By default, Mainflux uses credentials described in .env for the default admin. $ curl -s -S -i -X POST -H \"Content-Type: application/json\" http://localhost/tokens -d '{\"email\":\"admin@example.com\", \"password\":\"12345678\"}' HTTP/1.1 201 Created Server: nginx/1.20.0 Date: Wed, 13 Oct 2021 08:26:45 GMT Content-Type: application/json Content-Length: 285 Connection: keep-alive Strict-Transport-Security: max-age=63072000; includeSubdomains X-Frame-Options: DENY X-Content-Type-Options: nosniff Access-Control-Allow-Origin: * Access-Control-Allow-Methods: * Access-Control-Allow-Headers: * {\"token\":\"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJleHAiOjE2MzQxNDk2MDUsImlhdCI6MTYzNDExMzYwNSwiaXNzIjoibWFpbmZsdXguYXV0aCIsInN1YiI6ImFkbWluQGV4YW1wbGUuY29tIiwiaXNzdWVyX2lkIjoiOTA3MjkzMDMtZDMwZC00YmQ5LTkwMTYtNDljMThjZmY4YjUxIiwidHlwZSI6MH0.G1kjXiGX76BqpytmLdXtjLF9s9K5CVm4ScNMIaKlkwE\"} You can store the generated token because we will need it in further steps. $ export token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJleHAiOjE2MzQxNDk2MDUsImlhdCI6MTYzNDExMzYwNSwiaXNzIjoibWFpbmZsdXguYXV0aCIsInN1YiI6ImFkbWluQGV4YW1wbGUuY29tIiwiaXNzdWVyX2lkIjoiOTA3MjkzMDMtZDMwZC00YmQ5LTkwMTYtNDljMThjZmY4YjUxIiwidHlwZSI6MH0.G1kjXiGX76BqpytmLdXtjLF9s9K5CVm4ScNMIaKlkwE Now, we can create a new user as follows: curl -s -S -i -X POST -H \"Content-Type: application/json\" http://localhost/users -d '{\"email\":\"user@example.com\", \"password\":\"12345678\"}' HTTP/1.1 201 Created Server: nginx/1.20.0 Date: Wed, 13 Oct 2021 08:45:57 GMT Content-Type: application/json Content-Length: 0 Connection: keep-alive Location: /users/f31f8a0a-11b1-4aa6-a4a3-9629378c0326 Strict-Transport-Security: max-age=63072000; includeSubdomains X-Frame-Options: DENY X-Content-Type-Options: nosniff Access-Control-Allow-Origin: * Access-Control-Allow-Methods: * Access-Control-Allow-Headers: * You can obtain the user ID via Location . The ID of the user@example.com is f31f8a0a-11b1-4aa6-a4a3-9629378c0326 . After creating the new user, we have two users on the system as admin@example.com and user@example.com . Then, the admin creates multiple Things called admin-thing-1 and admin-thing-2 . curl -s -S -i -X POST -H \"Content-Type: application/json\" -H \"Authorization: Bearer $TOKEN\" http://localhost/things/bulk -d '[{\"name\": \"a dmin-thing-1\"}, {\"name\": \"admin-thing-2\"}]' HTTP/1.1 201 Created Server: nginx/1.20.0 Date: Wed, 13 Oct 2021 08:53:38 GMT Content-Type: application/json Content-Length: 241 Connection: keep-alive Access-Control-Expose-Headers: Location {\"things\":[{\"id\":\"c3d75452-ae00-4aea-84f9-29ab79fd0d26\",\"name\":\"admin-thing-1\",\"key\":\"4fb36389-f7a5-424d-8c4f-da5c9e91f3c5\"},{\"id\":\"ee589c61-0b98-4176-9da0-d91913087be6\",\"name\":\"admin-thing-2\",\"key\":\"410f5889-c756-470d-bd65-2e99b4ecc679\"}]} export th1=c3d75452-ae00-4aea-84f9-29ab79fd0d26 export th2=ee589c61-0b98-4176-9da0-d91913087be6 Mainflux identifies admin@example.com via the token provided through the Authorization request header. On top of that, Mainflux claims ownership of things ( admin-thing-1 and admin-thing-2 ) on the admin@example.com . So that, the creator of Things (in this case admin@example.com ) is going to have read , write and delete policies on the Thing. If user@example.com logs in the system, user@example.com cannot access the things created by the admin@example.com due to lack of policies. The next step is creating the user and things Groups respectively. You can create groups as follows: curl -s -S -i -X POST -H \"Content-Type: application/json\" -H \"Authorization: Bearer $TOKEN\" http://localhost/groups -d '{\"name\": \"user_group\"}' HTTP/1.1 201 Created Server: nginx/1.20.0 Date: Wed, 13 Oct 2021 09:24:39 GMT Content-Type: application/json Content-Length: 0 Connection: keep-alive Location: /groups/01FHWFFMME9N2N26DG0DMNRWRW Access-Control-Expose-Headers: Location curl -s -S -i -X POST -H \"Content-Type: application/json\" -H \"Authorization: Bearer $TOKEN\" http://localhost/groups -d '{\"name\": \"thing_group\"}' HTTP/1.1 201 Created Server: nginx/1.20.0 Date: Wed, 13 Oct 2021 09:24:58 GMT Content-Type: application/json Content-Length: 0 Connection: keep-alive Location: /groups/01FHWFG78DSYA458D8ST4YQ9Y9 Access-Control-Expose-Headers: Location Again, you can obtain group IDs via Location in response. It is convenient to store them in variables. export ug=01FHWFFMME9N2N26DG0DMNRWRW export tg=01FHWFG78DSYA458D8ST4YQ9Y9 After creating groups, we are ready to assign new members to groups. Let's start with the user group. curl -s -S -i -X POST -H \"Content-Type: application/json\" -H \"Authorization: Bearer $TOKEN\" http://localhost/groups/$ug/members -d '{\"members\":[\"f31f8a0a-11b1-4aa6-a4a3-9629378c0326\"], \"type\":\"users\"}' HTTP/1.1 200 OK Server: nginx/1.20.0 Date: Wed, 13 Oct 2021 09:37:05 GMT Content-Type: application/json Content-Length: 0 Connection: keep-alive Access-Control-Expose-Headers: Location If you remember, f31f8a0a-11b1-4aa6-a4a3-9629378c0326 is the ID of the user@example.com . Since the $ug represents the ID of the user group called user_group , we indicated the type of the group as \"users\" in the request body. Now, we can assign Things to the thing group. curl -s -S -i -X POST -H \"Content-Type: application/json\" -H \"Authorization: Bearer $TOKEN\" http://localhost/groups/$tg/members -d '{\"members\":[\"c3d75452-ae00-4aea-84f9-29ab79fd0d26\", \"ee589c61-0b98-4176-9da0-d91913087be6\"], \"type\":\"things\"}' HTTP/1.1 200 OK Server: nginx/1.20.0 Date: Wed, 13 Oct 2021 09:42:12 GMT Content-Type: application/json Content-Length: 0 Connection: keep-alive Access-Control-Expose-Headers: Location The same logic applies here as well. The IDs of the things that admin@example.com created are c3d75452-ae00-4aea-84f9-29ab79fd0d26 and ee589c61-0b98-4176-9da0-d91913087be6 . Since the $tg represents the ID of the thing group called thing_group , we indicated the type of the group as \"things\" in the request body. Before moving to the third step, let's analyze the current situation. We have two groups, two users, and two things. The first group is the user group and consists of two users, admin@example.com (since the admin created the group) and user@example.com . The second group is the thing group. It includes two things created by admin@example.com . user@example.com still has no access to things created by admin@example.com . You can verify it as: curl -s -S -i -X GET -H \"Authorization: Bearer $TOKEN\" http://localhost/things/$th1 HTTP/1.1 403 Forbidden Server: nginx/1.20.0 Date: Wed, 13 Oct 2021 09:51:45 GMT Content-Type: application/json Content-Length: 60 Connection: keep-alive {\"error\":\"failed to perform authorization over the entity\"} curl -s -S -i -X GET -H \"Authorization: Bearer $TOKEN\" http://localhost/things/$th2 HTTP/1.1 403 Forbidden Server: nginx/1.20.0 Date: Wed, 13 Oct 2021 09:51:49 GMT Content-Type: application/json Content-Length: 60 Connection: keep-alive {\"error\":\"failed to perform authorization over the entity\"} The $TOKEN is the token for user@example.com . As you can see, requests to access things are denied. Now, let's assign group access rights. curl -s -S -i -X POST http://localhost/groups/$ug/share -d '{\"thing_group_id\": \"01FHWFG78DSYA458D8ST4YQ9Y9\"}' -H 'Content-Type: application/json' -H \"Authorization: Bearer $TOKEN\" HTTP/1.1 200 OK Server: nginx/1.20.0 Date: Wed, 13 Oct 2021 09:59:13 GMT Content-Type: application/json Content-Length: 3 Connection: keep-alive Access-Control-Expose-Headers: Location Now, all the members of the user_group have access to things within the thing_group . Therefore, user@example.com has read , write and delete policies on the things within the thing_group. Try to access things as user@example.com . curl -s -S -i -X GET -H \"Authorization: Bearer $TOKEN\" http://localhost/things/$th1 HTTP/1.1 200 OK Server: nginx/1.20.0 Date: Wed, 13 Oct 2021 10:02:19 GMT Content-Type: application/json Content-Length: 114 Connection: keep-alive Access-Control-Expose-Headers: Location {\"id\":\"c3d75452-ae00-4aea-84f9-29ab79fd0d26\",\"name\":\"admin-thing-1\",\"key\":\"4fb36389-f7a5-424d-8c4f-da5c9e91f3c5\"} curl -s -S -i -X GET -H \"Authorization: Bearer $TOKEN\" http://localhost/things/$th2 HTTP/1.1 200 OK Server: nginx/1.20.0 Date: Wed, 13 Oct 2021 10:02:21 GMT Content-Type: application/json Content-Length: 114 Connection: keep-alive Access-Control-Expose-Headers: Location {\"id\":\"ee589c61-0b98-4176-9da0-d91913087be6\",\"name\":\"admin-thing-2\",\"key\":\"410f5889-c756-470d-bd65-2e99b4ecc679\"} Successful! Let's assume, admin@example.com does not want to share things with user@example.com anymore. In order to achieve that, admin@example.com unassigns user@example.com from the user_group . curl -s -S -i -X DELETE -H \"Content-Type: application/json\" -H \"Authorization: Bearer $TOKEN\" http://localhost/groups/$ug/members -d '{\"members\":[\"f31f8a0a-11b1-4aa6-a4a3-9629378c0326\"], \"type\":\"users\"}' HTTP/1.1 204 No Content Server: nginx/1.20.0 Date: Wed, 13 Oct 2021 10:08:56 GMT Content-Type: application/json Connection: keep-alive Access-Control-Expose-Headers: Location Now, when user@example.com tries to access the things, the request will be denied. curl -s -S -i -X GET -H \"Authorization: Bearer $TOKEN\" http://localhost/things/$th1 HTTP/1.1 403 Forbidden Server: nginx/1.20.0 Date: Wed, 13 Oct 2021 10:10:26 GMT Content-Type: application/json Content-Length: 60 Connection: keep-alive {\"error\":\"failed to perform authorization over the entity\"} curl -s -S -i -X GET -H \"Authorization: Bearer $TOKEN\" http://localhost/things/$th2 HTTP/1.1 403 Forbidden Server: nginx/1.20.0 Date: Wed, 13 Oct 2021 10:10:28 GMT Content-Type: application/json Content-Length: 60 Connection: keep-alive {\"error\":\"failed to perform authorization over the entity\"}","title":"Authorization"},{"location":"authorization/#authorization","text":"","title":"Authorization"},{"location":"authorization/#policies","text":"Mainflux uses policies to control permissions on entities: users , things , and groups . Under the hood, Mainflux uses ORY Keto that is an open-source implementation of \"Zanzibar: Google's Consistent, Global Authorization System\" . Policies define permissions for the entities. For example, which user has access to a specific thing . Such policies have three main components: subject , object , and relation . To put it briefly: Subject : As the name suggests, it is the subject that will have the policy such as users . Mainflux uses entity UUID on behalf of the real entities. Object : Objects are Mainflux entities (e.g. thing or group ) represented by their UUID. Relation : This is the action that the subject wants to do on the object. For more conceptual details, you can refer official ORY Keto documentation All three components create a single policy. For example, let's assume we have a following policy: \"user_id_123\" has \"read\" relation on \"thing_id_123\" . This policy means that subject (a user with ID: user_id_123 ) has a relation ( read ) on the object (a thing with ID: thing_id_123 ). Based upon this example, If the user wants to view a Thing , Mainflux first identifies the user with Authentication Keys and checks the policy as: User with ID: `user_id_123` has `read` relation on the thing with ID: `thing_id_123`. If the user has no such policy, the operation will be denied; otherwise, the operation will be allowed. In this case, since the user user_id_123 has the policy, the read operation on the thing thing_id_123 will be allowed for the user with ID user_id_123 . On the other hand, requests coming from other users (who have a different ID than user_id_123 ) will be denied. In order to check whether a user has the policy or not, Mainflux makes a gRPC call to Keto API, then Keto handles the checking existence of the policy. All policies are stored in the Keto Database. The database responsible for storing all policies is deployed along with the Mainflux, as a standalone PostgreSQL database container.","title":"Policies"},{"location":"authorization/#predefined-policies","text":"Mainflux comes with predefined policies.","title":"Predefined Policies"},{"location":"authorization/#users-service-related-policies","text":"By default, Mainflux allows anybody to create a user. If you disable this default policy, only admin is able to create a user. This default policy can be disabled through an environment variable called MF_USERS_ALLOW_SELF_REGISTER in deployment time. MF_USERS_ALLOW_SELF_REGISTER is a boolean. Therefore, it expects \"true\" or \"false\" . If you assign \"false\" to this environment variable, only admin can create a user. Mainflux creates a special policy to enable this feature as follows: user#create@* . This policy dictates that subject * has create relation on the object users . Here, Mainflux uses a special * subject to represent all users. If this policy is defined, everybody can create new users. All users are a member of the users . To be more precise, once the new user is created, the policy service creates the following policy: users#member@<user_id> indicating that the subject <user_id > has member relation on the object users . The admin has a special policy indicating that the user is admin. This policy is the following: <admin_id> has member relation on the object authorities .","title":"Users service related policies"},{"location":"authorization/#things-service-related-policies","text":"There are 3 policies regarding Things : read , write and delete . When a user creates a thing, the user will have read , write and delete policies on the Thing . In order to view a thing, you need read policy on that thing. In order to update and share the thing, you need a write policy on that thing. In order to remove a thing, you need a delete policy on that thing.","title":"Things service related policies"},{"location":"authorization/#group-entity-related-policies","text":"Once the user creates a new group, the user will have a member policy on the group. If you assign a new User member to your group, the new user will have a member policy on this particular group. If you assign a new Thing member to your group, whatever has member policy on that group will have read , write and delete policies on the Things defined in the Group. Mainflux allows users to assign access rights of the Things group with the Users group. Thus, each member of the User group can access Things defined in the Thing group. In order to do so, the Policy service adds members of the User group as a member of the Thing Group. Therefore, the Users group members have read , write and delete policy on the Things defined in the Thing Group.","title":"Group entity related policies"},{"location":"authorization/#summary-of-the-defined-policies","text":"member : Identifies registered user's role such as admin . Also, it indicates memberships on the Group entity. read , write and delete : Controls access control for the Things. create : Mainflux uses special create policy to allow everybody to create new users. If you want to enable this feature through the HTTP, you need to make following request: curl -isSX POST http://localhost/policies -d '{\"subjects\":[\"*\"],\"policies\": [\"create\"], \"object\": \"user\"}' -H \"Authorization: Bearer <admin_token>\" -H 'Content-Type: application/json'","title":"Summary of the Defined Policies"},{"location":"authorization/#add-policies","text":"You can add policies as well through an HTTP endpoint. Only admin can use this endpoint. Therefore, you need an authentication token for the admin. Caveat: Only policies defined under Summary of the Defined Policies are allowed. Other policies are not allowed. For example, you can add member policy but not custom-member policy because custom-member policy is not defined on the system. curl -isSX POST http://localhost/policies -d '{\"subjects\": [\"<subject_id1>\",...\"<subject_idN>\"], \"object\": \"<object>\", \"policies\": [\"<action_1>, ...\"<action_N>\"]}' -H \"Authorization: Bearer <admin_token>\" -H 'Content-Type: application/json'","title":"Add Policies"},{"location":"authorization/#delete-policies","text":"The admin can delete policies. Only policies defined on Predefined Policies section are allowed. Must-have: admin_token, object, subjects_ids and policies curl -isSX PUT http://localhost/policies -d '{\"subjects\": [\"<subject_id1>\",...\"<subject_idN>\"], \"object\": \"<object>\", \"policies\": [\"<action_1>, ...\"<action_N>\"]}' -H \"Authorization: Bearer <admin_token>\" -H 'Content-Type: application/json' admin_token must belong to the admin. Response: HTTP/1.1 204 No Content Content-Type: application/json Date: Wed, 03 Nov 2021 13:00:05 GMT If you delete policies, the policy will be removed from the policy storage. Further authorization checks related to that policy will fail. For example, let's assume user1 has read policy on the thing thing-123 . If you delete this policy as: curl -isSX PUT http://localhost/policies -d '{\"subjects\": [\"<user1_id>\"], \"object\": \"thing-123\", \"policies\": [\"read\"]}' -H \"Authorization: Bearer <admin_token>\" -H 'Content-Type: application/json' user1 will not be able to view the thing-123 anymore because the policy which allows user1 to view thing-123 is deleted by the admin.","title":"Delete Policies"},{"location":"authorization/#example-usage-of-adding-a-policy","text":"Suppose we are using the Mainflux version that doesn't have a policies feature yet. Once you migrate a new version of the Mainflux including the Policy feature, your users will face a lack of authorization. For example, there is a user created before the Policy feature. This user is authenticated by <user_token >. Although the following operation is valid, the user will have an authorization error. mainflux-cli things create '{\"name\":\"user-thing\"}' <user_token> error: failed to create entity: 403 Forbidden The reason is that the user has not enough policy to create a new Thing after migration. In order to create a new thing, the user has to have a member relation on the users key. So that, Mainflux understands that the requester user is authorized to create new Things. The easiest solution for this problem is adding policies for the users through the HTTP endpoint. As described above, the user needs a member relation on the users . curl -isSX POST http://localhost/policies -d '{\"subjects\":[\"<user_id>\"],\"policies\": [\"member\"], \"object\": \"users\"}' -H \"Authorization: Bearer <admin_token> \" -H 'Content-Type: application/json' So what this request does is add new policies for the subject defined in the subjects field of the request body. Henceforth, the subject (here <user_id> ) will have a member relation on the object users . This policy allows the user to create new Things. Please, keep in mind that this endpoint requires you to use <admin_token> , not any token. So, the token must belong to the admin.","title":"Example usage of adding a policy"},{"location":"authorization/#example-usage-of-sharing-a-thing","text":"Let's assume, we have two users (called user1 and user2 ) registered on the system who have user_id_1 and user_id_2 as their ID respectively. Let's create a thing with the following command: mainflux-cli things create '{\"name\":\"user1-thing\"}' <user1_token> created: a1109d52-6281-410e-93ae-38ba7daa9381 This command creates a thing called \"user1-thing\" with ID = a1109d52-6281-410e-93ae-38ba7daa9381 . Mainflux identifies the user1 by using the <user1_token> . After identifying the requester as user1 , the Policy service adds read , write and delete policies to user1 on \"user1-thing\" . If user2 wants to view the \"user1-thing\" , the request will be denied. mainflux-cli things get a1109d52-6281-410e-93ae-38ba7daa9381 <user2_token> error: failed to fetch entity : 403 Forbidden After identifying the requester as user2 , the Policy service checks that Is user2 allowed to view the \"user1-thing\"? Since user2 has no such policy ( read policy on \"user1-thing\" ), the Policy service denies this request. Now, user1 wants to share the \"user1-thing\" with user2 . user1 can achieve this via HTTP endpoint for sharing things as follows: curl -isSX POST http://localhost/things/a1109d52-6281-410e-93ae-38ba7daa9381/share -d '{\"user_ids\":[\"<user2_id>]\", \"policies\": [\"read\", \"delete\"]}' -H \"Authorization: Bearer <user1_token>\" -H 'Content-Type: application/json' HTTP/1.1 200 OK Server: nginx/1.20.0 Date: Thu, 09 Sep 2021 11:36:10 GMT Content-Type: application/json Content-Length: 3 Connection: keep-alive Access-Control-Expose-Headers: Location {} Note: Since sharing a thing requires a write policy on the thing, user2 cannot assign a new policy for \"user1-thing\" by itself. Now, user2 has read and delete policies on \"user1-thing\" which allows user2 to view and delete \"user1-thing\" . However, user2 cannot update the \"user1-thing\" because user2 has no write policy on \"user1-thing\" . Let's try again viewing the \"user1-thing\" as user2 : mainflux-cli things get a1109d52-6281-410e-93ae-38ba7daa9381 <user2_token> { \"id\": \"a1109d52-6281-410e-93ae-38ba7daa9381\", \"key\": \"6c9c2146-de49-460d-8f0d-adce4ad37500\", \"name\": \"user1-thing\" } As we expected, the operation is successfully done. The policy server checked that Is user2 allowed to view \"user1-thing\"? Since user2 has a read policy on \"user1-thing\" , the Policy server allows this request.","title":"Example usage of sharing a Thing"},{"location":"authorization/#example-usage-of-groups","text":"In this scenario, there will be two users called user1@example.com and user2@example.com . user1@example.com will create one Thing called thing-test . Then, the Group entity will be utilized to store all of the created entities ( user1@example.com , user2@example.com , and thing-test ). At the end of this scenario, we will verify that although user2@example.com has no ownership of thing-test , user2@example.com can access the thing-test because they are in the same group. Let's start with creating users: - Create user1@example.com curl -s -S -i -X POST -H \"Content-Type: application/json\" http://localhost/users -d '{\"email\":\"user1@example.com\", \"password\":\"12345678\"}' Create user2@example.com curl -s -S -i -X POST -H \"Content-Type: application/json\" http://localhost/users -d '{\"email\":\"user2@example.com\", \"password\":\"12345678\"}' Now, let's create a Thing called thing-test owned by user1@example.com . Prior to creating it, first, obtain a token for user1@example.com as follows: curl -s -S -i -X POST -H \"Content-Type: application/json\" http://localhost/tokens -d '{\"email\":\"user1@example.com\", \"password\":\"12345678\"}' It is convenient to store the generated token because the token will be required in further steps repeatedly. export USER1TOKEN=<USER1TOKEN> And create a Thing called thing-test curl -s -S -i -X POST -H \"Content-Type: application/json\" -H \"Authorization: Bearer $USER1TOKEN\" http://localhost/things/bulk -d '[{\"name\": \"thing-test\"}]' Note: We will need the ID of newly created Thing in further steps. Again, it is better to store it. If user2@example.com tries to view thing-test , the operation will be denied by policy service because user2@example.com has no policies related to reading thing-test . curl -s -S -i -X GET -H \"Authorization: Bearer $USER2TOKEN\" http://localhost/things/<thing_id> HTTP/1.1 403 Forbidden Server: nginx/1.20.0 Date: Fri, 05 Nov 2021 06:03:42 GMT Content-Type: application/json Content-Length: 60 Connection: keep-alive {\"error\":\"failed to perform authorization over the entity\"} It is time to create a new Group and put all entities into that Group. Mainflux provides HTTP API for Groups like other entities. We will utilize this HTTP API for Group operations. For more details about Groups, please see Groups documentation . curl -s -S -i -X POST -H \"Content-Type: application/json\" -H \"Authorization: Bearer $USER1TOKEN\" http://localhost/groups -d '{\"name\": \"my_group\"}' HTTP/1.1 201 Created Server: nginx/1.20.0 Date: Fri, 05 Nov 2021 06:10:32 GMT Content-Type: application/json Content-Length: 0 Connection: keep-alive Location: /groups/01FKQBGQEP71DG9C99J37YBJD7 Access-Control-Expose-Headers: Location The POST /groups API creates a new group. In our case, it creates a group called my_group . Since user1@example.com 's token is used, the policy service creates a policy to indicate that user1@example.com is member of my_group . In the Location response header , you can see the ID of the my_group . For the response above, the location is Location: /groups/01FKQBGQEP71DG9C99J37YBJD7 . Therefore, the group ID of my_group is 01FKQBGQEP71DG9C99J37YBJD7 . We will need this ID while assigning new members to my_group . The group my_group includes just a member that is user1@example.com , yet. In order to add new members, we will use POST /groups/<group_id>/members . While assigning entities, you will need the ID of the entities respectively. Let's start with assigning thing-test to my_group . curl -s -S -i -X POST -H \"Content-Type: application/json\" -H \"Authorization: Bearer $USER1TOKEN\" http://localhost/groups/<group_id>/members -d '{\"members\":[\"<thing_id>\"], \"type\":\"things\"}' The crucial point here is that since we are assigning a Thing to the Group, the \"type\" field of the request body must be things . Now, assign user2@example.com to my_group . curl -s -S -i -X POST -H \"Content-Type: application/json\" -H \"Authorization: Bearer $USER1TOKEN\" http://localhost/groups/$g/members -d '{\"members\": \"c0fb3fdb-ecfa-407a-bd11-93884d70baf7\"], \"type\":\"users\"}' Again, please be careful about the \"type\" field of the request body. Since we are assigning the user, the type is users . Under the hood, the Policy service creates member policies for each entity respectively. Also, each user member will have access to Things defined in the Group. That's why the type field is crucial. Okay, let's check whether user2@example.com is capable to view the my-thing . Previously, the Policy service denied that request from user2@example.com . Try again: curl -s -S -i -X GET -H \"Authorization: Bearer $USER2TOKEN\" http://localhost/things/<thing_id> HTTP/1.1 200 OK Server: nginx/1.20.0 Date: Fri, 05 Nov 2021 06:36:03 GMT Content-Type: application/json Content-Length: 111 Connection: keep-alive Access-Control-Expose-Headers: Location {\"id\":\"7d551538-834d-4398-bd4d-38940dd4bfa9\",\"name\":\"thing-test\",\"key\":\"4305f78d-399b-4cc4-ad42-3fd5bac09715\"} Successful as we expected. Since user2@example.com and my-thing reside in the same group, user2@example.com can access the my-thing through Group policies. If you unassign user2@example.com, the user cannot access my-thing . In order to test it, you can unassign the user2@example.com as follows: curl -s -S -i -X DELETE -H \"Content-Type: application/json\" -H \"Authorization: Bearer $USER1TOKEN\" http://localhost/groups/$g/members -d '{\"members\": \"c0fb3fdb-ecfa-407a-bd11-93884d70baf7\"], \"type\":\"users\"}' Since user2@example.com is not a member of the my_group anymore, the Policy service denies incoming request related to viewing the my-thing from user2@example.com . curl -s -S -i -X GET -H \"Authorization: Bearer $USER2TOKEN\" http://localhost/things/$th HTTP/1.1 403 Forbidden Server: nginx/1.20.0 Date: Fri, 05 Nov 2021 06:39:47 GMT Content-Type: application/json Content-Length: 60 Connection: keep-alive {\"error\":\"failed to perform authorization over the entity\"}","title":"Example usage of Groups"},{"location":"authorization/#example-usage-of-sharing-entities-via-group","text":"Mainflux allows you to group entities (e.g., Users and Things ) through Group object in auth service. You can find more details about usage of the Group at Groups documentation. In this example, we will demonstrate how you can share access of the Users group to the Things group. So, each member of the Users group will have access to each Thing assigned to the Things group. We are going to start from a clean Mainflux setup and follow these steps: Create a new user and multiple Things, Create a Thing and User group, and assign members to groups, Share access of the groups First of all, obtain a token for the default admin. You can use any user but for the simplicity of the document, the default admin will be used. By default, Mainflux uses credentials described in .env for the default admin. $ curl -s -S -i -X POST -H \"Content-Type: application/json\" http://localhost/tokens -d '{\"email\":\"admin@example.com\", \"password\":\"12345678\"}' HTTP/1.1 201 Created Server: nginx/1.20.0 Date: Wed, 13 Oct 2021 08:26:45 GMT Content-Type: application/json Content-Length: 285 Connection: keep-alive Strict-Transport-Security: max-age=63072000; includeSubdomains X-Frame-Options: DENY X-Content-Type-Options: nosniff Access-Control-Allow-Origin: * Access-Control-Allow-Methods: * Access-Control-Allow-Headers: * {\"token\":\"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJleHAiOjE2MzQxNDk2MDUsImlhdCI6MTYzNDExMzYwNSwiaXNzIjoibWFpbmZsdXguYXV0aCIsInN1YiI6ImFkbWluQGV4YW1wbGUuY29tIiwiaXNzdWVyX2lkIjoiOTA3MjkzMDMtZDMwZC00YmQ5LTkwMTYtNDljMThjZmY4YjUxIiwidHlwZSI6MH0.G1kjXiGX76BqpytmLdXtjLF9s9K5CVm4ScNMIaKlkwE\"} You can store the generated token because we will need it in further steps. $ export token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJleHAiOjE2MzQxNDk2MDUsImlhdCI6MTYzNDExMzYwNSwiaXNzIjoibWFpbmZsdXguYXV0aCIsInN1YiI6ImFkbWluQGV4YW1wbGUuY29tIiwiaXNzdWVyX2lkIjoiOTA3MjkzMDMtZDMwZC00YmQ5LTkwMTYtNDljMThjZmY4YjUxIiwidHlwZSI6MH0.G1kjXiGX76BqpytmLdXtjLF9s9K5CVm4ScNMIaKlkwE Now, we can create a new user as follows: curl -s -S -i -X POST -H \"Content-Type: application/json\" http://localhost/users -d '{\"email\":\"user@example.com\", \"password\":\"12345678\"}' HTTP/1.1 201 Created Server: nginx/1.20.0 Date: Wed, 13 Oct 2021 08:45:57 GMT Content-Type: application/json Content-Length: 0 Connection: keep-alive Location: /users/f31f8a0a-11b1-4aa6-a4a3-9629378c0326 Strict-Transport-Security: max-age=63072000; includeSubdomains X-Frame-Options: DENY X-Content-Type-Options: nosniff Access-Control-Allow-Origin: * Access-Control-Allow-Methods: * Access-Control-Allow-Headers: * You can obtain the user ID via Location . The ID of the user@example.com is f31f8a0a-11b1-4aa6-a4a3-9629378c0326 . After creating the new user, we have two users on the system as admin@example.com and user@example.com . Then, the admin creates multiple Things called admin-thing-1 and admin-thing-2 . curl -s -S -i -X POST -H \"Content-Type: application/json\" -H \"Authorization: Bearer $TOKEN\" http://localhost/things/bulk -d '[{\"name\": \"a dmin-thing-1\"}, {\"name\": \"admin-thing-2\"}]' HTTP/1.1 201 Created Server: nginx/1.20.0 Date: Wed, 13 Oct 2021 08:53:38 GMT Content-Type: application/json Content-Length: 241 Connection: keep-alive Access-Control-Expose-Headers: Location {\"things\":[{\"id\":\"c3d75452-ae00-4aea-84f9-29ab79fd0d26\",\"name\":\"admin-thing-1\",\"key\":\"4fb36389-f7a5-424d-8c4f-da5c9e91f3c5\"},{\"id\":\"ee589c61-0b98-4176-9da0-d91913087be6\",\"name\":\"admin-thing-2\",\"key\":\"410f5889-c756-470d-bd65-2e99b4ecc679\"}]} export th1=c3d75452-ae00-4aea-84f9-29ab79fd0d26 export th2=ee589c61-0b98-4176-9da0-d91913087be6 Mainflux identifies admin@example.com via the token provided through the Authorization request header. On top of that, Mainflux claims ownership of things ( admin-thing-1 and admin-thing-2 ) on the admin@example.com . So that, the creator of Things (in this case admin@example.com ) is going to have read , write and delete policies on the Thing. If user@example.com logs in the system, user@example.com cannot access the things created by the admin@example.com due to lack of policies. The next step is creating the user and things Groups respectively. You can create groups as follows: curl -s -S -i -X POST -H \"Content-Type: application/json\" -H \"Authorization: Bearer $TOKEN\" http://localhost/groups -d '{\"name\": \"user_group\"}' HTTP/1.1 201 Created Server: nginx/1.20.0 Date: Wed, 13 Oct 2021 09:24:39 GMT Content-Type: application/json Content-Length: 0 Connection: keep-alive Location: /groups/01FHWFFMME9N2N26DG0DMNRWRW Access-Control-Expose-Headers: Location curl -s -S -i -X POST -H \"Content-Type: application/json\" -H \"Authorization: Bearer $TOKEN\" http://localhost/groups -d '{\"name\": \"thing_group\"}' HTTP/1.1 201 Created Server: nginx/1.20.0 Date: Wed, 13 Oct 2021 09:24:58 GMT Content-Type: application/json Content-Length: 0 Connection: keep-alive Location: /groups/01FHWFG78DSYA458D8ST4YQ9Y9 Access-Control-Expose-Headers: Location Again, you can obtain group IDs via Location in response. It is convenient to store them in variables. export ug=01FHWFFMME9N2N26DG0DMNRWRW export tg=01FHWFG78DSYA458D8ST4YQ9Y9 After creating groups, we are ready to assign new members to groups. Let's start with the user group. curl -s -S -i -X POST -H \"Content-Type: application/json\" -H \"Authorization: Bearer $TOKEN\" http://localhost/groups/$ug/members -d '{\"members\":[\"f31f8a0a-11b1-4aa6-a4a3-9629378c0326\"], \"type\":\"users\"}' HTTP/1.1 200 OK Server: nginx/1.20.0 Date: Wed, 13 Oct 2021 09:37:05 GMT Content-Type: application/json Content-Length: 0 Connection: keep-alive Access-Control-Expose-Headers: Location If you remember, f31f8a0a-11b1-4aa6-a4a3-9629378c0326 is the ID of the user@example.com . Since the $ug represents the ID of the user group called user_group , we indicated the type of the group as \"users\" in the request body. Now, we can assign Things to the thing group. curl -s -S -i -X POST -H \"Content-Type: application/json\" -H \"Authorization: Bearer $TOKEN\" http://localhost/groups/$tg/members -d '{\"members\":[\"c3d75452-ae00-4aea-84f9-29ab79fd0d26\", \"ee589c61-0b98-4176-9da0-d91913087be6\"], \"type\":\"things\"}' HTTP/1.1 200 OK Server: nginx/1.20.0 Date: Wed, 13 Oct 2021 09:42:12 GMT Content-Type: application/json Content-Length: 0 Connection: keep-alive Access-Control-Expose-Headers: Location The same logic applies here as well. The IDs of the things that admin@example.com created are c3d75452-ae00-4aea-84f9-29ab79fd0d26 and ee589c61-0b98-4176-9da0-d91913087be6 . Since the $tg represents the ID of the thing group called thing_group , we indicated the type of the group as \"things\" in the request body. Before moving to the third step, let's analyze the current situation. We have two groups, two users, and two things. The first group is the user group and consists of two users, admin@example.com (since the admin created the group) and user@example.com . The second group is the thing group. It includes two things created by admin@example.com . user@example.com still has no access to things created by admin@example.com . You can verify it as: curl -s -S -i -X GET -H \"Authorization: Bearer $TOKEN\" http://localhost/things/$th1 HTTP/1.1 403 Forbidden Server: nginx/1.20.0 Date: Wed, 13 Oct 2021 09:51:45 GMT Content-Type: application/json Content-Length: 60 Connection: keep-alive {\"error\":\"failed to perform authorization over the entity\"} curl -s -S -i -X GET -H \"Authorization: Bearer $TOKEN\" http://localhost/things/$th2 HTTP/1.1 403 Forbidden Server: nginx/1.20.0 Date: Wed, 13 Oct 2021 09:51:49 GMT Content-Type: application/json Content-Length: 60 Connection: keep-alive {\"error\":\"failed to perform authorization over the entity\"} The $TOKEN is the token for user@example.com . As you can see, requests to access things are denied. Now, let's assign group access rights. curl -s -S -i -X POST http://localhost/groups/$ug/share -d '{\"thing_group_id\": \"01FHWFG78DSYA458D8ST4YQ9Y9\"}' -H 'Content-Type: application/json' -H \"Authorization: Bearer $TOKEN\" HTTP/1.1 200 OK Server: nginx/1.20.0 Date: Wed, 13 Oct 2021 09:59:13 GMT Content-Type: application/json Content-Length: 3 Connection: keep-alive Access-Control-Expose-Headers: Location Now, all the members of the user_group have access to things within the thing_group . Therefore, user@example.com has read , write and delete policies on the things within the thing_group. Try to access things as user@example.com . curl -s -S -i -X GET -H \"Authorization: Bearer $TOKEN\" http://localhost/things/$th1 HTTP/1.1 200 OK Server: nginx/1.20.0 Date: Wed, 13 Oct 2021 10:02:19 GMT Content-Type: application/json Content-Length: 114 Connection: keep-alive Access-Control-Expose-Headers: Location {\"id\":\"c3d75452-ae00-4aea-84f9-29ab79fd0d26\",\"name\":\"admin-thing-1\",\"key\":\"4fb36389-f7a5-424d-8c4f-da5c9e91f3c5\"} curl -s -S -i -X GET -H \"Authorization: Bearer $TOKEN\" http://localhost/things/$th2 HTTP/1.1 200 OK Server: nginx/1.20.0 Date: Wed, 13 Oct 2021 10:02:21 GMT Content-Type: application/json Content-Length: 114 Connection: keep-alive Access-Control-Expose-Headers: Location {\"id\":\"ee589c61-0b98-4176-9da0-d91913087be6\",\"name\":\"admin-thing-2\",\"key\":\"410f5889-c756-470d-bd65-2e99b4ecc679\"} Successful! Let's assume, admin@example.com does not want to share things with user@example.com anymore. In order to achieve that, admin@example.com unassigns user@example.com from the user_group . curl -s -S -i -X DELETE -H \"Content-Type: application/json\" -H \"Authorization: Bearer $TOKEN\" http://localhost/groups/$ug/members -d '{\"members\":[\"f31f8a0a-11b1-4aa6-a4a3-9629378c0326\"], \"type\":\"users\"}' HTTP/1.1 204 No Content Server: nginx/1.20.0 Date: Wed, 13 Oct 2021 10:08:56 GMT Content-Type: application/json Connection: keep-alive Access-Control-Expose-Headers: Location Now, when user@example.com tries to access the things, the request will be denied. curl -s -S -i -X GET -H \"Authorization: Bearer $TOKEN\" http://localhost/things/$th1 HTTP/1.1 403 Forbidden Server: nginx/1.20.0 Date: Wed, 13 Oct 2021 10:10:26 GMT Content-Type: application/json Content-Length: 60 Connection: keep-alive {\"error\":\"failed to perform authorization over the entity\"} curl -s -S -i -X GET -H \"Authorization: Bearer $TOKEN\" http://localhost/things/$th2 HTTP/1.1 403 Forbidden Server: nginx/1.20.0 Date: Wed, 13 Oct 2021 10:10:28 GMT Content-Type: application/json Content-Length: 60 Connection: keep-alive {\"error\":\"failed to perform authorization over the entity\"}","title":"Example usage of sharing entities via Group"},{"location":"benchmark/","text":"Test spec # Tools # MZBench vmq_mzbench mzb_api_ec2_plugin Setting up MZBench # MZbench is open-source tool for that can generate large traffic and measure performance of the application. MZBench is distributed, cloud-aware benchmarking tool that can seamlessly scale to millions of requests. It's originally developed by satori-com but we will use mzbench fork because it can run with newest Erlang releases and the original MzBench repository is not maintained anymore. We will describe installing MZBench server on Ubuntu 18.04 (this can be on your PC or some external cloud server, like droplet on Digital Ocean) Install latest OTP/Erlang (it's version 22.3 for me) sudo apt update sudo apt install erlang For running this tool you will also need libz-dev package: sudo apt-get update sudo apt-get install libz-dev and pip: sudo apt install python-pip Clone mzbench tool and install the requirements: git clone https://github.com/mzbench/mzbench cd mzbench sudo pip install -r requirements.txt This should be enough for installing MZBench, and you can now start MZBench server with this CLI command: ./bin/mzbench start_server The MZBench CLI lets you control the server and benchmarks from the command line. Another way of using MZBench is over Dashboard . After starting server you should check dashboard on http://localhost:4800 . Note that if you are installing MZBench on external server (i.e. Digital Ocean droplet), that you'll be able to reach MZBench dashboard on your server's IP address:4800, if you previously: - change default value for network_interface from 127.0.0.1 to 0.0.0.0 in configuration file. Default configuration file location is ~/.config/mzbench/server.config , create it from sample configuration file ~/.config/mzbench/server.config.example - open port 4800 with ufw allow 4800 MZBench can run your test scenarios on many nodes, simultaneously. For now, you are able to run tests locally, so your nodes will be virtual nodes on machine where MZBench server is installed (your PC or DO droplet). You can try one of our MQTT scenarios that uses vmq_mzbench worker. Copy-paste scenario in MZBench dashboard, click button Environmental variables -> Add from script and add appropriate values. Because it's running locally, you should try with smaller values, for example for fan-in scenario use 100 publishers on 2 nodes. Try this before moving forward in setting up Amazon EC2 plugin. Setting up Amazon EC2 plugin # For larger-scale tests we will set up MZBench to run each node as one of Amazon EC2 instance with built-in plugin mzb_api_ec2_plugin . This is basic architecture when running MZBench: Every node that runs your scenarios will be one of Amazon EC2 instance; plus one more additional node \u2014 the director node. The director doesn't run scenarios, it collects the metrics from the other nodes and runs post and pre hooks . So, if you want to run jobs on 10 nodes, actually 11 EC2 instances will be created. All instances will be automatically terminated when the test finishes. We will use one of ready-to-use Amazon Machine Images (AMI) with all necessary dependencies. We will choose AMI with OTP 22, because that is the version we have on MZBench server. So, we will search for MZBench-erl22 AMI and find one with id ami-03a169923be706764 available in us-west-1b zone. If you have chosen this AMI, everything you do from now must be in us-west-1 zone. We must have IAM user with AmazonEC2FullAccess and IAMFullAccess permissions policies, and his access_key_id and secret_access_key goes to configuration file. In EC2 dashboard, you must create new security group MZbench_cluster where you will add inbound rules to open ssh and TCP ports 4801-4804. Also, in EC2 dashboard go to section key pairs , click Actions -> Import key pair and upload public key you have on your MZBench server in ~/.ssh/id_rsa.pub (if you need to create new, run ssh-keygen and follow instructions). Give it a name on EC2 dashboard, put that name ( key_name ) and path ( keyfile ) in configuration file. [ {mzbench_api, [ {network_interface,\"0.0.0.0\"}, {keyfile, \"~/.ssh/id_rsa\"}, {cloud_plugins, [ {local,#{module => mzb_dummycloud_plugin}}, {ec2, #{module => mzb_api_ec2_plugin, instance_spec => [ {image_id, \"ami-03a169923be706764\"}, {group_set, [\"MZbench_cluster\"]}, {instance_type, \"t2.micro\"}, {availability_zone, \"us-west-1b\"}, {iam_instance_profile_name, \"mzbench\"}, {key_name, \"key_pair_name\"} ], config => [ {ec2_host, \"ec2.us-west-1.amazonaws.com\"}, {access_key_id, \"IAM_USER_ACCESS_KEY_ID\"}, {secret_access_key, \"IAM_USER_SECRET_ACCESS_KEY\"} ], instance_user => \"ec2-user\" }} ] } ]}]. There is both local and ec2 plugin in this configuration file, so you can choose to run tests on either of them. Default path for configuration file is ~/.config/mzbench/server.config , if it's somewhere else, server is starting with: $ ./bin/mzbench start_server --config <config_file> Note that every time you update the configuration you have to restart the server: $ ./bin/mzbench restart_server Test scenarios # Testing environment to be determined. Message publishing # In this scenario, large number of requests are sent to HTTP adapter service every second. This test checks how much time HTTP adapter needs to respond to each request. Results # TBD Create and get client # In this scenario, large number of requests are sent to things service to create things and than to retrieve their data. This test checks how much time things service needs to respond to each request. Results # TBD","title":"Benchmark"},{"location":"benchmark/#test-spec","text":"","title":"Test spec"},{"location":"benchmark/#tools","text":"MZBench vmq_mzbench mzb_api_ec2_plugin","title":"Tools"},{"location":"benchmark/#setting-up-mzbench","text":"MZbench is open-source tool for that can generate large traffic and measure performance of the application. MZBench is distributed, cloud-aware benchmarking tool that can seamlessly scale to millions of requests. It's originally developed by satori-com but we will use mzbench fork because it can run with newest Erlang releases and the original MzBench repository is not maintained anymore. We will describe installing MZBench server on Ubuntu 18.04 (this can be on your PC or some external cloud server, like droplet on Digital Ocean) Install latest OTP/Erlang (it's version 22.3 for me) sudo apt update sudo apt install erlang For running this tool you will also need libz-dev package: sudo apt-get update sudo apt-get install libz-dev and pip: sudo apt install python-pip Clone mzbench tool and install the requirements: git clone https://github.com/mzbench/mzbench cd mzbench sudo pip install -r requirements.txt This should be enough for installing MZBench, and you can now start MZBench server with this CLI command: ./bin/mzbench start_server The MZBench CLI lets you control the server and benchmarks from the command line. Another way of using MZBench is over Dashboard . After starting server you should check dashboard on http://localhost:4800 . Note that if you are installing MZBench on external server (i.e. Digital Ocean droplet), that you'll be able to reach MZBench dashboard on your server's IP address:4800, if you previously: - change default value for network_interface from 127.0.0.1 to 0.0.0.0 in configuration file. Default configuration file location is ~/.config/mzbench/server.config , create it from sample configuration file ~/.config/mzbench/server.config.example - open port 4800 with ufw allow 4800 MZBench can run your test scenarios on many nodes, simultaneously. For now, you are able to run tests locally, so your nodes will be virtual nodes on machine where MZBench server is installed (your PC or DO droplet). You can try one of our MQTT scenarios that uses vmq_mzbench worker. Copy-paste scenario in MZBench dashboard, click button Environmental variables -> Add from script and add appropriate values. Because it's running locally, you should try with smaller values, for example for fan-in scenario use 100 publishers on 2 nodes. Try this before moving forward in setting up Amazon EC2 plugin.","title":"Setting up MZBench"},{"location":"benchmark/#setting-up-amazon-ec2-plugin","text":"For larger-scale tests we will set up MZBench to run each node as one of Amazon EC2 instance with built-in plugin mzb_api_ec2_plugin . This is basic architecture when running MZBench: Every node that runs your scenarios will be one of Amazon EC2 instance; plus one more additional node \u2014 the director node. The director doesn't run scenarios, it collects the metrics from the other nodes and runs post and pre hooks . So, if you want to run jobs on 10 nodes, actually 11 EC2 instances will be created. All instances will be automatically terminated when the test finishes. We will use one of ready-to-use Amazon Machine Images (AMI) with all necessary dependencies. We will choose AMI with OTP 22, because that is the version we have on MZBench server. So, we will search for MZBench-erl22 AMI and find one with id ami-03a169923be706764 available in us-west-1b zone. If you have chosen this AMI, everything you do from now must be in us-west-1 zone. We must have IAM user with AmazonEC2FullAccess and IAMFullAccess permissions policies, and his access_key_id and secret_access_key goes to configuration file. In EC2 dashboard, you must create new security group MZbench_cluster where you will add inbound rules to open ssh and TCP ports 4801-4804. Also, in EC2 dashboard go to section key pairs , click Actions -> Import key pair and upload public key you have on your MZBench server in ~/.ssh/id_rsa.pub (if you need to create new, run ssh-keygen and follow instructions). Give it a name on EC2 dashboard, put that name ( key_name ) and path ( keyfile ) in configuration file. [ {mzbench_api, [ {network_interface,\"0.0.0.0\"}, {keyfile, \"~/.ssh/id_rsa\"}, {cloud_plugins, [ {local,#{module => mzb_dummycloud_plugin}}, {ec2, #{module => mzb_api_ec2_plugin, instance_spec => [ {image_id, \"ami-03a169923be706764\"}, {group_set, [\"MZbench_cluster\"]}, {instance_type, \"t2.micro\"}, {availability_zone, \"us-west-1b\"}, {iam_instance_profile_name, \"mzbench\"}, {key_name, \"key_pair_name\"} ], config => [ {ec2_host, \"ec2.us-west-1.amazonaws.com\"}, {access_key_id, \"IAM_USER_ACCESS_KEY_ID\"}, {secret_access_key, \"IAM_USER_SECRET_ACCESS_KEY\"} ], instance_user => \"ec2-user\" }} ] } ]}]. There is both local and ec2 plugin in this configuration file, so you can choose to run tests on either of them. Default path for configuration file is ~/.config/mzbench/server.config , if it's somewhere else, server is starting with: $ ./bin/mzbench start_server --config <config_file> Note that every time you update the configuration you have to restart the server: $ ./bin/mzbench restart_server","title":"Setting up Amazon EC2 plugin"},{"location":"benchmark/#test-scenarios","text":"Testing environment to be determined.","title":"Test scenarios"},{"location":"benchmark/#message-publishing","text":"In this scenario, large number of requests are sent to HTTP adapter service every second. This test checks how much time HTTP adapter needs to respond to each request.","title":"Message publishing"},{"location":"benchmark/#results","text":"TBD","title":"Results"},{"location":"benchmark/#create-and-get-client","text":"In this scenario, large number of requests are sent to things service to create things and than to retrieve their data. This test checks how much time things service needs to respond to each request.","title":"Create and get client"},{"location":"benchmark/#results_1","text":"TBD","title":"Results"},{"location":"bootstrap/","text":"Bootstrap # Bootstrapping refers to a self-starting process that is supposed to proceed without external input. Mainflux platform supports bootstrapping process, but some of the preconditions need to be fulfilled in advance. The device can trigger a bootstrap when:s device contains only bootstrap credentials and no Mainflux credentials device, for any reason, fails to start a communication with the configured Mainflux services (server not responding, authentication failure, etc..). device, for any reason, wants to update its configuration Bootstrapping and provisioning are two different procedures. Provisioning refers to entities management while bootstrapping is related to entity configuration. Bootstrapping procedure is the following: 1) Configure device with Bootstrap service URL, an external key and external ID Optionally create Mainflux channels if they don't exist Optionally create Mainflux thing if it doesn't exist 2) Upload configuration for the Mainflux thing 3) Bootstrap - send a request for the configuration 4) Connect/disconnect thing from channels, update or remove configuration Configuration # The configuration of Mainflux thing consists of three major parts: The list of Mainflux channels the thing is connected to Custom configuration related to the specific thing Thing key and certificate data related to that thing Also, the configuration contains an external ID and external key, which will be explained later. In order to enable the thing to start bootstrapping process, the user needs to upload a valid configuration for that specific thing. This can be done using the following HTTP request: curl -s -S -i -X POST -H \"Authorization: Bearer <user_token>\" -H \"Content-Type: application/json\" http://localhost:8202/things/configs -d '{ \"external_id\":\"09:6:0:sb:sa\", \"thing_id\": \"1b9b8fae-9035-4969-a240-7fe5bdc0ed28\", \"external_key\":\"key\", \"name\":\"some\", \"channels\":[ \"c3642289-501d-4974-82f2-ecccc71b2d83\", \"cd4ce940-9173-43e3-86f7-f788e055eb14\", \"ff13ca9c-7322-4c28-a25c-4fe5c7b753fc\", \"c3642289-501d-4974-82f2-ecccc71b2d82\" ], \"content\": \"config...\", \"client_cert\": \"PEM cert\", \"client_key\": \"PEM client cert key\", \"ca_cert\": \"PEM CA cert\" }' In this example, channels field represents the list of Mainflux channel IDs the thing is connected to. These channels need to be provisioned before the configuration is uploaded. Field content represents custom configuration. This custom configuration contains parameters that can be used to set up the thing. It can also be empty if no additional set up is needed. Field name is human readable name and thing_id is an ID of the Mainflux thing. This field is not required. If thing_id is empty, corresponding Mainflux thing will be created implicitly and its ID will be sent as a part of Location header of the response. Fields client_cert , client_key and ca_cert represent PEM or base64-encoded DER client certificate, client certificate key and trusted CA, respectively. There are two more fields: external_id and external_key . External ID represents an ID of the device that corresponds to the given thing. For example, this can be a MAC address or the serial number of the device. The external key represents the device key. This is the secret key that's safely stored on the device and it is used to authorize the thing during the bootstrapping process. Please note that external ID and external key and Mainflux ID and Mainflux key are completely different concepts . External id and key are only used to authenticate a device that corresponds to the specific Mainflux thing during the bootstrapping procedure. As Configuration optionally contains client certificate and issuing CA, it's possible that device is not able to establish TLS encrypted communication with Mainflux before bootstrapping. For that purpose, Bootstrap service exposes endpoint used for secure bootstrapping which can be used regardless of protocol (HTTP or HTTPS). Both device and Bootstrap service use a secret key to encrypt the content. Encryption is done as follows: 1) Device uses the secret encryption key to encrypt the value of that exact external key 2) Device sends a bootstrap request using the value from 1 as an Authorization header 3) Bootstrap service fetches config by its external ID 4) Bootstrap service uses the secret encryption key to decrypt Authorization header 5) Bootstrap service compares value from 4 with the external key of the config from 3 and proceeds to 6 if they're equal 6) Bootstrap service uses the secret encryption key to encrypt the content of the bootstrap response Please have on mind that secret key is passed to the Bootstrap service as an environment variable. As security measurement, Bootstrap service removes this variable once it reads it on startup. However, depending on your deployment, this variable can still be visible as a part of your configuration or terminal emulator environment. For more details on which encryption mechanisms are used, please take a look at the implementation. Bootstrapping # Currently, the bootstrapping procedure is executed over the HTTP protocol. Bootstrapping is nothing else but fetching and applying the configuration that corresponds to the given Mainflux thing. In order to fetch the configuration, the thing needs to send a bootstrapping request: curl -s -S -i -H \"Authorization: Thing <external_key>\" http://localhost:8202/things/bootstrap/<external_id> The response body should look something like: { \"mainflux_id\":\"7c9df5eb-d06b-4402-8c1a-df476e4394c8\", \"mainflux_key\":\"86a4f870-eba4-46a0-bef9-d94db2b64392\", \"mainflux_channels\":[ { \"id\":\"ff13ca9c-7322-4c28-a25c-4fe5c7b753fc\", \"name\":\"some channel\", \"metadata\":{ \"operation\":\"someop\", \"type\":\"metadata\" } }, { \"id\":\"925461e6-edfb-4755-9242-8a57199b90a5\", \"name\":\"channel1\", \"metadata\":{ \"type\":\"control\" } } ], \"content\":\"config...\" } The response consists of an ID and key of the Mainflux thing, the list of channels and custom configuration ( content field). The list of channels contains not just channel IDs, but the additional Mainflux channel data ( name and metadata fields), as well. Enabling and disabling things # Uploading configuration does not automatically connect thing to the given list of channels. In order to connect the thing to the channels, user needs to send the following HTTP request: curl -s -S -i -X PUT -H \"Authorization: Bearer <user_token>\" -H \"Content-Type: application/json\" http://localhost:8202/things/state/<thing_id> -d '{\"state\": 1}' In order to disconnect, the same request should be sent with the value of state set to 0. For more information about the Bootstrap service API, please check out the API documentation .","title":"Bootstrap"},{"location":"bootstrap/#bootstrap","text":"Bootstrapping refers to a self-starting process that is supposed to proceed without external input. Mainflux platform supports bootstrapping process, but some of the preconditions need to be fulfilled in advance. The device can trigger a bootstrap when:s device contains only bootstrap credentials and no Mainflux credentials device, for any reason, fails to start a communication with the configured Mainflux services (server not responding, authentication failure, etc..). device, for any reason, wants to update its configuration Bootstrapping and provisioning are two different procedures. Provisioning refers to entities management while bootstrapping is related to entity configuration. Bootstrapping procedure is the following: 1) Configure device with Bootstrap service URL, an external key and external ID Optionally create Mainflux channels if they don't exist Optionally create Mainflux thing if it doesn't exist 2) Upload configuration for the Mainflux thing 3) Bootstrap - send a request for the configuration 4) Connect/disconnect thing from channels, update or remove configuration","title":"Bootstrap"},{"location":"bootstrap/#configuration","text":"The configuration of Mainflux thing consists of three major parts: The list of Mainflux channels the thing is connected to Custom configuration related to the specific thing Thing key and certificate data related to that thing Also, the configuration contains an external ID and external key, which will be explained later. In order to enable the thing to start bootstrapping process, the user needs to upload a valid configuration for that specific thing. This can be done using the following HTTP request: curl -s -S -i -X POST -H \"Authorization: Bearer <user_token>\" -H \"Content-Type: application/json\" http://localhost:8202/things/configs -d '{ \"external_id\":\"09:6:0:sb:sa\", \"thing_id\": \"1b9b8fae-9035-4969-a240-7fe5bdc0ed28\", \"external_key\":\"key\", \"name\":\"some\", \"channels\":[ \"c3642289-501d-4974-82f2-ecccc71b2d83\", \"cd4ce940-9173-43e3-86f7-f788e055eb14\", \"ff13ca9c-7322-4c28-a25c-4fe5c7b753fc\", \"c3642289-501d-4974-82f2-ecccc71b2d82\" ], \"content\": \"config...\", \"client_cert\": \"PEM cert\", \"client_key\": \"PEM client cert key\", \"ca_cert\": \"PEM CA cert\" }' In this example, channels field represents the list of Mainflux channel IDs the thing is connected to. These channels need to be provisioned before the configuration is uploaded. Field content represents custom configuration. This custom configuration contains parameters that can be used to set up the thing. It can also be empty if no additional set up is needed. Field name is human readable name and thing_id is an ID of the Mainflux thing. This field is not required. If thing_id is empty, corresponding Mainflux thing will be created implicitly and its ID will be sent as a part of Location header of the response. Fields client_cert , client_key and ca_cert represent PEM or base64-encoded DER client certificate, client certificate key and trusted CA, respectively. There are two more fields: external_id and external_key . External ID represents an ID of the device that corresponds to the given thing. For example, this can be a MAC address or the serial number of the device. The external key represents the device key. This is the secret key that's safely stored on the device and it is used to authorize the thing during the bootstrapping process. Please note that external ID and external key and Mainflux ID and Mainflux key are completely different concepts . External id and key are only used to authenticate a device that corresponds to the specific Mainflux thing during the bootstrapping procedure. As Configuration optionally contains client certificate and issuing CA, it's possible that device is not able to establish TLS encrypted communication with Mainflux before bootstrapping. For that purpose, Bootstrap service exposes endpoint used for secure bootstrapping which can be used regardless of protocol (HTTP or HTTPS). Both device and Bootstrap service use a secret key to encrypt the content. Encryption is done as follows: 1) Device uses the secret encryption key to encrypt the value of that exact external key 2) Device sends a bootstrap request using the value from 1 as an Authorization header 3) Bootstrap service fetches config by its external ID 4) Bootstrap service uses the secret encryption key to decrypt Authorization header 5) Bootstrap service compares value from 4 with the external key of the config from 3 and proceeds to 6 if they're equal 6) Bootstrap service uses the secret encryption key to encrypt the content of the bootstrap response Please have on mind that secret key is passed to the Bootstrap service as an environment variable. As security measurement, Bootstrap service removes this variable once it reads it on startup. However, depending on your deployment, this variable can still be visible as a part of your configuration or terminal emulator environment. For more details on which encryption mechanisms are used, please take a look at the implementation.","title":"Configuration"},{"location":"bootstrap/#bootstrapping","text":"Currently, the bootstrapping procedure is executed over the HTTP protocol. Bootstrapping is nothing else but fetching and applying the configuration that corresponds to the given Mainflux thing. In order to fetch the configuration, the thing needs to send a bootstrapping request: curl -s -S -i -H \"Authorization: Thing <external_key>\" http://localhost:8202/things/bootstrap/<external_id> The response body should look something like: { \"mainflux_id\":\"7c9df5eb-d06b-4402-8c1a-df476e4394c8\", \"mainflux_key\":\"86a4f870-eba4-46a0-bef9-d94db2b64392\", \"mainflux_channels\":[ { \"id\":\"ff13ca9c-7322-4c28-a25c-4fe5c7b753fc\", \"name\":\"some channel\", \"metadata\":{ \"operation\":\"someop\", \"type\":\"metadata\" } }, { \"id\":\"925461e6-edfb-4755-9242-8a57199b90a5\", \"name\":\"channel1\", \"metadata\":{ \"type\":\"control\" } } ], \"content\":\"config...\" } The response consists of an ID and key of the Mainflux thing, the list of channels and custom configuration ( content field). The list of channels contains not just channel IDs, but the additional Mainflux channel data ( name and metadata fields), as well.","title":"Bootstrapping"},{"location":"bootstrap/#enabling-and-disabling-things","text":"Uploading configuration does not automatically connect thing to the given list of channels. In order to connect the thing to the channels, user needs to send the following HTTP request: curl -s -S -i -X PUT -H \"Authorization: Bearer <user_token>\" -H \"Content-Type: application/json\" http://localhost:8202/things/state/<thing_id> -d '{\"state\": 1}' In order to disconnect, the same request should be sent with the value of state set to 0. For more information about the Bootstrap service API, please check out the API documentation .","title":"Enabling and disabling things"},{"location":"cli/","text":"CLI # Mainflux CLI makes it easy to manage users, things, channels and messages. CLI can be downloaded as separate asset from project realeses or it can be built with GNU Make tool: Get the mainflux code go get github.com/mainflux/mainflux Build the mainflux-cli make cli which will build mainflux-cli in <project_root>/build folder. Executing build/mainflux-cli without any arguments will output help with all available commands and flags: Usage: mainflux-cli [command] Available Commands: channels Channels management help Help about any command messages Send or read messages provision Bulk create things and channels from a config file things Things management users Users management health Mainflux Things service health check Flags: -c, --content-type string Mainflux message content type (default \"application/senml+json\") -h, --help help for mainflux-cli -a, --http-prefix string Mainflux http adapter prefix (default \"http\") -i, --insecure Do not check for TLS cert -l, --limit uint limit query parameter (default 100) -m, --mainflux-url string Mainflux host URL (default \"http://localhost\") -o, --offset uint offset query parameter -t, --things-prefix string Mainflux things service prefix -u, --users-prefix string Mainflux users service prefix Use \"mainflux-cli [command] --help\" for more information about a command. It is also possible to use the docker image mainflux/cli to execute CLI command: docker run -it --rm mainflux/cli -m http://<IP_SERVER> [command] You can execute each command with -h flag for more information about that command, e.g. mainflux-cli channels -h will get you usage info: Channels management: create, get, update or delete Channels and get list of Things connected to Channels Usage: mainflux-cli channels [flags] mainflux-cli channels [command] Available Commands: connections connections <channel_id> <user_token> create create <JSON_channel> <user_token> delete delete <channel_id> <user_token> get get <channel_id | all> <user_token> update update <JSON_string> <user_token> Service # Get Mainflux Things services health check # mainflux-cli health Users management # Create User # Mainflux has two options for user creation. Either everybody or just the admin is able to create new users. This option is dictated through policies and be configured through environment variable ( MF_USERS_ALLOW_SELF_REGISTER ). If only the admin is allowed to create new users, then the <user_token> is required because the token is used to verify that the requester is admin or not. Otherwise, the token is not used, since everybody can create new users. However, the token is still required, in order to be consistent. For more details, please see Authorization page . if env `MF_USERS_ALLOW_SELF_REGISTER` is \"true\" then mainflux-cli users create <user_email> <user_password> else mainflux-cli users create <user_email> <user_password> <admin_token> MF_USERS_ALLOW_SELF_REGISTER is true by default. Therefore, you do not need to provide <admin_token> if MF_USERS_ALLOW_SELF_REGISTER is true. On the other hand, if you set MF_USERS_ALLOW_SELF_REGISTER to false , the Admin token is required for authorization. Therefore, you have to provide the admin token through third argument stated as <admin_token> . Login User # mainflux-cli users token <user_email> <user_password> Retrieve User # mainflux-cli users get <user_token> Update User Metadata # mainflux-cli users update '{\"key1\":\"value1\", \"key2\":\"value2\"}' <user_token> Update User Password # mainflux-cli users password <old_password> <password> <user_token> System Provisioning # Create Thing # mainflux-cli things create '{\"name\":\"myThing\"}' <user_token> Create Thing with metadata # mainflux-cli things create '{\"name\":\"myThing\", \"metadata\": {\\\"key1\\\":\\\"value1\\\"}}' <user_token> Bulk Provision Things # mainflux-cli provision things <file> <user_token> file - A CSV or JSON file containing things (must have extension .csv or .json ) user_token - A valid user auth token for the current system Update Thing # mainflux-cli things update '{\"id\":\"<thing_id>\", \"name\":\"myNewName\"}' <user_token> Remove Thing # mainflux-cli things delete <thing_id> <user_token> Retrieve a subset list of provisioned Things # mainflux-cli things get all --offset=1 --limit=5 <user_token> Retrieve Thing By ID # mainflux-cli things get <thing_id> <user_token> Create Channel # mainflux-cli channels create '{\"name\":\"myChannel\"}' <user_token> Bulk Provision Channels # mainflux-cli provision channels <file> <user_token> file - A CSV or JSON file containing channels (must have extension .csv or .json ) user_token - A valid user auth token for the current system Update Channel # mainflux-cli channels update '{\"id\":\"<channel_id>\",\"name\":\"myNewName\"}' <user_token> Remove Channel # mainflux-cli channels delete <channel_id> <user_token> Retrieve a subset list of provisioned Channels # mainflux-cli channels get all --offset=1 --limit=5 <user_token> Retrieve Channel By ID # mainflux-cli channels get <channel_id> <user_token> Access control # Connect Thing to Channel # mainflux-cli things connect <thing_id> <channel_id> <user_token> Bulk Connect Things to Channels # mainflux-cli provision connect <file> <user_token> file - A CSV or JSON file containing thing and channel ids (must have extension .csv or .json ) user_token - A valid user auth token for the current system An example CSV file might be <thing_id>,<channel_id> <thing_id>,<channel_id> in which the first column is thing IDs and the second column is channel IDs. A connection will be created for each thing to each channel. This example would result in 4 connections being created. A comparable JSON file would be { \"thing_ids\": [ \"<thing_id>\", \"<thing_id>\" ], \"channel_ids\": [ \"<channel_id>\", \"<channel_id>\" ] } Disconnect Thing from Channel # mainflux-cli things disconnect <thing_id> <channel_id> <user_token> Retrieve a subset list of Channels connected to Thing # mainflux-cli things connections <thing_id> <user_token> Retrieve a subset list of Things connected to Channel # mainflux-cli channels connections <channel_id> <user_token> Messaging # Send a message over HTTP # mainflux-cli messages send <channel_id> '[{\"bn\":\"Dev1\",\"n\":\"temp\",\"v\":20}, {\"n\":\"hum\",\"v\":40}, {\"bn\":\"Dev2\", \"n\":\"temp\",\"v\":20}, {\"n\":\"hum\",\"v\":40}]' <thing_auth_token> Read messages over HTTP # mainflux-cli messages read <channel_id> <thing_auth_token> Bootstrap # Add configuration # mainflux-cli bootstrap add '{\"external_id\": \"myExtID\", \"external_key\": \"myExtKey\", \"name\": \"myName\", \"content\": \"myContent\"}' <user_token> View configuration # mainflux-cli bootstrap view <thing_id> <user_token> Update configuration # mainflux-cli bootstrap update '{\"MFThing\":\"<thing_id>\", \"name\": \"newName\", \"content\": \"newContent\"}' <user_token> Remove configuration # mainflux-cli bootstrap remove <thing_id> <user_token> Bootstrap configuration # mainflux-cli bootstrap bootstrap <external_id> <external_key>","title":"CLI"},{"location":"cli/#cli","text":"Mainflux CLI makes it easy to manage users, things, channels and messages. CLI can be downloaded as separate asset from project realeses or it can be built with GNU Make tool: Get the mainflux code go get github.com/mainflux/mainflux Build the mainflux-cli make cli which will build mainflux-cli in <project_root>/build folder. Executing build/mainflux-cli without any arguments will output help with all available commands and flags: Usage: mainflux-cli [command] Available Commands: channels Channels management help Help about any command messages Send or read messages provision Bulk create things and channels from a config file things Things management users Users management health Mainflux Things service health check Flags: -c, --content-type string Mainflux message content type (default \"application/senml+json\") -h, --help help for mainflux-cli -a, --http-prefix string Mainflux http adapter prefix (default \"http\") -i, --insecure Do not check for TLS cert -l, --limit uint limit query parameter (default 100) -m, --mainflux-url string Mainflux host URL (default \"http://localhost\") -o, --offset uint offset query parameter -t, --things-prefix string Mainflux things service prefix -u, --users-prefix string Mainflux users service prefix Use \"mainflux-cli [command] --help\" for more information about a command. It is also possible to use the docker image mainflux/cli to execute CLI command: docker run -it --rm mainflux/cli -m http://<IP_SERVER> [command] You can execute each command with -h flag for more information about that command, e.g. mainflux-cli channels -h will get you usage info: Channels management: create, get, update or delete Channels and get list of Things connected to Channels Usage: mainflux-cli channels [flags] mainflux-cli channels [command] Available Commands: connections connections <channel_id> <user_token> create create <JSON_channel> <user_token> delete delete <channel_id> <user_token> get get <channel_id | all> <user_token> update update <JSON_string> <user_token>","title":"CLI"},{"location":"cli/#service","text":"","title":"Service"},{"location":"cli/#get-mainflux-things-services-health-check","text":"mainflux-cli health","title":"Get Mainflux Things services health check"},{"location":"cli/#users-management","text":"","title":"Users management"},{"location":"cli/#create-user","text":"Mainflux has two options for user creation. Either everybody or just the admin is able to create new users. This option is dictated through policies and be configured through environment variable ( MF_USERS_ALLOW_SELF_REGISTER ). If only the admin is allowed to create new users, then the <user_token> is required because the token is used to verify that the requester is admin or not. Otherwise, the token is not used, since everybody can create new users. However, the token is still required, in order to be consistent. For more details, please see Authorization page . if env `MF_USERS_ALLOW_SELF_REGISTER` is \"true\" then mainflux-cli users create <user_email> <user_password> else mainflux-cli users create <user_email> <user_password> <admin_token> MF_USERS_ALLOW_SELF_REGISTER is true by default. Therefore, you do not need to provide <admin_token> if MF_USERS_ALLOW_SELF_REGISTER is true. On the other hand, if you set MF_USERS_ALLOW_SELF_REGISTER to false , the Admin token is required for authorization. Therefore, you have to provide the admin token through third argument stated as <admin_token> .","title":"Create User"},{"location":"cli/#login-user","text":"mainflux-cli users token <user_email> <user_password>","title":"Login User"},{"location":"cli/#retrieve-user","text":"mainflux-cli users get <user_token>","title":"Retrieve User"},{"location":"cli/#update-user-metadata","text":"mainflux-cli users update '{\"key1\":\"value1\", \"key2\":\"value2\"}' <user_token>","title":"Update User Metadata"},{"location":"cli/#update-user-password","text":"mainflux-cli users password <old_password> <password> <user_token>","title":"Update User Password"},{"location":"cli/#system-provisioning","text":"","title":"System Provisioning"},{"location":"cli/#create-thing","text":"mainflux-cli things create '{\"name\":\"myThing\"}' <user_token>","title":"Create Thing"},{"location":"cli/#create-thing-with-metadata","text":"mainflux-cli things create '{\"name\":\"myThing\", \"metadata\": {\\\"key1\\\":\\\"value1\\\"}}' <user_token>","title":"Create Thing with metadata"},{"location":"cli/#bulk-provision-things","text":"mainflux-cli provision things <file> <user_token> file - A CSV or JSON file containing things (must have extension .csv or .json ) user_token - A valid user auth token for the current system","title":"Bulk Provision Things"},{"location":"cli/#update-thing","text":"mainflux-cli things update '{\"id\":\"<thing_id>\", \"name\":\"myNewName\"}' <user_token>","title":"Update Thing"},{"location":"cli/#remove-thing","text":"mainflux-cli things delete <thing_id> <user_token>","title":"Remove Thing"},{"location":"cli/#retrieve-a-subset-list-of-provisioned-things","text":"mainflux-cli things get all --offset=1 --limit=5 <user_token>","title":"Retrieve a subset list of provisioned Things"},{"location":"cli/#retrieve-thing-by-id","text":"mainflux-cli things get <thing_id> <user_token>","title":"Retrieve Thing By ID"},{"location":"cli/#create-channel","text":"mainflux-cli channels create '{\"name\":\"myChannel\"}' <user_token>","title":"Create Channel"},{"location":"cli/#bulk-provision-channels","text":"mainflux-cli provision channels <file> <user_token> file - A CSV or JSON file containing channels (must have extension .csv or .json ) user_token - A valid user auth token for the current system","title":"Bulk Provision Channels"},{"location":"cli/#update-channel","text":"mainflux-cli channels update '{\"id\":\"<channel_id>\",\"name\":\"myNewName\"}' <user_token>","title":"Update Channel"},{"location":"cli/#remove-channel","text":"mainflux-cli channels delete <channel_id> <user_token>","title":"Remove Channel"},{"location":"cli/#retrieve-a-subset-list-of-provisioned-channels","text":"mainflux-cli channels get all --offset=1 --limit=5 <user_token>","title":"Retrieve a subset list of provisioned Channels"},{"location":"cli/#retrieve-channel-by-id","text":"mainflux-cli channels get <channel_id> <user_token>","title":"Retrieve Channel By ID"},{"location":"cli/#access-control","text":"","title":"Access control"},{"location":"cli/#connect-thing-to-channel","text":"mainflux-cli things connect <thing_id> <channel_id> <user_token>","title":"Connect Thing to Channel"},{"location":"cli/#bulk-connect-things-to-channels","text":"mainflux-cli provision connect <file> <user_token> file - A CSV or JSON file containing thing and channel ids (must have extension .csv or .json ) user_token - A valid user auth token for the current system An example CSV file might be <thing_id>,<channel_id> <thing_id>,<channel_id> in which the first column is thing IDs and the second column is channel IDs. A connection will be created for each thing to each channel. This example would result in 4 connections being created. A comparable JSON file would be { \"thing_ids\": [ \"<thing_id>\", \"<thing_id>\" ], \"channel_ids\": [ \"<channel_id>\", \"<channel_id>\" ] }","title":"Bulk Connect Things to Channels"},{"location":"cli/#disconnect-thing-from-channel","text":"mainflux-cli things disconnect <thing_id> <channel_id> <user_token>","title":"Disconnect Thing from Channel"},{"location":"cli/#retrieve-a-subset-list-of-channels-connected-to-thing","text":"mainflux-cli things connections <thing_id> <user_token>","title":"Retrieve a subset list of Channels connected to Thing"},{"location":"cli/#retrieve-a-subset-list-of-things-connected-to-channel","text":"mainflux-cli channels connections <channel_id> <user_token>","title":"Retrieve a subset list of Things connected to Channel"},{"location":"cli/#messaging","text":"","title":"Messaging"},{"location":"cli/#send-a-message-over-http","text":"mainflux-cli messages send <channel_id> '[{\"bn\":\"Dev1\",\"n\":\"temp\",\"v\":20}, {\"n\":\"hum\",\"v\":40}, {\"bn\":\"Dev2\", \"n\":\"temp\",\"v\":20}, {\"n\":\"hum\",\"v\":40}]' <thing_auth_token>","title":"Send a message over HTTP"},{"location":"cli/#read-messages-over-http","text":"mainflux-cli messages read <channel_id> <thing_auth_token>","title":"Read messages over HTTP"},{"location":"cli/#bootstrap","text":"","title":"Bootstrap"},{"location":"cli/#add-configuration","text":"mainflux-cli bootstrap add '{\"external_id\": \"myExtID\", \"external_key\": \"myExtKey\", \"name\": \"myName\", \"content\": \"myContent\"}' <user_token>","title":"Add configuration"},{"location":"cli/#view-configuration","text":"mainflux-cli bootstrap view <thing_id> <user_token>","title":"View configuration"},{"location":"cli/#update-configuration","text":"mainflux-cli bootstrap update '{\"MFThing\":\"<thing_id>\", \"name\": \"newName\", \"content\": \"newContent\"}' <user_token>","title":"Update configuration"},{"location":"cli/#remove-configuration","text":"mainflux-cli bootstrap remove <thing_id> <user_token>","title":"Remove configuration"},{"location":"cli/#bootstrap-configuration","text":"mainflux-cli bootstrap bootstrap <external_id> <external_key>","title":"Bootstrap configuration"},{"location":"dev-guide/","text":"Developer's guide # Getting Mainflux # Mainflux source can be found in the official Mainflux GitHub repository . You should fork this repository in order to make changes to the project. The forked version of the repository should be cloned using the following: git clone <forked repository> $SOMEPATH/mainflux cd $SOMEPATH/mainflux Note: If your $SOMEPATH is equal to $GOPATH/src/github.com/mainflux/mainflux , make sure that your $GOROOT and $GOPATH do not overlap (otherwise, go modules won't work). Building # Prerequisites # Make sure that you have Protocol Buffers (version 3.19.1) compiler ( protoc ) installed. Go Protobuf installation instructions are here . Go Protobuf uses C bindings, so you will need to install C++ protobuf as a prerequisite. Mainflux uses Protocol Buffers for Go with Gadgets to generate faster marshaling and unmarshaling Go code. Protocol Buffers for Go with Gadgets installation instructions can be found here . A copy of Go (version 1.17.5) and docker template (version 3.7) will also need to be installed on your system. If any of these versions seem outdated, the latest can always be found in our CI script . Build All Services # Use the GNU Make tool to build all Mainflux services: make Build artifacts will be put in the build directory. N.B. All Mainflux services are built as a statically linked binaries. This way they can be portable (transferred to any platform just by placing them there and running them) as they contain all needed libraries and do not relay on shared system libraries. This helps creating FROM scratch dockers. Build Individual Microservice # Individual microservices can be built with: make <microservice_name> For example: make http will build the HTTP Adapter microservice. Building Dockers # Dockers can be built with: make dockers or individually with: make docker_<microservice_name> For example: make docker_http N.B. Mainflux creates FROM scratch docker containers which are compact and small in size. N.B. The things-db and users-db containers are built from a vanilla PostgreSQL docker image downloaded from docker hub which does not persist the data when these containers are rebuilt. Thus, rebuilding of all docker containers with make dockers or rebuilding the things-db and users-db containers separately with make docker_things-db and make docker_users-db respectively, will cause data loss. All your users, things, channels and connections between them will be lost! As we use this setup only for development, we don't guarantee any permanent data persistence. Though, in order to enable data retention, we have configured persistent volumes for each container that stores some data. If you want to update your Mainflux dockerized installation and want to keep your data, use make cleandocker to clean the containers and images and keep the data (stored in docker persistent volumes) and then make run to update the images and the containers. Check the Cleaning up your dockerized Mainflux setup section for details. Please note that this kind of updating might not work if there are database changes. Building Docker images for development # In order to speed up build process, you can use commands such as: make dockers_dev or individually with make docker_dev_<microservice_name> Commands make dockers and make dockers_dev are similar. The main difference is that building images in the development mode is done on the local machine, rather than an intermediate image, which makes building images much faster. Before running this command, corresponding binary needs to be built in order to make changes visible. This can be done using make or make <service_name> command. Commands make dockers_dev and make docker_dev_<service_name> should be used only for development to speed up the process of image building. For deployment images, commands from section above should be used. Suggested workflow # When the project is first cloned to your system, you will need to make sure and build all of the Mainflux services. make make dockers_dev As you develop and test changes, only the services related to your changes will need to be rebuilt. This will reduce compile time and create a much more enjoyable development experience. make <microservice_name> make docker_dev_<microservice_name> make run Overriding the default docker-compose configuration # Sometimes, depending on the use case and the user's needs it might be useful to override or add some extra parameters to the docker-compose configuration. These configuration changes can be done by specifying multiple compose files with the docker-compose command line option -f as described here . The following format of the docker-compose command can be used to extend or override the configuration: docker-compose -f docker/docker-compose.yml -f docker/docker-compose.custom1.yml -f docker/docker-compose.custom2.yml up [-d] In the command above each successive file overrides the previous parameters. A practical example in our case would be to enable debugging and tracing in NATS so that we can see better how are the messages moving around. docker-compose.nats-debugging.yml version: \"3\" services: nats: command: --debug -DV When we have the override files in place, to compose the whole infrastructure including the persistent volumes we can execute: docker-compose -f docker/docker-compose.yml -f docker/docker-compose.nats-debugging.yml up -d Note: Please store your customizations to some folder outside the Mainflux's source folder and maybe add them to some other git repository. You can always apply your customizations by pointing to the right file using docker-compose -f ... . Cleaning up your dockerized Mainflux setup # If you want to clean your whole dockerized Mainflux installation you can use the make pv=true cleandocker command. Please note that by default the make cleandocker command will stop and delete all of the containers and images, but NOT DELETE persistent volumes . If you want to delete the gathered data in the system (the persistent volumes) please use the following command make pv=true cleandocker (pv = persistent volumes). This form of the command will stop and delete the containers, the images and will also delete the persistent volumes. MQTT Microservice # The MQTT Microservice in Mainflux is special, as it is currently the only microservice written in NodeJS. It is not compiled, but node modules need to be downloaded in order to start the service: cd mqtt npm install Note that there is a shorthand for doing these commands with make tool: make mqtt After that, the MQTT Adapter can be started from top directory (as it needs to find *.proto files) with: node mqtt/mqtt.js Troubleshooting # Depending on your use case, MQTT topics, message size, the number of clients and the frequency with which the messages are sent it can happen that you experience some problems. Up until now it has been noticed that in case of high load, big messages and many clients it can happen that the MQTT microservice crashes with the following error: mainflux-mqtt | FATAL ERROR: CALL_AND_RETRY_LAST Allocation failed - JavaScript heap out of memory mainflux-mqtt exited with code 137 This problem is caused the default allowed memory in node (V8). V8 gives the user 1.7GB per default . To fix the problem you should add the following environment variable NODE_OPTIONS:--max-old-space-size=SPACE_IN_MB in the environment section of the aedes.yml configuration. To find the right value for the --max-old-space-size parameter you'll have to experiment a bit depending on your needs. The Mainflux MQTT service uses the Aedes MQTT Broker for implementation of the MQTT related things. Therefore, for some questions or problems you can also check out the Aedes's documentation or reach out its contributors. Protobuf # If you've made any changes to .proto files, you should call protoc command prior to compiling individual microservices. To do this by hand, execute: protoc --gofast_out=plugins=grpc:. *.proto protoc --gogo_out=plugins=grpc:. broker/*.proto A shorthand to do this via make tool is: make proto N.B. This must be done once at the beginning in order to generate protobuf Go structures needed for the build. However, if you don't change any of .proto files, this step is not mandatory, since all generated files are included in the repository (those are files with .pb.go extension). Cross-compiling for ARM # Mainflux can be compiled for ARM platform and run on Raspberry Pi or other similar IoT gateways, by following the instructions here or here as well as information found here . The environment variables GOARCH=arm and GOARM=7 must be set for the compilation. Cross-compilation for ARM with Mainflux make: GOOS=linux GOARCH=arm GOARM=7 make Running tests # To run all of the tests you can execute: make test Dockertest is used for the tests, so to run them, you will need the Docker daemon/service running. Installing # Installing Go binaries is simple: just move them from build to $GOBIN (do not fortget to add $GOBIN to your $PATH ). You can execute: make install which will do this copying of the binaries. N.B. Only Go binaries will be installed this way. The MQTT adapter is a NodeJS script and will stay in the mqtt dir. Deployment # Prerequisites # Mainflux depends on several infrastructural services, notably NATS broker and PostgreSQL database. NATS # Mainflux uses NATS as it's central message bus. For development purposes (when not run via Docker), it expects that NATS is installed on the local system. To do this execute: go get github.com/nats-io/gnatsd This will install gnatsd binary that can be simply run by executing: gnatsd PostgreSQL # Mainflux uses PostgreSQL to store metadata ( users , things and channels entities alongside with authorization tokens). It expects that PostgreSQL DB is installed, set up and running on the local system. Information how to set-up (prepare) PostgreSQL database can be found here , and it is done by executing following commands: # Create `users` and `things` databases sudo -u postgres createdb users sudo -u postgres createdb things # Set-up Postgres roles sudo su - postgres psql -U postgres postgres=# CREATE ROLE mainflux WITH LOGIN ENCRYPTED PASSWORD 'mainflux'; postgres=# ALTER USER mainflux WITH LOGIN ENCRYPTED PASSWORD 'mainflux'; Mainflux Services # Running of the Mainflux microservices can be tricky, as there is a lot of them and each demand configuration in the form of environment variables. The whole system (set of microservices) can be run with one command: make rundev which will properly configure and run all microservices. Please assure that MQTT microservice has node_modules installed, as explained in MQTT Microservice chapter. N.B. make rundev actually calls helper script scripts/run.sh , so you can inspect this script for the details. Events # In order to be easily integratable system, Mainflux is using Redis Streams as an event log for event sourcing. Services that are publishing events to Redis Streams are things service, bootstrap service and mqtt adapter. Things Service # For every operation that has side effects (that is changing service state) things service will generate new event and publish it to Redis Stream called mainflux.things . Every event has its own event ID that is automatically generated and operation field that can have one of the following values: - thing.create for thing creation, - thing.update for thing update, - thing.remove for thing removal, - thing.connect for connecting a thing to a channel, - thing.disconnect for disconnecting thing from a channel, - channel.create for channel creation, - channel.update for channel update, - channel.remove for channel removal. By fetching and processing these events you can reconstruct things service state. If you store some of your custom data in metadata field, this is the perfect way to fetch it and process it. If you want to integrate through docker-compose.yml you can use mainflux-es-redis service. Just connect to it and consume events from Redis Stream named mainflux.things . Thing create event # Whenever thing is created, things service will generate new create event. This event will have the following format: 1) \"1555334740911-0\" 2) 1) \"operation\" 2) \"thing.create\" 3) \"name\" 4) \"d0\" 5) \"id\" 6) \"3c36273a-94ea-4802-84d6-a51de140112e\" 7) \"owner\" 8) \"john.doe@email.com\" 9) \"metadata\" 10) \"{}\" As you can see from this example, every odd field represents field name while every even field represents field value. This is standard event format for Redis Streams. If you want to extract metadata field from this event, you'll have to read it as string first and then you can deserialize it to some structured format. Thing update event # Whenever thing instance is updated, things service will generate new update event. This event will have the following format: 1) \"1555336161544-0\" 2) 1) \"operation\" 2) \"thing.update\" 3) \"name\" 4) \"weio\" 5) \"id\" 6) \"3c36273a-94ea-4802-84d6-a51de140112e\" Note that thing update event will contain only those fields that were updated using update endpoint. Thing remove event # Whenever thing instance is removed from the system, things service will generate and publish new remove event. This event will have the following format: 1) 1) \"1555339313003-0\" 2) 1) \"id\" 2) \"3c36273a-94ea-4802-84d6-a51de140112e\" 3) \"operation\" 4) \"thing.remove\" Channel create event # Whenever channel instance is created, things service will generate and publish new create event. This event will have the following format: 1) \"1555334740918-0\" 2) 1) \"id\" 2) \"16fb2748-8d3b-4783-b272-bb5f4ad4d661\" 3) \"owner\" 4) \"john.doe@email.com\" 5) \"operation\" 6) \"channel.create\" 7) \"name\" 8) \"c1\" Channel update event # Whenever channel instance is updated, things service will generate and publish new update event. This event will have the following format: 1) \"1555338870341-0\" 2) 1) \"name\" 2) \"chan\" 3) \"id\" 4) \"d9d8f31b-f8d4-49c5-b943-6db10d8e2949\" 5) \"operation\" 6) \"channel.update\" Note that update channel event will contain only those fields that were updated using update channel endpoint. Channel remove event # Whenever channel instance is removed from the system, things service will generate and publish new remove event. This event will have the following format: 1) 1) \"1555339429661-0\" 2) 1) \"id\" 2) \"d9d8f31b-f8d4-49c5-b943-6db10d8e2949\" 3) \"operation\" 4) \"channel.remove\" Connect thing to a channel event # Whenever thing is connected to a channel on things service, things service will generate and publish new connect event. This event will have the following format: 1) \"1555334740920-0\" 2) 1) \"chan_id\" 2) \"d9d8f31b-f8d4-49c5-b943-6db10d8e2949\" 3) \"thing_id\" 4) \"3c36273a-94ea-4802-84d6-a51de140112e\" 5) \"operation\" 6) \"thing.connect\" Disconnect thing from a channel event # Whenever thing is disconnected from a channel on things service, things service will generate and publish new disconnect event. This event will have the following format: 1) \"1555334740920-0\" 2) 1) \"chan_id\" 2) \"d9d8f31b-f8d4-49c5-b943-6db10d8e2949\" 3) \"thing_id\" 4) \"3c36273a-94ea-4802-84d6-a51de140112e\" 5) \"operation\" 6) \"thing.disconnect\" Note: Every one of these events will omit fields that were not used or are not relevant for specific operation. Also, field ordering is not guaranteed, so DO NOT rely on it. Bootstrap Service # Bootstrap service publishes events to Redis Stream called mainflux.bootstrap . Every event from this service contains operation field which indicates one of the following event types: - config.create for configuration creation, - config.update for configuration update, - config.remove for configuration removal, - thing.bootstrap for device bootstrap, - thing.state_change for device state change, - thing.update_connections for device connection update. If you want to integrate through docker-compose.yml you can use mainflux-es-redis service. Just connect to it and consume events from Redis Stream named mainflux.bootstrap . Configuration create event # Whenever configuration is created, bootstrap service will generate and publish new create event. This event will have the following format: 1) \"1555404899581-0\" 2) 1) \"owner\" 2) \"john.doe@email.com\" 3) \"name\" 4) \"some\" 5) \"channels\" 6) \"ff13ca9c-7322-4c28-a25c-4fe5c7b753fc, c3642289-501d-4974-82f2-ecccc71b2d82, c3642289-501d-4974-82f2-ecccc71b2d83, cd4ce940-9173-43e3-86f7-f788e055eb14\" 7) \"externalID\" 8) \"9c:b6:d:eb:9f:fd\" 9) \"content\" 10) \"{}\" 11) \"timestamp\" 12) \"1555404899\" 13) \"operation\" 14) \"config.create\" 15) \"thing_id\" 16) \"63a110d4-2b77-48d2-aa46-2582681eeb82\" Configuration update event # Whenever configuration is updated, bootstrap service will generate and publish new update event. This event will have the following format: 1) \"1555405104368-0\" 2) 1) \"content\" 2) \"NOV_MGT_HOST: http://127.0.0.1:7000\\nDOCKER_MGT_HOST: http://127.0.0.1:2375\\nAGENT_MGT_HOST: https://127.0.0.1:7003\\nMF_MQTT_HOST: tcp://104.248.142.133:8443\" 3) \"timestamp\" 4) \"1555405104\" 5) \"operation\" 6) \"config.update\" 7) \"thing_id\" 8) \"63a110d4-2b77-48d2-aa46-2582681eeb82\" 9) \"name\" 10) \"weio\" Configuration remove event # Whenever configuration is removed, bootstrap service will generate and publish new remove event. This event will have the following format: 1) \"1555405464328-0\" 2) 1) \"thing_id\" 2) \"63a110d4-2b77-48d2-aa46-2582681eeb82\" 3) \"timestamp\" 4) \"1555405464\" 5) \"operation\" 6) \"config.remove\" Thing bootstrap event # Whenever thing is bootstrapped, bootstrap service will generate and publish new bootstrap event. This event will have the following format: 1) \"1555405173785-0\" 2) 1) \"externalID\" 2) \"9c:b6:d:eb:9f:fd\" 3) \"success\" 4) \"1\" 5) \"timestamp\" 6) \"1555405173\" 7) \"operation\" 8) \"thing.bootstrap\" Thing change state event # Whenever thing's state changes, bootstrap service will generate and publish new change state event. This event will have the following format: 1) \"1555405294806-0\" 2) 1) \"thing_id\" 2) \"63a110d4-2b77-48d2-aa46-2582681eeb82\" 3) \"state\" 4) \"0\" 5) \"timestamp\" 6) \"1555405294\" 7) \"operation\" 8) \"thing.state_change\" Thing update connections event # Whenever thing's list of connections is updated, bootstrap service will generate and publish new update connections event. This event will have the following format: 1) \"1555405373360-0\" 2) 1) \"operation\" 2) \"thing.update_connections\" 3) \"thing_id\" 4) \"63a110d4-2b77-48d2-aa46-2582681eeb82\" 5) \"channels\" 6) \"ff13ca9c-7322-4c28-a25c-4fe5c7b753fc, 925461e6-edfb-4755-9242-8a57199b90a5, c3642289-501d-4974-82f2-ecccc71b2d82\" 7) \"timestamp\" 8) \"1555405373\" MQTT Adapter # Instead of using heartbeat to know when client is connected through MQTT adapter one can fetch events from Redis Streams that MQTT adapter publishes. MQTT adapter publishes events every time client connects and disconnects to stream named mainflux.mqtt . Events that are coming from MQTT adapter have following fields: - thing_id ID of a thing that has connected to MQTT adapter, - timestamp is in Epoch UNIX Time Stamp format, - event_type can have two possible values, connect and disconnect, - instance represents MQTT adapter instance. If you want to integrate through docker-compose.yml you can use mainflux-es-redis service. Just connect to it and consume events from Redis Stream named mainflux.mqtt . Example of connect event: 1) 1) \"1555351214144-0\" 2) 1) \"thing_id\" 2) \"1c597a85-b68e-42ff-8ed8-a3a761884bc4\" 3) \"timestamp\" 4) \"1555351214\" 5) \"event_type\" 6) \"connect\" 7) \"instance\" 8) \"mqtt-adapter-1\" Example of disconnect event: 1) 1) \"1555351214188-0\" 2) 1) \"thing_id\" 2) \"1c597a85-b68e-42ff-8ed8-a3a761884bc4\" 3) \"timestamp\" 4) \"1555351214\" 5) \"event_type\" 6) \"disconnect\" 7) \"instance\" 8) \"mqtt-adapter-1\"","title":"Developer's Guide"},{"location":"dev-guide/#developers-guide","text":"","title":"Developer's guide"},{"location":"dev-guide/#getting-mainflux","text":"Mainflux source can be found in the official Mainflux GitHub repository . You should fork this repository in order to make changes to the project. The forked version of the repository should be cloned using the following: git clone <forked repository> $SOMEPATH/mainflux cd $SOMEPATH/mainflux Note: If your $SOMEPATH is equal to $GOPATH/src/github.com/mainflux/mainflux , make sure that your $GOROOT and $GOPATH do not overlap (otherwise, go modules won't work).","title":"Getting Mainflux"},{"location":"dev-guide/#building","text":"","title":"Building"},{"location":"dev-guide/#prerequisites","text":"Make sure that you have Protocol Buffers (version 3.19.1) compiler ( protoc ) installed. Go Protobuf installation instructions are here . Go Protobuf uses C bindings, so you will need to install C++ protobuf as a prerequisite. Mainflux uses Protocol Buffers for Go with Gadgets to generate faster marshaling and unmarshaling Go code. Protocol Buffers for Go with Gadgets installation instructions can be found here . A copy of Go (version 1.17.5) and docker template (version 3.7) will also need to be installed on your system. If any of these versions seem outdated, the latest can always be found in our CI script .","title":"Prerequisites"},{"location":"dev-guide/#build-all-services","text":"Use the GNU Make tool to build all Mainflux services: make Build artifacts will be put in the build directory. N.B. All Mainflux services are built as a statically linked binaries. This way they can be portable (transferred to any platform just by placing them there and running them) as they contain all needed libraries and do not relay on shared system libraries. This helps creating FROM scratch dockers.","title":"Build All Services"},{"location":"dev-guide/#build-individual-microservice","text":"Individual microservices can be built with: make <microservice_name> For example: make http will build the HTTP Adapter microservice.","title":"Build Individual Microservice"},{"location":"dev-guide/#building-dockers","text":"Dockers can be built with: make dockers or individually with: make docker_<microservice_name> For example: make docker_http N.B. Mainflux creates FROM scratch docker containers which are compact and small in size. N.B. The things-db and users-db containers are built from a vanilla PostgreSQL docker image downloaded from docker hub which does not persist the data when these containers are rebuilt. Thus, rebuilding of all docker containers with make dockers or rebuilding the things-db and users-db containers separately with make docker_things-db and make docker_users-db respectively, will cause data loss. All your users, things, channels and connections between them will be lost! As we use this setup only for development, we don't guarantee any permanent data persistence. Though, in order to enable data retention, we have configured persistent volumes for each container that stores some data. If you want to update your Mainflux dockerized installation and want to keep your data, use make cleandocker to clean the containers and images and keep the data (stored in docker persistent volumes) and then make run to update the images and the containers. Check the Cleaning up your dockerized Mainflux setup section for details. Please note that this kind of updating might not work if there are database changes.","title":"Building Dockers"},{"location":"dev-guide/#building-docker-images-for-development","text":"In order to speed up build process, you can use commands such as: make dockers_dev or individually with make docker_dev_<microservice_name> Commands make dockers and make dockers_dev are similar. The main difference is that building images in the development mode is done on the local machine, rather than an intermediate image, which makes building images much faster. Before running this command, corresponding binary needs to be built in order to make changes visible. This can be done using make or make <service_name> command. Commands make dockers_dev and make docker_dev_<service_name> should be used only for development to speed up the process of image building. For deployment images, commands from section above should be used.","title":"Building Docker images for development"},{"location":"dev-guide/#suggested-workflow","text":"When the project is first cloned to your system, you will need to make sure and build all of the Mainflux services. make make dockers_dev As you develop and test changes, only the services related to your changes will need to be rebuilt. This will reduce compile time and create a much more enjoyable development experience. make <microservice_name> make docker_dev_<microservice_name> make run","title":"Suggested workflow"},{"location":"dev-guide/#overriding-the-default-docker-compose-configuration","text":"Sometimes, depending on the use case and the user's needs it might be useful to override or add some extra parameters to the docker-compose configuration. These configuration changes can be done by specifying multiple compose files with the docker-compose command line option -f as described here . The following format of the docker-compose command can be used to extend or override the configuration: docker-compose -f docker/docker-compose.yml -f docker/docker-compose.custom1.yml -f docker/docker-compose.custom2.yml up [-d] In the command above each successive file overrides the previous parameters. A practical example in our case would be to enable debugging and tracing in NATS so that we can see better how are the messages moving around. docker-compose.nats-debugging.yml version: \"3\" services: nats: command: --debug -DV When we have the override files in place, to compose the whole infrastructure including the persistent volumes we can execute: docker-compose -f docker/docker-compose.yml -f docker/docker-compose.nats-debugging.yml up -d Note: Please store your customizations to some folder outside the Mainflux's source folder and maybe add them to some other git repository. You can always apply your customizations by pointing to the right file using docker-compose -f ... .","title":"Overriding the default docker-compose configuration"},{"location":"dev-guide/#cleaning-up-your-dockerized-mainflux-setup","text":"If you want to clean your whole dockerized Mainflux installation you can use the make pv=true cleandocker command. Please note that by default the make cleandocker command will stop and delete all of the containers and images, but NOT DELETE persistent volumes . If you want to delete the gathered data in the system (the persistent volumes) please use the following command make pv=true cleandocker (pv = persistent volumes). This form of the command will stop and delete the containers, the images and will also delete the persistent volumes.","title":"Cleaning up your dockerized Mainflux setup"},{"location":"dev-guide/#mqtt-microservice","text":"The MQTT Microservice in Mainflux is special, as it is currently the only microservice written in NodeJS. It is not compiled, but node modules need to be downloaded in order to start the service: cd mqtt npm install Note that there is a shorthand for doing these commands with make tool: make mqtt After that, the MQTT Adapter can be started from top directory (as it needs to find *.proto files) with: node mqtt/mqtt.js","title":"MQTT Microservice"},{"location":"dev-guide/#troubleshooting","text":"Depending on your use case, MQTT topics, message size, the number of clients and the frequency with which the messages are sent it can happen that you experience some problems. Up until now it has been noticed that in case of high load, big messages and many clients it can happen that the MQTT microservice crashes with the following error: mainflux-mqtt | FATAL ERROR: CALL_AND_RETRY_LAST Allocation failed - JavaScript heap out of memory mainflux-mqtt exited with code 137 This problem is caused the default allowed memory in node (V8). V8 gives the user 1.7GB per default . To fix the problem you should add the following environment variable NODE_OPTIONS:--max-old-space-size=SPACE_IN_MB in the environment section of the aedes.yml configuration. To find the right value for the --max-old-space-size parameter you'll have to experiment a bit depending on your needs. The Mainflux MQTT service uses the Aedes MQTT Broker for implementation of the MQTT related things. Therefore, for some questions or problems you can also check out the Aedes's documentation or reach out its contributors.","title":"Troubleshooting"},{"location":"dev-guide/#protobuf","text":"If you've made any changes to .proto files, you should call protoc command prior to compiling individual microservices. To do this by hand, execute: protoc --gofast_out=plugins=grpc:. *.proto protoc --gogo_out=plugins=grpc:. broker/*.proto A shorthand to do this via make tool is: make proto N.B. This must be done once at the beginning in order to generate protobuf Go structures needed for the build. However, if you don't change any of .proto files, this step is not mandatory, since all generated files are included in the repository (those are files with .pb.go extension).","title":"Protobuf"},{"location":"dev-guide/#cross-compiling-for-arm","text":"Mainflux can be compiled for ARM platform and run on Raspberry Pi or other similar IoT gateways, by following the instructions here or here as well as information found here . The environment variables GOARCH=arm and GOARM=7 must be set for the compilation. Cross-compilation for ARM with Mainflux make: GOOS=linux GOARCH=arm GOARM=7 make","title":"Cross-compiling for ARM"},{"location":"dev-guide/#running-tests","text":"To run all of the tests you can execute: make test Dockertest is used for the tests, so to run them, you will need the Docker daemon/service running.","title":"Running tests"},{"location":"dev-guide/#installing","text":"Installing Go binaries is simple: just move them from build to $GOBIN (do not fortget to add $GOBIN to your $PATH ). You can execute: make install which will do this copying of the binaries. N.B. Only Go binaries will be installed this way. The MQTT adapter is a NodeJS script and will stay in the mqtt dir.","title":"Installing"},{"location":"dev-guide/#deployment","text":"","title":"Deployment"},{"location":"dev-guide/#prerequisites_1","text":"Mainflux depends on several infrastructural services, notably NATS broker and PostgreSQL database.","title":"Prerequisites"},{"location":"dev-guide/#nats","text":"Mainflux uses NATS as it's central message bus. For development purposes (when not run via Docker), it expects that NATS is installed on the local system. To do this execute: go get github.com/nats-io/gnatsd This will install gnatsd binary that can be simply run by executing: gnatsd","title":"NATS"},{"location":"dev-guide/#postgresql","text":"Mainflux uses PostgreSQL to store metadata ( users , things and channels entities alongside with authorization tokens). It expects that PostgreSQL DB is installed, set up and running on the local system. Information how to set-up (prepare) PostgreSQL database can be found here , and it is done by executing following commands: # Create `users` and `things` databases sudo -u postgres createdb users sudo -u postgres createdb things # Set-up Postgres roles sudo su - postgres psql -U postgres postgres=# CREATE ROLE mainflux WITH LOGIN ENCRYPTED PASSWORD 'mainflux'; postgres=# ALTER USER mainflux WITH LOGIN ENCRYPTED PASSWORD 'mainflux';","title":"PostgreSQL"},{"location":"dev-guide/#mainflux-services","text":"Running of the Mainflux microservices can be tricky, as there is a lot of them and each demand configuration in the form of environment variables. The whole system (set of microservices) can be run with one command: make rundev which will properly configure and run all microservices. Please assure that MQTT microservice has node_modules installed, as explained in MQTT Microservice chapter. N.B. make rundev actually calls helper script scripts/run.sh , so you can inspect this script for the details.","title":"Mainflux Services"},{"location":"dev-guide/#events","text":"In order to be easily integratable system, Mainflux is using Redis Streams as an event log for event sourcing. Services that are publishing events to Redis Streams are things service, bootstrap service and mqtt adapter.","title":"Events"},{"location":"dev-guide/#things-service","text":"For every operation that has side effects (that is changing service state) things service will generate new event and publish it to Redis Stream called mainflux.things . Every event has its own event ID that is automatically generated and operation field that can have one of the following values: - thing.create for thing creation, - thing.update for thing update, - thing.remove for thing removal, - thing.connect for connecting a thing to a channel, - thing.disconnect for disconnecting thing from a channel, - channel.create for channel creation, - channel.update for channel update, - channel.remove for channel removal. By fetching and processing these events you can reconstruct things service state. If you store some of your custom data in metadata field, this is the perfect way to fetch it and process it. If you want to integrate through docker-compose.yml you can use mainflux-es-redis service. Just connect to it and consume events from Redis Stream named mainflux.things .","title":"Things Service"},{"location":"dev-guide/#thing-create-event","text":"Whenever thing is created, things service will generate new create event. This event will have the following format: 1) \"1555334740911-0\" 2) 1) \"operation\" 2) \"thing.create\" 3) \"name\" 4) \"d0\" 5) \"id\" 6) \"3c36273a-94ea-4802-84d6-a51de140112e\" 7) \"owner\" 8) \"john.doe@email.com\" 9) \"metadata\" 10) \"{}\" As you can see from this example, every odd field represents field name while every even field represents field value. This is standard event format for Redis Streams. If you want to extract metadata field from this event, you'll have to read it as string first and then you can deserialize it to some structured format.","title":"Thing create event"},{"location":"dev-guide/#thing-update-event","text":"Whenever thing instance is updated, things service will generate new update event. This event will have the following format: 1) \"1555336161544-0\" 2) 1) \"operation\" 2) \"thing.update\" 3) \"name\" 4) \"weio\" 5) \"id\" 6) \"3c36273a-94ea-4802-84d6-a51de140112e\" Note that thing update event will contain only those fields that were updated using update endpoint.","title":"Thing update event"},{"location":"dev-guide/#thing-remove-event","text":"Whenever thing instance is removed from the system, things service will generate and publish new remove event. This event will have the following format: 1) 1) \"1555339313003-0\" 2) 1) \"id\" 2) \"3c36273a-94ea-4802-84d6-a51de140112e\" 3) \"operation\" 4) \"thing.remove\"","title":"Thing remove event"},{"location":"dev-guide/#channel-create-event","text":"Whenever channel instance is created, things service will generate and publish new create event. This event will have the following format: 1) \"1555334740918-0\" 2) 1) \"id\" 2) \"16fb2748-8d3b-4783-b272-bb5f4ad4d661\" 3) \"owner\" 4) \"john.doe@email.com\" 5) \"operation\" 6) \"channel.create\" 7) \"name\" 8) \"c1\"","title":"Channel create event"},{"location":"dev-guide/#channel-update-event","text":"Whenever channel instance is updated, things service will generate and publish new update event. This event will have the following format: 1) \"1555338870341-0\" 2) 1) \"name\" 2) \"chan\" 3) \"id\" 4) \"d9d8f31b-f8d4-49c5-b943-6db10d8e2949\" 5) \"operation\" 6) \"channel.update\" Note that update channel event will contain only those fields that were updated using update channel endpoint.","title":"Channel update event"},{"location":"dev-guide/#channel-remove-event","text":"Whenever channel instance is removed from the system, things service will generate and publish new remove event. This event will have the following format: 1) 1) \"1555339429661-0\" 2) 1) \"id\" 2) \"d9d8f31b-f8d4-49c5-b943-6db10d8e2949\" 3) \"operation\" 4) \"channel.remove\"","title":"Channel remove event"},{"location":"dev-guide/#connect-thing-to-a-channel-event","text":"Whenever thing is connected to a channel on things service, things service will generate and publish new connect event. This event will have the following format: 1) \"1555334740920-0\" 2) 1) \"chan_id\" 2) \"d9d8f31b-f8d4-49c5-b943-6db10d8e2949\" 3) \"thing_id\" 4) \"3c36273a-94ea-4802-84d6-a51de140112e\" 5) \"operation\" 6) \"thing.connect\"","title":"Connect thing to a channel event"},{"location":"dev-guide/#disconnect-thing-from-a-channel-event","text":"Whenever thing is disconnected from a channel on things service, things service will generate and publish new disconnect event. This event will have the following format: 1) \"1555334740920-0\" 2) 1) \"chan_id\" 2) \"d9d8f31b-f8d4-49c5-b943-6db10d8e2949\" 3) \"thing_id\" 4) \"3c36273a-94ea-4802-84d6-a51de140112e\" 5) \"operation\" 6) \"thing.disconnect\" Note: Every one of these events will omit fields that were not used or are not relevant for specific operation. Also, field ordering is not guaranteed, so DO NOT rely on it.","title":"Disconnect thing from a channel event"},{"location":"dev-guide/#bootstrap-service","text":"Bootstrap service publishes events to Redis Stream called mainflux.bootstrap . Every event from this service contains operation field which indicates one of the following event types: - config.create for configuration creation, - config.update for configuration update, - config.remove for configuration removal, - thing.bootstrap for device bootstrap, - thing.state_change for device state change, - thing.update_connections for device connection update. If you want to integrate through docker-compose.yml you can use mainflux-es-redis service. Just connect to it and consume events from Redis Stream named mainflux.bootstrap .","title":"Bootstrap Service"},{"location":"dev-guide/#configuration-create-event","text":"Whenever configuration is created, bootstrap service will generate and publish new create event. This event will have the following format: 1) \"1555404899581-0\" 2) 1) \"owner\" 2) \"john.doe@email.com\" 3) \"name\" 4) \"some\" 5) \"channels\" 6) \"ff13ca9c-7322-4c28-a25c-4fe5c7b753fc, c3642289-501d-4974-82f2-ecccc71b2d82, c3642289-501d-4974-82f2-ecccc71b2d83, cd4ce940-9173-43e3-86f7-f788e055eb14\" 7) \"externalID\" 8) \"9c:b6:d:eb:9f:fd\" 9) \"content\" 10) \"{}\" 11) \"timestamp\" 12) \"1555404899\" 13) \"operation\" 14) \"config.create\" 15) \"thing_id\" 16) \"63a110d4-2b77-48d2-aa46-2582681eeb82\"","title":"Configuration create event"},{"location":"dev-guide/#configuration-update-event","text":"Whenever configuration is updated, bootstrap service will generate and publish new update event. This event will have the following format: 1) \"1555405104368-0\" 2) 1) \"content\" 2) \"NOV_MGT_HOST: http://127.0.0.1:7000\\nDOCKER_MGT_HOST: http://127.0.0.1:2375\\nAGENT_MGT_HOST: https://127.0.0.1:7003\\nMF_MQTT_HOST: tcp://104.248.142.133:8443\" 3) \"timestamp\" 4) \"1555405104\" 5) \"operation\" 6) \"config.update\" 7) \"thing_id\" 8) \"63a110d4-2b77-48d2-aa46-2582681eeb82\" 9) \"name\" 10) \"weio\"","title":"Configuration update event"},{"location":"dev-guide/#configuration-remove-event","text":"Whenever configuration is removed, bootstrap service will generate and publish new remove event. This event will have the following format: 1) \"1555405464328-0\" 2) 1) \"thing_id\" 2) \"63a110d4-2b77-48d2-aa46-2582681eeb82\" 3) \"timestamp\" 4) \"1555405464\" 5) \"operation\" 6) \"config.remove\"","title":"Configuration remove event"},{"location":"dev-guide/#thing-bootstrap-event","text":"Whenever thing is bootstrapped, bootstrap service will generate and publish new bootstrap event. This event will have the following format: 1) \"1555405173785-0\" 2) 1) \"externalID\" 2) \"9c:b6:d:eb:9f:fd\" 3) \"success\" 4) \"1\" 5) \"timestamp\" 6) \"1555405173\" 7) \"operation\" 8) \"thing.bootstrap\"","title":"Thing bootstrap event"},{"location":"dev-guide/#thing-change-state-event","text":"Whenever thing's state changes, bootstrap service will generate and publish new change state event. This event will have the following format: 1) \"1555405294806-0\" 2) 1) \"thing_id\" 2) \"63a110d4-2b77-48d2-aa46-2582681eeb82\" 3) \"state\" 4) \"0\" 5) \"timestamp\" 6) \"1555405294\" 7) \"operation\" 8) \"thing.state_change\"","title":"Thing change state event"},{"location":"dev-guide/#thing-update-connections-event","text":"Whenever thing's list of connections is updated, bootstrap service will generate and publish new update connections event. This event will have the following format: 1) \"1555405373360-0\" 2) 1) \"operation\" 2) \"thing.update_connections\" 3) \"thing_id\" 4) \"63a110d4-2b77-48d2-aa46-2582681eeb82\" 5) \"channels\" 6) \"ff13ca9c-7322-4c28-a25c-4fe5c7b753fc, 925461e6-edfb-4755-9242-8a57199b90a5, c3642289-501d-4974-82f2-ecccc71b2d82\" 7) \"timestamp\" 8) \"1555405373\"","title":"Thing update connections event"},{"location":"dev-guide/#mqtt-adapter","text":"Instead of using heartbeat to know when client is connected through MQTT adapter one can fetch events from Redis Streams that MQTT adapter publishes. MQTT adapter publishes events every time client connects and disconnects to stream named mainflux.mqtt . Events that are coming from MQTT adapter have following fields: - thing_id ID of a thing that has connected to MQTT adapter, - timestamp is in Epoch UNIX Time Stamp format, - event_type can have two possible values, connect and disconnect, - instance represents MQTT adapter instance. If you want to integrate through docker-compose.yml you can use mainflux-es-redis service. Just connect to it and consume events from Redis Stream named mainflux.mqtt . Example of connect event: 1) 1) \"1555351214144-0\" 2) 1) \"thing_id\" 2) \"1c597a85-b68e-42ff-8ed8-a3a761884bc4\" 3) \"timestamp\" 4) \"1555351214\" 5) \"event_type\" 6) \"connect\" 7) \"instance\" 8) \"mqtt-adapter-1\" Example of disconnect event: 1) 1) \"1555351214188-0\" 2) 1) \"thing_id\" 2) \"1c597a85-b68e-42ff-8ed8-a3a761884bc4\" 3) \"timestamp\" 4) \"1555351214\" 5) \"event_type\" 6) \"disconnect\" 7) \"instance\" 8) \"mqtt-adapter-1\"","title":"MQTT Adapter"},{"location":"edge/","text":"Edge # Mainflux IoT platform provides services for supporting management of devices on the edge. Typically, IoT solution includes devices (sensors/actuators) deployed in far edge and connected through some proxy gateway. Although most devices could be connected to the Mainflux directly, using gateways decentralizes system, decreases load on the cloud and makes setup less difficult. Also, gateways can provide additional data processing, filtering and storage. Services that can be used on gateway to enable data and control plane for edge: Agent Export Mainflux Figure 1 - Edge services deployment Figure shows edge gateway that is running Agent, Export and minimal deployment of Mainflux services. Mainflux services enable device management and MQTT protocol, NATS being a central message bus in Mainflux becomes also central message bus for other services like Agent and Export as well as for any new custom developed service that can be built to interface with devices with any of hardware supported interfaces on the gateway, those services would publish data to NATS where Export service can pick them up and send to cloud. Agent can be used to control deployed services as well as to monitor their liveliness through subcribing to heartbeat NATS subject where services should publish their liveliness status, like Export service does. Agent # Agent is service that is used to manage gateways that are connected to Mainflux in cloud. It provides a way to send commands to gateway and receive response via mqtt. There are two types of channels used for Agent data and control . Over the control we are sending commands and receiving response from commands. Data collected from sensors connected to gateway are being sent over data channel. Agent is able to configure itself provided that bootstrap server is running, it will retrieve configuration from bootstrap server provided few arguments - external_id and external_key see bootstraping . Agent service has following features: * Remote execution of commands * Remote terminal, remote session to bash managed by Agent * Heartbeat - listening to NATS topic heartbeat.> it can remotely provide info on running services, if services are publishing heartbeat ( like Export ) * Proxying commands to other gateway services * Edgex SMA - remotely making requests to EdgeX endpoints and fetching results, if EdgeX is deployed. Run Agent # Before running agent we need to provision a thing and DATA and CONTROL channel. Thing that will be used as gateway representation and make bootstrap configuration. If using Mainflux UI this is done automatically when adding gateway through UI. Gateway can be provisioned with provision service. When you provisioned gateway as described in provision you can check results curl -s -S -X GET http://mainflux-domain.com:8202/things/bootstrap/<external_id> -H \"Authorization: Thing <external_key>\" -H 'Content-Type: application/json' |jq { \"mainflux_id\": \"e22c383a-d2ab-47c1-89cd-903955da993d\", \"mainflux_key\": \"fc987711-1828-461b-aa4b-16d5b2c642fe\", \"mainflux_channels\": [ { \"id\": \"fa5f9ba8-a1fc-4380-9edb-d0c23eaa24ec\", \"name\": \"control-channel\", \"metadata\": { \"type\": \"control\" } }, { \"id\": \"24e5473e-3cbe-43d9-8a8b-a725ff918c0e\", \"name\": \"data-channel\", \"metadata\": { \"type\": \"data\" } }, { \"id\": \"1eac45c2-0f72-4089-b255-ebd2e5732bbb\", \"name\": \"export-channel\", \"metadata\": { \"type\": \"export\" } } ], \"content\": \"{\\\"agent\\\":{\\\"edgex\\\":{\\\"url\\\":\\\"http://localhost:48090/api/v1/\\\"},\\\"heartbeat\\\":{\\\"interval\\\":\\\"30s\\\"},\\\"log\\\":{\\\"level\\\":\\\"debug\\\"},\\\"mqtt\\\":{\\\"mtls\\\":false,\\\"qos\\\":0,\\\"retain\\\":false,\\\"skip_tls_ver\\\":true,\\\"url\\\":\\\"tcp://mainflux-domain.com:1883\\\"},\\\"server\\\":{\\\"nats_url\\\":\\\"localhost:4222\\\",\\\"port\\\":\\\"9000\\\"},\\\"terminal\\\":{\\\"session_timeout\\\":\\\"30s\\\"}},\\\"export\\\":{\\\"exp\\\":{\\\"cache_db\\\":\\\"0\\\",\\\"cache_pass\\\":\\\"\\\",\\\"cache_url\\\":\\\"localhost:6379\\\",\\\"log_level\\\":\\\"debug\\\",\\\"nats\\\":\\\"nats://localhost:4222\\\",\\\"port\\\":\\\"8172\\\"},\\\"mqtt\\\":{\\\"ca_path\\\":\\\"ca.crt\\\",\\\"cert_path\\\":\\\"thing.crt\\\",\\\"channel\\\":\\\"\\\",\\\"host\\\":\\\"tcp://mainflux-domain.com:1883\\\",\\\"mtls\\\":false,\\\"password\\\":\\\"\\\",\\\"priv_key_path\\\":\\\"thing.key\\\",\\\"qos\\\":0,\\\"retain\\\":false,\\\"skip_tls_ver\\\":false,\\\"username\\\":\\\"\\\"},\\\"routes\\\":[{\\\"mqtt_topic\\\":\\\"\\\",\\\"nats_topic\\\":\\\"channels\\\",\\\"subtopic\\\":\\\"\\\",\\\"type\\\":\\\"mfx\\\",\\\"workers\\\":10},{\\\"mqtt_topic\\\":\\\"\\\",\\\"nats_topic\\\":\\\"export\\\",\\\"subtopic\\\":\\\"\\\",\\\"type\\\":\\\"default\\\",\\\"workers\\\":10}]}}\" } external_id is usually MAC address, but anything that suits applications requirements can be used external_key is key that will be provided to agent process thing_id is mainflux thing id channels is 2-element array where first channel is CONTROL and second is DATA, both channels should be assigned to thing content is used for configuring parameters of agent and export service. Then to start the agent service you can do it like this git clone https://github.com/mainflux/agent make cd build MF_AGENT_LOG_LEVEL=debug \\ MF_AGENT_BOOTSTRAP_KEY=edged \\ MF_AGENT_BOOTSTRAP_ID=34:e1:2d:e6:cf:03 ./mainflux-agent {\"level\":\"info\",\"message\":\"Requesting config for 34:e1:2d:e6:cf:03 from http://localhost:8202/things/bootstrap\",\"ts\":\"2019-12-05T04:47:24.98411512Z\"} {\"level\":\"info\",\"message\":\"Getting config for 34:e1:2d:e6:cf:03 from http://localhost:8202/things/bootstrap succeeded\",\"ts\":\"2019-12-05T04:47:24.995465239Z\"} {\"level\":\"info\",\"message\":\"Connected to MQTT broker\",\"ts\":\"2019-12-05T04:47:25.009645082Z\"} {\"level\":\"info\",\"message\":\"Agent service started, exposed port 9000\",\"ts\":\"2019-12-05T04:47:25.009755345Z\"} {\"level\":\"info\",\"message\":\"Subscribed to MQTT broker\",\"ts\":\"2019-12-05T04:47:25.012930443Z\"} MF_AGENT_BOOTSTRAP_KEY - is external_key in bootstrap configuration. MF_AGENT_BOOSTRAP_ID - is external_id in bootstrap configuration. Remote execution of commands via Agent # # Set connection parameters as environment variables in shell CH=`curl -s -S -X GET http://some-domain-name:8202/things/bootstrap/34:e1:2d:e6:cf:03 -H \"Authorization: edged\" -H 'Content-Type: application/json' | jq -r '.mainflux_channels[0].id'` TH=`curl -s -S -X GET http://some-domain-name:8202/things/bootstrap/34:e1:2d:e6:cf:03 -H \"Authorization: edged\" -H 'Content-Type: application/json' | jq -r .mainflux_id` KEY=`curl -s -S -X GET http://some-domain-name:8202/things/bootstrap/34:e1:2d:e6:cf:03 -H \"Authorization: edged\" -H 'Content-Type: application/json' | jq -r .mainflux_key` # Subscribe for response mosquitto_sub -d -u $TH -P $KEY -t channels/$CH/messages/res/# -h some-domain-name -p 1883 # Publish command e.g `ls` mosquitto_pub -d -u $TH -P $KEY -t channels/$CH/messages/req -h some-domain-name -p 1883 -m '[{\"bn\":\"1:\", \"n\":\"exec\", \"vs\":\"ls, -l\"}]' Remote terminal # This can be checked from the UI, click on the details for gateway and below the gateway parameters you will se box with prompt, if agent is running and it is properly connected you should be able to execute commands remotely. Heartbeat # If there are services that are running on same gateway as agent and they are publishing heartbeat to NATS subject heartbeat.service_name.service You can get the list of services by sending following mqtt message # View services that are sending heartbeat mosquitto_pub -d -u $TH -P $KEY -t channels/$CH/messages/req -h some-domain-name -p 1883 -m '[{\"bn\":\"1:\", \"n\":\"service\", \"vs\":\"view\"}]' Response can be observed on channels/$CH/messages/res/# Proxying commands # You can send commands to services running on the same edge gateway as Agent if they are subscribed on same NATS server and correct subject. Service commands are being sent via MQTT to topic: channels/<control_channel_id>/messages/services/<service_name>/<subtopic> when messages is received Agent forwards them to NATS on subject: commands.<service_name>.<subtopic> Payload is up to the application and service itself. EdgeX # Edgex control messages are sent and received over control channel. MF sends a control SenML of the following form: [{\"bn\":\"<uuid>:\", \"n\":\"control\", \"vs\":\"<cmd>, <param>, edgexsvc1, edgexsvc2, \u2026, edgexsvcN\"}}] For example, [{\"bn\":\"1:\", \"n\":\"control\", \"vs\":\"operation, stop, edgex-support-notifications, edgex-core-data\"}] Agent, on the other hand, returns a response SenML of the following form: [{\"bn\":\"<uuid>:\", \"n\":\"<>\", \"v\":\"<RESP>\"}] Remote Commands # EdgeX defines SMA commands in the following RAML file Commands are: OPERATION CONFIG METRICS PING Operation mosquitto_pub -u <thing_id> -P <thing_key> -t channels/<channel_id>/messages/req -h localhost -m '[{\"bn\":\"1:\", \"n\":\"control\", \"vs\":\"edgex-operation, start, edgex-support-notifications, edgex-core-data\"}]' Config mosquitto_pub -u <thing_id> -P <thing_key> -t channels/<channel_id>/messages/req -h localhost -m '[{\"bn\":\"1:\", \"n\":\"control\", \"vs\":\"edgex-config, edgex-support-notifications, edgex-core-data\"}]' Metrics mosquitto_pub -u <thing_id> -P <thing_key> -t channels/<channel_id>/messages/req -h localhost -m '[{\"bn\":\"1:\", \"n\":\"control\", \"vs\":\"edgex-metrics, edgex-support-notifications, edgex-core-data\"}]' If you subscribe to mosquitto_sub -u <thing_id> -P <thing_key> -t channels/<channel_id>/messages/# You can observe commands and response from commands executed against edgex [{\"bn\":\"1:\", \"n\":\"control\", \"vs\":\"edgex-metrics, edgex-support-notifications, edgex-core-data\"}] [{\"bn\":\"1\",\"n\":\"edgex-metrics\",\"vs\":\"{\\\"Metrics\\\":{\\\"edgex-core-data\\\":{\\\"CpuBusyAvg\\\":15.568632467698606,\\\"Memory\\\":{\\\"Alloc\\\":2040136,\\\"Frees\\\":876344,\\\"LiveObjects\\\":15134,\\\"Mallocs\\\":891478,\\\"Sys\\\":73332984,\\\"TotalAlloc\\\":80657464}},\\\"edgex-support-notifications\\\":{\\\"CpuBusyAvg\\\":14.65381169745318,\\\"Memory\\\":{\\\"Alloc\\\":961784,\\\"Frees\\\":127430,\\\"LiveObjects\\\":6095,\\\"Mallocs\\\":133525,\\\"Sys\\\":72808696,\\\"TotalAlloc\\\":11665416}}}}\\n\"}] Export # Mainflux Export service can send message from one Mainflux cloud to another via MQTT, or it can send messages from edge gateway to Mainflux Cloud. Export service is subscribed to local message bus and connected to MQTT broker in the cloud. Messages collected on local message bus are redirected to the cloud. When connection is lost, if QoS2 is used, messages from the local bus are stored into file or in memory to be resent upon reconnection. Additonaly Export service publishes liveliness status to Agent via NATS subject heartbeat.export.service Install # Get the code: go get github.com/mainflux/export cd $GOPATH/github.com/mainflux/export Make: make Usage # cd build ./mainflux-export Configuration # By default Export service looks for config file at ../configs/config.toml if no env vars are specified. [exp] log_level = \"debug\" nats = \"localhost:4222\" port = \"8170\" [mqtt] username = \"<thing_id>\" password = \"<thing_password>\" ca_path = \"ca.crt\" client_cert = \"\" client_cert_key = \"\" client_cert_path = \"thing.crt\" client_priv_key_path = \"thing.key\" mtls = \"false\" priv_key = \"thing.key\" retain = \"false\" skip_tls_ver = \"false\" url = \"tcp://mainflux.com:1883\" [[routes]] mqtt_topic = \"channel/<channel_id>/messages\" subtopic = \"subtopic\" nats_topic = \"export\" type = \"default\" workers = 10 [[routes]] mqtt_topic = \"channel/<channel_id>/messages\" subtopic = \"subtopic\" nats_topic = \"channels\" type = \"mfx\" workers = 10 Environment variables # Service will first look for MF_EXPORT_CONFIG_FILE for configuration and if not found it will be configured with env variables and new config file specified with MF_EXPORT_CONFIG_FILE (default value will be used if none specified) will be saved with values populated from env vars. The service is configured using the environment variables as presented in the table. Note that any unset variables will be replaced with their default values. For values in environment variables to take effect make sure that there is no MF_EXPORT_CONFIG_FILE file. If you run with environment variables you can create config file: MF_EXPORT_PORT=8178 \\ MF_EXPORT_LOG_LEVEL=debug \\ MF_EXPORT_MQTT_HOST=tcp://localhost:1883 \\ MF_EXPORT_MQTT_USERNAME=<thing_id> \\ MF_EXPORT_MQTT_PASSWORD=<thing_key> \\ MF_EXPORT_MQTT_CHANNEL=<channel_id> \\ MF_EXPORT_MQTT_SKIP_TLS=true \\ MF_EXPORT_MQTT_MTLS=false \\ MF_EXPORT_MQTT_CA=ca.crt \\ MF_EXPORT_MQTT_CLIENT_CERT=thing.crt \\ MF_EXPORT_MQTT_CLIENT_PK=thing.key \\ MF_EXPORT_CONFIG_FILE=export.toml \\ ../build/mainflux-export& Values from environment variables will be used to populate export.toml Http port # port - HTTP port where status of Export service can be fetched. curl -X GET http://localhost:8170/health '{\"status\": \"pass\", \"version\":\"0.12.1\", \"commit\":\"57cca9677721025da055c47957fc3e869e0325aa\" , \"description\":\"export service\", \"build_time\": \"2022-01-19_10:13:17\"}' MQTT connection # To establish connection to MQTT broker following settings are needed: - username - Mainflux - password - Mainflux - url - url of MQTT broker Additionally, you will need MQTT client certificates if you enable mTLS. To obtain certificates ca.crt , thing.crt and key thing.key follow instructions here or here . MTLS # To setup MTLS connection Export service requires client certificate and mtls in config or MF_EXPORT_MQTT_MTLS must be set to true . Client certificate can be provided in a file, client_cert_path and client_cert_key_path are used for specifying path to certificate files. If MTLS is used and no certificate file paths are specified then Export will look in client_cert and client_cert_key of config file expecting certificate content stored as string. Routes # Routes are being used for specifying which subscriber's topic(subject) goes to which publishing topic. Currently only MQTT is supported for publishing. To match Mainflux requirements mqtt_topic must contain channel/<channel_id>/messages , additional subtopics can be appended. mqtt_topic - channel/<channel_id>/messages/<custom_subtopic> nats_topic - Export service will be subscribed to NATS subject <nats_topic>.> subtopic - messages will be published to MQTT topic <mqtt_topic>/<subtopic>/<nats_subject> , where dots in nats_subject are replaced with '/' workers - specifies number of workers that will be used for message forwarding. type - specifies message transformation: default is for sending messages as they are received on NATS with no transformation (so they should be in SenML or JSON format if we want to persist them in Mainflux in cloud). If you don't want to persist messages in Mainflux or you are not exporting to Mainflux cloud - message format can be anything that suits your application as message passes untransformed. mfx is for messages that are being picked up on internal Mainflux NATS bus. When using Export along with Mainflux deployed on gateway ( Fig. 1 ) messages coming from MQTT broker that are published to NATS bus are Mainflux message . Using mfx type will extract payload and export will publish it to mqtt_topic . Extracted payload is SenML or JSON if we want to persist messages. nats_topic in this case must be channels , or if you want to pick messages from a specific channel in local Mainflux instance to be exported to cloud you can put channels.<local_mainflux_channel_id> . Before running Export service edit configs/config.toml and provide username , password and url * username - matches thing_id in Mainflux cloud instance * password - matches thing_key * channel - MQTT part of the topic where to publish MQTT data ( channel/<channel_id>/messages is format of mainflux MQTT topic) and plays a part in authorization. If Mainflux and Export service are deployed on same gateway Export can be configured to send messages from Mainflux internal NATS bus to Mainflux in a cloud. In order for Export service to listen on Mainflux NATS deployed on the same machine NATS port must be exposed. Edit Mainflux docker-compose.yml . NATS section must look like below: nats: image: nats:1.3.0 container_name: mainflux-nats restart: on-failure networks: - mainflux-base-net ports: - 4222:4222 How to save config via agent # Configuration file for Export service can be sent over MQTT using Agent service. mosquitto_pub -u <thing_id> -P <thing_key> -t channels/<control_ch_id>/messages/req -h localhost -p 18831 -m \"[{\\\"bn\\\":\\\"1:\\\", \\\"n\\\":\\\"config\\\", \\\"vs\\\":\\\"save, export, <config_file_path>, <file_content_base64>\\\"}]\" vs=\"save, export, config_file_path, file_content_base64\" - vs determines where to save file and contains file content in base64 encoding payload: b,_ := toml.Marshal(export.Config) payload := base64.StdEncoding.EncodeToString(b) Using configure script # There is a configuration.sh script in a scripts directory that can be used for automatic configuration and start up of remotely deployed export . For this to work it is presumed that mainflux-export and scripts/export_start are placed in executable path on remote device. Additionally this script requires that remote device is provisioned following the steps described for provision service. To run it first edit script to set parameters MTLS=false EXTERNAL_KEY='raspberry' EXTERNAL_ID='pi' MAINFLUX_HOST='mainflux.com' MAINFLUX_USER_EMAIL='edge@email.com' MAINFLUX_USER_PASSWORD='12345678' EXTERNAL_KEY and EXTERNAL_ID are parameters posted to /mapping endpoint of provision service, MAINFLUX_HOST is location of cloud instance of Mainflux that export should connect to and MAINFLUX_USER_EMAIL and MAINFLUX_USER_PASSWORD are users credentials in the cloud. Example deployment # Edge deployment # The following are steps that are an example usage of Mainflux components to connect edge with cloud. We will start Mainflux in the cloud with additional services Bootstrap and Provision . Using Bootstrap and Provision we will create a configuration for use in gateway deployment. On the gateway we will start services Agent and Export using previously created configuration. Services in the cloud # Start the Mainflux: docker-compose -f docker/docker-compose.yml up Start the Bootstrap service: docker-compose -f docker/addons/bootstrap/docker-compose.yml up Start the Provision service docker-compose -f docker/addons/provision/docker-compose.yml up Create user: mainflux-cli -m http://localhost:8180 users create test@email.com 12345678 Obtain user token: mainflux-cli -m http://localhost:8180 users token test@email.com 12345678 created: eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJleHAiOjE1ODk5MDQ4MDQsImlhdCI6MTU4OTg2ODgwNCwiaXNzIjoibWFpbmZsdXguYXV0aG4iLCJzdWIiOiJ0ZXN0QGVtYWlsLmNvbSIsInR5cGUiOjB9.VSwpGoflOLqrHlCGoVVFPBdnnvsAhv2gc3EomXg9yM0 TOK=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJleHAiOjE1ODk5MDQ4MDQsImlhdCI6MTU4OTg2ODgwNCwiaXNzIjoibWFpbmZsdXguYXV0aG4iLCJzdWIiOiJ0ZXN0QGVtYWlsLmNvbSIsInR5cGUiOjB9.VSwpGoflOLqrHlCGoVVFPBdnnvsAhv2gc3EomXg9yM0 Provision a gateway: curl -s -S -X POST http://localhost:8190/mapping -H \"Authorization: Bearer $TOKEN\" -H 'Content-Type: application/json' -d '{\"name\":\"testing\", \"external_id\" : \"54:FG:66:DC:43\", \"external_key\":\"223334fw2\" }' | jq { \"things\": [ { \"id\": \"88529fb2-6c1e-4b60-b9ab-73b5d89f7404\", \"name\": \"thing\", \"key\": \"3529c1bb-7211-4d40-9cd8-b05833196093\", \"metadata\": { \"external_id\": \"54:FG:66:DC:43\" } } ], \"channels\": [ { \"id\": \"1aa3f736-0bd3-44b5-a917-a72cc743f633\", \"name\": \"control-channel\", \"metadata\": { \"type\": \"control\" } }, { \"id\": \"e2adcfa6-96b2-425d-8cd4-ff8cb9c056ce\", \"name\": \"data-channel\", \"metadata\": { \"type\": \"data\" } } ], \"whitelisted\": { \"88529fb2-6c1e-4b60-b9ab-73b5d89f7404\": true } } Parameters and are representing the gateway. Provision will use them to create a bootstrap configuration that will make a relation with Mainflux entities used for connection, authentication and authorization thing and channel . These parameters will be used by Agent service on the gateway to retrieve that information and establish a connection with the cloud. Services on the Edge # Agent # Start the [NATS][nats] and Agent service: gnatsd MF_AGENT_BOOTSTRAP_ID=54:FG:66:DC:43 \\ MF_AGENT_BOOTSTRAP_KEY=\"223334fw2\" \\ MF_AGENT_BOOTSTRAP_URL=http://localhost:8202/things/bootstrap \\ build/mainflux-agent {\"level\":\"info\",\"message\":\"Requesting config for 54:FG:66:DC:43 from http://localhost:8202/things/bootstrap\",\"ts\":\"2020-05-07T15:50:58.041145096Z\"} {\"level\":\"info\",\"message\":\"Getting config for 54:FG:66:DC:43 from http://localhost:8202/things/bootstrap succeeded\",\"ts\":\"2020-05-07T15:50:58.120779415Z\"} {\"level\":\"info\",\"message\":\"Saving export config file /configs/export/config.toml\",\"ts\":\"2020-05-07T15:50:58.121602229Z\"} {\"level\":\"warn\",\"message\":\"Failed to save export config file Error writing config file: open /configs/export/config.toml: no such file or directory\",\"ts\":\"2020-05-07T15:50:58.121752142Z\"} {\"level\":\"info\",\"message\":\"Client agent-88529fb2-6c1e-4b60-b9ab-73b5d89f7404 connected\",\"ts\":\"2020-05-07T15:50:58.128500603Z\"} {\"level\":\"info\",\"message\":\"Agent service started, exposed port 9003\",\"ts\":\"2020-05-07T15:50:58.128531057Z\"} Export # git clone https://github.com/mainflux/export make Edit the configs/config.toml setting - username - thing from the results of provision request. - password - key from the results of provision request. - mqtt_topic - in routes set to channels/<channel_data_id>/messages from results of provision. - nats_topic - whatever you need, export will subscribe to export.<nats_topic> and forward messages to MQTT. - host - url of MQTT broker. [exp] cache_pass = \"\" cache_url = \"\" log_level = \"debug\" nats = \"localhost:4222\" port = \"8170\" [mqtt] ca_path = \"\" cert_path = \"\" host = \"tcp://localhost:1883\" mtls = false password = \"3529c1bb-7211-4d40-9cd8-b05833196093\" priv_key_path = \"\" qos = 0 retain = false skip_tls_ver = false username = \"88529fb2-6c1e-4b60-b9ab-73b5d89f7404\" [[routes]] mqtt_topic = \"channels/e2adcfa6-96b2-425d-8cd4-ff8cb9c056ce/messages\" nats_topic = \">\" workers = 10 cd build ./mainflux-export 2020/05/07 17:36:57 Configuration loaded from file ../configs/config.toml {\"level\":\"info\",\"message\":\"Export service started, exposed port :8170\",\"ts\":\"2020-05-07T15:36:57.528398548Z\"} {\"level\":\"debug\",\"message\":\"Client export-88529fb2-6c1e-4b60-b9ab-73b5d89f7404 connected\",\"ts\":\"2020-05-07T15:36:57.528405818Z\"} Testing Export # git clone https://github.com/nats-io/nats.go cd github.com/nats-io/nats.go/examples/nats-pub go run main.go -s http://localhost:4222 export.test \"[{\\\"bn\\\":\\\"test\\\"}]\"; We have configured route for export, nats_topic = \">\" means that it will listen to NATS subject export.> and mqtt_topic is configured so that data will be sent to MQTT broker on topic channels/e2adcfa6-96b2-425d-8cd4-ff8cb9c056ce/messages with appended NATS subject. In terminal where export is started you should see following message: {\"level\":\"debug\",\"message\":\"Published to: export.test, payload: [{\\\"bn\\\":\\\"test\\\"}]\",\"ts\":\"2020-05-08T15:14:15.757298992Z\"} In Mainflux mqtt service: mainflux-mqtt | {\"level\":\"info\",\"message\":\"Publish - client ID export-88529fb2-6c1e-4b60-b9ab-73b5d89f7404 to the topic: channels/e2adcfa6-96b2-425d-8cd4-ff8cb9c056ce/messages/export/test\",\"ts\":\"2020-05-08T15:16:02.999684791Z\"}","title":"Edge"},{"location":"edge/#edge","text":"Mainflux IoT platform provides services for supporting management of devices on the edge. Typically, IoT solution includes devices (sensors/actuators) deployed in far edge and connected through some proxy gateway. Although most devices could be connected to the Mainflux directly, using gateways decentralizes system, decreases load on the cloud and makes setup less difficult. Also, gateways can provide additional data processing, filtering and storage. Services that can be used on gateway to enable data and control plane for edge: Agent Export Mainflux Figure 1 - Edge services deployment Figure shows edge gateway that is running Agent, Export and minimal deployment of Mainflux services. Mainflux services enable device management and MQTT protocol, NATS being a central message bus in Mainflux becomes also central message bus for other services like Agent and Export as well as for any new custom developed service that can be built to interface with devices with any of hardware supported interfaces on the gateway, those services would publish data to NATS where Export service can pick them up and send to cloud. Agent can be used to control deployed services as well as to monitor their liveliness through subcribing to heartbeat NATS subject where services should publish their liveliness status, like Export service does.","title":"Edge"},{"location":"edge/#agent","text":"Agent is service that is used to manage gateways that are connected to Mainflux in cloud. It provides a way to send commands to gateway and receive response via mqtt. There are two types of channels used for Agent data and control . Over the control we are sending commands and receiving response from commands. Data collected from sensors connected to gateway are being sent over data channel. Agent is able to configure itself provided that bootstrap server is running, it will retrieve configuration from bootstrap server provided few arguments - external_id and external_key see bootstraping . Agent service has following features: * Remote execution of commands * Remote terminal, remote session to bash managed by Agent * Heartbeat - listening to NATS topic heartbeat.> it can remotely provide info on running services, if services are publishing heartbeat ( like Export ) * Proxying commands to other gateway services * Edgex SMA - remotely making requests to EdgeX endpoints and fetching results, if EdgeX is deployed.","title":"Agent"},{"location":"edge/#run-agent","text":"Before running agent we need to provision a thing and DATA and CONTROL channel. Thing that will be used as gateway representation and make bootstrap configuration. If using Mainflux UI this is done automatically when adding gateway through UI. Gateway can be provisioned with provision service. When you provisioned gateway as described in provision you can check results curl -s -S -X GET http://mainflux-domain.com:8202/things/bootstrap/<external_id> -H \"Authorization: Thing <external_key>\" -H 'Content-Type: application/json' |jq { \"mainflux_id\": \"e22c383a-d2ab-47c1-89cd-903955da993d\", \"mainflux_key\": \"fc987711-1828-461b-aa4b-16d5b2c642fe\", \"mainflux_channels\": [ { \"id\": \"fa5f9ba8-a1fc-4380-9edb-d0c23eaa24ec\", \"name\": \"control-channel\", \"metadata\": { \"type\": \"control\" } }, { \"id\": \"24e5473e-3cbe-43d9-8a8b-a725ff918c0e\", \"name\": \"data-channel\", \"metadata\": { \"type\": \"data\" } }, { \"id\": \"1eac45c2-0f72-4089-b255-ebd2e5732bbb\", \"name\": \"export-channel\", \"metadata\": { \"type\": \"export\" } } ], \"content\": \"{\\\"agent\\\":{\\\"edgex\\\":{\\\"url\\\":\\\"http://localhost:48090/api/v1/\\\"},\\\"heartbeat\\\":{\\\"interval\\\":\\\"30s\\\"},\\\"log\\\":{\\\"level\\\":\\\"debug\\\"},\\\"mqtt\\\":{\\\"mtls\\\":false,\\\"qos\\\":0,\\\"retain\\\":false,\\\"skip_tls_ver\\\":true,\\\"url\\\":\\\"tcp://mainflux-domain.com:1883\\\"},\\\"server\\\":{\\\"nats_url\\\":\\\"localhost:4222\\\",\\\"port\\\":\\\"9000\\\"},\\\"terminal\\\":{\\\"session_timeout\\\":\\\"30s\\\"}},\\\"export\\\":{\\\"exp\\\":{\\\"cache_db\\\":\\\"0\\\",\\\"cache_pass\\\":\\\"\\\",\\\"cache_url\\\":\\\"localhost:6379\\\",\\\"log_level\\\":\\\"debug\\\",\\\"nats\\\":\\\"nats://localhost:4222\\\",\\\"port\\\":\\\"8172\\\"},\\\"mqtt\\\":{\\\"ca_path\\\":\\\"ca.crt\\\",\\\"cert_path\\\":\\\"thing.crt\\\",\\\"channel\\\":\\\"\\\",\\\"host\\\":\\\"tcp://mainflux-domain.com:1883\\\",\\\"mtls\\\":false,\\\"password\\\":\\\"\\\",\\\"priv_key_path\\\":\\\"thing.key\\\",\\\"qos\\\":0,\\\"retain\\\":false,\\\"skip_tls_ver\\\":false,\\\"username\\\":\\\"\\\"},\\\"routes\\\":[{\\\"mqtt_topic\\\":\\\"\\\",\\\"nats_topic\\\":\\\"channels\\\",\\\"subtopic\\\":\\\"\\\",\\\"type\\\":\\\"mfx\\\",\\\"workers\\\":10},{\\\"mqtt_topic\\\":\\\"\\\",\\\"nats_topic\\\":\\\"export\\\",\\\"subtopic\\\":\\\"\\\",\\\"type\\\":\\\"default\\\",\\\"workers\\\":10}]}}\" } external_id is usually MAC address, but anything that suits applications requirements can be used external_key is key that will be provided to agent process thing_id is mainflux thing id channels is 2-element array where first channel is CONTROL and second is DATA, both channels should be assigned to thing content is used for configuring parameters of agent and export service. Then to start the agent service you can do it like this git clone https://github.com/mainflux/agent make cd build MF_AGENT_LOG_LEVEL=debug \\ MF_AGENT_BOOTSTRAP_KEY=edged \\ MF_AGENT_BOOTSTRAP_ID=34:e1:2d:e6:cf:03 ./mainflux-agent {\"level\":\"info\",\"message\":\"Requesting config for 34:e1:2d:e6:cf:03 from http://localhost:8202/things/bootstrap\",\"ts\":\"2019-12-05T04:47:24.98411512Z\"} {\"level\":\"info\",\"message\":\"Getting config for 34:e1:2d:e6:cf:03 from http://localhost:8202/things/bootstrap succeeded\",\"ts\":\"2019-12-05T04:47:24.995465239Z\"} {\"level\":\"info\",\"message\":\"Connected to MQTT broker\",\"ts\":\"2019-12-05T04:47:25.009645082Z\"} {\"level\":\"info\",\"message\":\"Agent service started, exposed port 9000\",\"ts\":\"2019-12-05T04:47:25.009755345Z\"} {\"level\":\"info\",\"message\":\"Subscribed to MQTT broker\",\"ts\":\"2019-12-05T04:47:25.012930443Z\"} MF_AGENT_BOOTSTRAP_KEY - is external_key in bootstrap configuration. MF_AGENT_BOOSTRAP_ID - is external_id in bootstrap configuration.","title":"Run Agent"},{"location":"edge/#remote-execution-of-commands-via-agent","text":"# Set connection parameters as environment variables in shell CH=`curl -s -S -X GET http://some-domain-name:8202/things/bootstrap/34:e1:2d:e6:cf:03 -H \"Authorization: edged\" -H 'Content-Type: application/json' | jq -r '.mainflux_channels[0].id'` TH=`curl -s -S -X GET http://some-domain-name:8202/things/bootstrap/34:e1:2d:e6:cf:03 -H \"Authorization: edged\" -H 'Content-Type: application/json' | jq -r .mainflux_id` KEY=`curl -s -S -X GET http://some-domain-name:8202/things/bootstrap/34:e1:2d:e6:cf:03 -H \"Authorization: edged\" -H 'Content-Type: application/json' | jq -r .mainflux_key` # Subscribe for response mosquitto_sub -d -u $TH -P $KEY -t channels/$CH/messages/res/# -h some-domain-name -p 1883 # Publish command e.g `ls` mosquitto_pub -d -u $TH -P $KEY -t channels/$CH/messages/req -h some-domain-name -p 1883 -m '[{\"bn\":\"1:\", \"n\":\"exec\", \"vs\":\"ls, -l\"}]'","title":"Remote execution of commands via Agent"},{"location":"edge/#remote-terminal","text":"This can be checked from the UI, click on the details for gateway and below the gateway parameters you will se box with prompt, if agent is running and it is properly connected you should be able to execute commands remotely.","title":"Remote terminal"},{"location":"edge/#heartbeat","text":"If there are services that are running on same gateway as agent and they are publishing heartbeat to NATS subject heartbeat.service_name.service You can get the list of services by sending following mqtt message # View services that are sending heartbeat mosquitto_pub -d -u $TH -P $KEY -t channels/$CH/messages/req -h some-domain-name -p 1883 -m '[{\"bn\":\"1:\", \"n\":\"service\", \"vs\":\"view\"}]' Response can be observed on channels/$CH/messages/res/#","title":"Heartbeat"},{"location":"edge/#proxying-commands","text":"You can send commands to services running on the same edge gateway as Agent if they are subscribed on same NATS server and correct subject. Service commands are being sent via MQTT to topic: channels/<control_channel_id>/messages/services/<service_name>/<subtopic> when messages is received Agent forwards them to NATS on subject: commands.<service_name>.<subtopic> Payload is up to the application and service itself.","title":"Proxying commands"},{"location":"edge/#edgex","text":"Edgex control messages are sent and received over control channel. MF sends a control SenML of the following form: [{\"bn\":\"<uuid>:\", \"n\":\"control\", \"vs\":\"<cmd>, <param>, edgexsvc1, edgexsvc2, \u2026, edgexsvcN\"}}] For example, [{\"bn\":\"1:\", \"n\":\"control\", \"vs\":\"operation, stop, edgex-support-notifications, edgex-core-data\"}] Agent, on the other hand, returns a response SenML of the following form: [{\"bn\":\"<uuid>:\", \"n\":\"<>\", \"v\":\"<RESP>\"}]","title":"EdgeX"},{"location":"edge/#remote-commands","text":"EdgeX defines SMA commands in the following RAML file Commands are: OPERATION CONFIG METRICS PING Operation mosquitto_pub -u <thing_id> -P <thing_key> -t channels/<channel_id>/messages/req -h localhost -m '[{\"bn\":\"1:\", \"n\":\"control\", \"vs\":\"edgex-operation, start, edgex-support-notifications, edgex-core-data\"}]' Config mosquitto_pub -u <thing_id> -P <thing_key> -t channels/<channel_id>/messages/req -h localhost -m '[{\"bn\":\"1:\", \"n\":\"control\", \"vs\":\"edgex-config, edgex-support-notifications, edgex-core-data\"}]' Metrics mosquitto_pub -u <thing_id> -P <thing_key> -t channels/<channel_id>/messages/req -h localhost -m '[{\"bn\":\"1:\", \"n\":\"control\", \"vs\":\"edgex-metrics, edgex-support-notifications, edgex-core-data\"}]' If you subscribe to mosquitto_sub -u <thing_id> -P <thing_key> -t channels/<channel_id>/messages/# You can observe commands and response from commands executed against edgex [{\"bn\":\"1:\", \"n\":\"control\", \"vs\":\"edgex-metrics, edgex-support-notifications, edgex-core-data\"}] [{\"bn\":\"1\",\"n\":\"edgex-metrics\",\"vs\":\"{\\\"Metrics\\\":{\\\"edgex-core-data\\\":{\\\"CpuBusyAvg\\\":15.568632467698606,\\\"Memory\\\":{\\\"Alloc\\\":2040136,\\\"Frees\\\":876344,\\\"LiveObjects\\\":15134,\\\"Mallocs\\\":891478,\\\"Sys\\\":73332984,\\\"TotalAlloc\\\":80657464}},\\\"edgex-support-notifications\\\":{\\\"CpuBusyAvg\\\":14.65381169745318,\\\"Memory\\\":{\\\"Alloc\\\":961784,\\\"Frees\\\":127430,\\\"LiveObjects\\\":6095,\\\"Mallocs\\\":133525,\\\"Sys\\\":72808696,\\\"TotalAlloc\\\":11665416}}}}\\n\"}]","title":"Remote Commands"},{"location":"edge/#export","text":"Mainflux Export service can send message from one Mainflux cloud to another via MQTT, or it can send messages from edge gateway to Mainflux Cloud. Export service is subscribed to local message bus and connected to MQTT broker in the cloud. Messages collected on local message bus are redirected to the cloud. When connection is lost, if QoS2 is used, messages from the local bus are stored into file or in memory to be resent upon reconnection. Additonaly Export service publishes liveliness status to Agent via NATS subject heartbeat.export.service","title":"Export"},{"location":"edge/#install","text":"Get the code: go get github.com/mainflux/export cd $GOPATH/github.com/mainflux/export Make: make","title":"Install"},{"location":"edge/#usage","text":"cd build ./mainflux-export","title":"Usage"},{"location":"edge/#configuration","text":"By default Export service looks for config file at ../configs/config.toml if no env vars are specified. [exp] log_level = \"debug\" nats = \"localhost:4222\" port = \"8170\" [mqtt] username = \"<thing_id>\" password = \"<thing_password>\" ca_path = \"ca.crt\" client_cert = \"\" client_cert_key = \"\" client_cert_path = \"thing.crt\" client_priv_key_path = \"thing.key\" mtls = \"false\" priv_key = \"thing.key\" retain = \"false\" skip_tls_ver = \"false\" url = \"tcp://mainflux.com:1883\" [[routes]] mqtt_topic = \"channel/<channel_id>/messages\" subtopic = \"subtopic\" nats_topic = \"export\" type = \"default\" workers = 10 [[routes]] mqtt_topic = \"channel/<channel_id>/messages\" subtopic = \"subtopic\" nats_topic = \"channels\" type = \"mfx\" workers = 10","title":"Configuration"},{"location":"edge/#environment-variables","text":"Service will first look for MF_EXPORT_CONFIG_FILE for configuration and if not found it will be configured with env variables and new config file specified with MF_EXPORT_CONFIG_FILE (default value will be used if none specified) will be saved with values populated from env vars. The service is configured using the environment variables as presented in the table. Note that any unset variables will be replaced with their default values. For values in environment variables to take effect make sure that there is no MF_EXPORT_CONFIG_FILE file. If you run with environment variables you can create config file: MF_EXPORT_PORT=8178 \\ MF_EXPORT_LOG_LEVEL=debug \\ MF_EXPORT_MQTT_HOST=tcp://localhost:1883 \\ MF_EXPORT_MQTT_USERNAME=<thing_id> \\ MF_EXPORT_MQTT_PASSWORD=<thing_key> \\ MF_EXPORT_MQTT_CHANNEL=<channel_id> \\ MF_EXPORT_MQTT_SKIP_TLS=true \\ MF_EXPORT_MQTT_MTLS=false \\ MF_EXPORT_MQTT_CA=ca.crt \\ MF_EXPORT_MQTT_CLIENT_CERT=thing.crt \\ MF_EXPORT_MQTT_CLIENT_PK=thing.key \\ MF_EXPORT_CONFIG_FILE=export.toml \\ ../build/mainflux-export& Values from environment variables will be used to populate export.toml","title":"Environment variables"},{"location":"edge/#http-port","text":"port - HTTP port where status of Export service can be fetched. curl -X GET http://localhost:8170/health '{\"status\": \"pass\", \"version\":\"0.12.1\", \"commit\":\"57cca9677721025da055c47957fc3e869e0325aa\" , \"description\":\"export service\", \"build_time\": \"2022-01-19_10:13:17\"}'","title":"Http port"},{"location":"edge/#mqtt-connection","text":"To establish connection to MQTT broker following settings are needed: - username - Mainflux - password - Mainflux - url - url of MQTT broker Additionally, you will need MQTT client certificates if you enable mTLS. To obtain certificates ca.crt , thing.crt and key thing.key follow instructions here or here .","title":"MQTT connection"},{"location":"edge/#mtls","text":"To setup MTLS connection Export service requires client certificate and mtls in config or MF_EXPORT_MQTT_MTLS must be set to true . Client certificate can be provided in a file, client_cert_path and client_cert_key_path are used for specifying path to certificate files. If MTLS is used and no certificate file paths are specified then Export will look in client_cert and client_cert_key of config file expecting certificate content stored as string.","title":"MTLS"},{"location":"edge/#routes","text":"Routes are being used for specifying which subscriber's topic(subject) goes to which publishing topic. Currently only MQTT is supported for publishing. To match Mainflux requirements mqtt_topic must contain channel/<channel_id>/messages , additional subtopics can be appended. mqtt_topic - channel/<channel_id>/messages/<custom_subtopic> nats_topic - Export service will be subscribed to NATS subject <nats_topic>.> subtopic - messages will be published to MQTT topic <mqtt_topic>/<subtopic>/<nats_subject> , where dots in nats_subject are replaced with '/' workers - specifies number of workers that will be used for message forwarding. type - specifies message transformation: default is for sending messages as they are received on NATS with no transformation (so they should be in SenML or JSON format if we want to persist them in Mainflux in cloud). If you don't want to persist messages in Mainflux or you are not exporting to Mainflux cloud - message format can be anything that suits your application as message passes untransformed. mfx is for messages that are being picked up on internal Mainflux NATS bus. When using Export along with Mainflux deployed on gateway ( Fig. 1 ) messages coming from MQTT broker that are published to NATS bus are Mainflux message . Using mfx type will extract payload and export will publish it to mqtt_topic . Extracted payload is SenML or JSON if we want to persist messages. nats_topic in this case must be channels , or if you want to pick messages from a specific channel in local Mainflux instance to be exported to cloud you can put channels.<local_mainflux_channel_id> . Before running Export service edit configs/config.toml and provide username , password and url * username - matches thing_id in Mainflux cloud instance * password - matches thing_key * channel - MQTT part of the topic where to publish MQTT data ( channel/<channel_id>/messages is format of mainflux MQTT topic) and plays a part in authorization. If Mainflux and Export service are deployed on same gateway Export can be configured to send messages from Mainflux internal NATS bus to Mainflux in a cloud. In order for Export service to listen on Mainflux NATS deployed on the same machine NATS port must be exposed. Edit Mainflux docker-compose.yml . NATS section must look like below: nats: image: nats:1.3.0 container_name: mainflux-nats restart: on-failure networks: - mainflux-base-net ports: - 4222:4222","title":"Routes"},{"location":"edge/#how-to-save-config-via-agent","text":"Configuration file for Export service can be sent over MQTT using Agent service. mosquitto_pub -u <thing_id> -P <thing_key> -t channels/<control_ch_id>/messages/req -h localhost -p 18831 -m \"[{\\\"bn\\\":\\\"1:\\\", \\\"n\\\":\\\"config\\\", \\\"vs\\\":\\\"save, export, <config_file_path>, <file_content_base64>\\\"}]\" vs=\"save, export, config_file_path, file_content_base64\" - vs determines where to save file and contains file content in base64 encoding payload: b,_ := toml.Marshal(export.Config) payload := base64.StdEncoding.EncodeToString(b)","title":"How to save config via agent"},{"location":"edge/#using-configure-script","text":"There is a configuration.sh script in a scripts directory that can be used for automatic configuration and start up of remotely deployed export . For this to work it is presumed that mainflux-export and scripts/export_start are placed in executable path on remote device. Additionally this script requires that remote device is provisioned following the steps described for provision service. To run it first edit script to set parameters MTLS=false EXTERNAL_KEY='raspberry' EXTERNAL_ID='pi' MAINFLUX_HOST='mainflux.com' MAINFLUX_USER_EMAIL='edge@email.com' MAINFLUX_USER_PASSWORD='12345678' EXTERNAL_KEY and EXTERNAL_ID are parameters posted to /mapping endpoint of provision service, MAINFLUX_HOST is location of cloud instance of Mainflux that export should connect to and MAINFLUX_USER_EMAIL and MAINFLUX_USER_PASSWORD are users credentials in the cloud.","title":"Using configure script"},{"location":"edge/#example-deployment","text":"","title":"Example deployment"},{"location":"edge/#edge-deployment","text":"The following are steps that are an example usage of Mainflux components to connect edge with cloud. We will start Mainflux in the cloud with additional services Bootstrap and Provision . Using Bootstrap and Provision we will create a configuration for use in gateway deployment. On the gateway we will start services Agent and Export using previously created configuration.","title":"Edge deployment"},{"location":"edge/#services-in-the-cloud","text":"Start the Mainflux: docker-compose -f docker/docker-compose.yml up Start the Bootstrap service: docker-compose -f docker/addons/bootstrap/docker-compose.yml up Start the Provision service docker-compose -f docker/addons/provision/docker-compose.yml up Create user: mainflux-cli -m http://localhost:8180 users create test@email.com 12345678 Obtain user token: mainflux-cli -m http://localhost:8180 users token test@email.com 12345678 created: eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJleHAiOjE1ODk5MDQ4MDQsImlhdCI6MTU4OTg2ODgwNCwiaXNzIjoibWFpbmZsdXguYXV0aG4iLCJzdWIiOiJ0ZXN0QGVtYWlsLmNvbSIsInR5cGUiOjB9.VSwpGoflOLqrHlCGoVVFPBdnnvsAhv2gc3EomXg9yM0 TOK=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJleHAiOjE1ODk5MDQ4MDQsImlhdCI6MTU4OTg2ODgwNCwiaXNzIjoibWFpbmZsdXguYXV0aG4iLCJzdWIiOiJ0ZXN0QGVtYWlsLmNvbSIsInR5cGUiOjB9.VSwpGoflOLqrHlCGoVVFPBdnnvsAhv2gc3EomXg9yM0 Provision a gateway: curl -s -S -X POST http://localhost:8190/mapping -H \"Authorization: Bearer $TOKEN\" -H 'Content-Type: application/json' -d '{\"name\":\"testing\", \"external_id\" : \"54:FG:66:DC:43\", \"external_key\":\"223334fw2\" }' | jq { \"things\": [ { \"id\": \"88529fb2-6c1e-4b60-b9ab-73b5d89f7404\", \"name\": \"thing\", \"key\": \"3529c1bb-7211-4d40-9cd8-b05833196093\", \"metadata\": { \"external_id\": \"54:FG:66:DC:43\" } } ], \"channels\": [ { \"id\": \"1aa3f736-0bd3-44b5-a917-a72cc743f633\", \"name\": \"control-channel\", \"metadata\": { \"type\": \"control\" } }, { \"id\": \"e2adcfa6-96b2-425d-8cd4-ff8cb9c056ce\", \"name\": \"data-channel\", \"metadata\": { \"type\": \"data\" } } ], \"whitelisted\": { \"88529fb2-6c1e-4b60-b9ab-73b5d89f7404\": true } } Parameters and are representing the gateway. Provision will use them to create a bootstrap configuration that will make a relation with Mainflux entities used for connection, authentication and authorization thing and channel . These parameters will be used by Agent service on the gateway to retrieve that information and establish a connection with the cloud.","title":"Services in the cloud"},{"location":"edge/#services-on-the-edge","text":"","title":"Services on the Edge"},{"location":"edge/#agent_1","text":"Start the [NATS][nats] and Agent service: gnatsd MF_AGENT_BOOTSTRAP_ID=54:FG:66:DC:43 \\ MF_AGENT_BOOTSTRAP_KEY=\"223334fw2\" \\ MF_AGENT_BOOTSTRAP_URL=http://localhost:8202/things/bootstrap \\ build/mainflux-agent {\"level\":\"info\",\"message\":\"Requesting config for 54:FG:66:DC:43 from http://localhost:8202/things/bootstrap\",\"ts\":\"2020-05-07T15:50:58.041145096Z\"} {\"level\":\"info\",\"message\":\"Getting config for 54:FG:66:DC:43 from http://localhost:8202/things/bootstrap succeeded\",\"ts\":\"2020-05-07T15:50:58.120779415Z\"} {\"level\":\"info\",\"message\":\"Saving export config file /configs/export/config.toml\",\"ts\":\"2020-05-07T15:50:58.121602229Z\"} {\"level\":\"warn\",\"message\":\"Failed to save export config file Error writing config file: open /configs/export/config.toml: no such file or directory\",\"ts\":\"2020-05-07T15:50:58.121752142Z\"} {\"level\":\"info\",\"message\":\"Client agent-88529fb2-6c1e-4b60-b9ab-73b5d89f7404 connected\",\"ts\":\"2020-05-07T15:50:58.128500603Z\"} {\"level\":\"info\",\"message\":\"Agent service started, exposed port 9003\",\"ts\":\"2020-05-07T15:50:58.128531057Z\"}","title":"Agent"},{"location":"edge/#export_1","text":"git clone https://github.com/mainflux/export make Edit the configs/config.toml setting - username - thing from the results of provision request. - password - key from the results of provision request. - mqtt_topic - in routes set to channels/<channel_data_id>/messages from results of provision. - nats_topic - whatever you need, export will subscribe to export.<nats_topic> and forward messages to MQTT. - host - url of MQTT broker. [exp] cache_pass = \"\" cache_url = \"\" log_level = \"debug\" nats = \"localhost:4222\" port = \"8170\" [mqtt] ca_path = \"\" cert_path = \"\" host = \"tcp://localhost:1883\" mtls = false password = \"3529c1bb-7211-4d40-9cd8-b05833196093\" priv_key_path = \"\" qos = 0 retain = false skip_tls_ver = false username = \"88529fb2-6c1e-4b60-b9ab-73b5d89f7404\" [[routes]] mqtt_topic = \"channels/e2adcfa6-96b2-425d-8cd4-ff8cb9c056ce/messages\" nats_topic = \">\" workers = 10 cd build ./mainflux-export 2020/05/07 17:36:57 Configuration loaded from file ../configs/config.toml {\"level\":\"info\",\"message\":\"Export service started, exposed port :8170\",\"ts\":\"2020-05-07T15:36:57.528398548Z\"} {\"level\":\"debug\",\"message\":\"Client export-88529fb2-6c1e-4b60-b9ab-73b5d89f7404 connected\",\"ts\":\"2020-05-07T15:36:57.528405818Z\"}","title":"Export"},{"location":"edge/#testing-export","text":"git clone https://github.com/nats-io/nats.go cd github.com/nats-io/nats.go/examples/nats-pub go run main.go -s http://localhost:4222 export.test \"[{\\\"bn\\\":\\\"test\\\"}]\"; We have configured route for export, nats_topic = \">\" means that it will listen to NATS subject export.> and mqtt_topic is configured so that data will be sent to MQTT broker on topic channels/e2adcfa6-96b2-425d-8cd4-ff8cb9c056ce/messages with appended NATS subject. In terminal where export is started you should see following message: {\"level\":\"debug\",\"message\":\"Published to: export.test, payload: [{\\\"bn\\\":\\\"test\\\"}]\",\"ts\":\"2020-05-08T15:14:15.757298992Z\"} In Mainflux mqtt service: mainflux-mqtt | {\"level\":\"info\",\"message\":\"Publish - client ID export-88529fb2-6c1e-4b60-b9ab-73b5d89f7404 to the topic: channels/e2adcfa6-96b2-425d-8cd4-ff8cb9c056ce/messages/export/test\",\"ts\":\"2020-05-08T15:16:02.999684791Z\"}","title":"Testing Export"},{"location":"getting-started/","text":"Getting Started # Step 1 - Run the System # Before proceeding, install the following prerequisites: Docker (version 18.09) Docker compose (version 1.24.1) Once everything is installed, execute the following command from project root: make run This will start Mainflux docker composition, which will output the logs from the containers. Step 2 - Install the CLI # Open a new terminal from which you can interact with the running Mainflux system. The easiest way to do this is by using the Mainflux CLI, which can be downloaded as a tarball from GitHub (here we use release 0.12.1 but be sure to use the latest CLI release ): wget -O- https://github.com/mainflux/mainflux/releases/download/0.12.1/mainflux-cli_0.12.1_linux-amd64.tar.gz | tar xvz -C $GOBIN Make sure that $GOBIN is added to your $PATH so that mainflux-cli command can be accessible system-wide Build mainflux-cli # Build mainflux-cli if the pre-built CLI is not compatible with your OS, i.e MacOS. Please see the CLI for further details. Step 3 - Provision the System # Once installed, you can use the CLI to quick-provision the system for testing: mainflux-cli provision test This command actually creates a temporary testing user, logs it in, then creates two things and two channels on behalf of this user. This quickly provisions a Mainflux system with one simple testing scenario. You can read more about system provisioning in the dedicated Provisioning chapter Output of the command follows this pattern: { \"email\": \"friendly_beaver@email.com\", \"password\": \"12345678\" } \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJleHAiOjE1NDcwMjE3ODAsImlhdCI6MTU0Njk4NTc4MCwiaXNzIjoibWFpbmZsdXgiLCJzdWIiOiJmcmllbmRseV9iZWF2ZXJAZW1haWwuY29tIn0.Tyk31Ae680KqMrDqP895PRZg_GUytLE0IMIR_o3oO7o\" [ { \"id\": \"513d02d2-16c1-4f23-98be-9e12f8fee898\", \"key\": \"69590b3a-9d76-4baa-adae-9b5fec0ea14f\", \"name\": \"d0\", }, { \"id\": \"bf78ca98-2fef-4cfc-9f26-e02da5ecdf67\", \"key\": \"840c1ea1-2e8d-4809-a6d3-3433a5c489d2\", \"name\": \"d1\", } ] [ { \"id\": \"b7bfc4b6-c18d-47c5-b343-98235c5acc19\", \"name\": \"c0\" }, { \"id\": \"378678cd-891b-4a39-b026-869938783f54\", \"name\": \"c1\" } ] In the Mainflux system terminal (where docker compose is running) you should see following logs: mainflux-users | {\"level\":\"info\",\"message\":\"Method register for user friendly_beaver@email.com took 97.573974ms to complete without errors.\",\"ts\":\"2019-01-08T22:16:20.745989495Z\"} mainflux-users | {\"level\":\"info\",\"message\":\"Method login for user friendly_beaver@email.com took 69.308406ms to complete without errors.\",\"ts\":\"2019-01-08T22:16:20.820610461Z\"} mainflux-users | {\"level\":\"info\",\"message\":\"Method identity for client friendly_beaver@email.com took 50.903\u00b5s to complete without errors.\",\"ts\":\"2019-01-08T22:16:20.822208948Z\"} mainflux-things | {\"level\":\"info\",\"message\":\"Method add_thing for token eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJleHAiOjE1NDcwMjE3ODAsImlhdCI6MTU0Njk4NTc4MCwiaXNzIjoibWFpbmZsdXgiLCJzdWIiOiJmcmllbmRseV9iZWF2ZXJAZW1haWwuY29tIn0.Tyk31Ae680KqMrDqP895PRZg_GUytLE0IMIR_o3oO7o and thing 513d02d2-16c1-4f23-98be-9e12f8fee898 took 4.865299ms to complete without errors.\",\"ts\":\"2019-01-08T22:16:20.826786175Z\"} ... This proves that these provisioning commands were sent from the CLI to the Mainflux system. Step 4 - Send Messages # Once system is provisioned, a thing can start sending messages on a channel : mainflux-cli messages send <channel_id> '[{\"bn\":\"some-base-name:\",\"bt\":1.276020076001e+09, \"bu\":\"A\",\"bver\":5, \"n\":\"voltage\",\"u\":\"V\",\"v\":120.1}, {\"n\":\"current\",\"t\":-5,\"v\":1.2}, {\"n\":\"current\",\"t\":-4,\"v\":1.3}]' <thing_key> For example: mainflux-cli messages send b7bfc4b6-c18d-47c5-b343-98235c5acc19 '[{\"bn\":\"some-base-name:\",\"bt\":1.276020076001e+09, \"bu\":\"A\",\"bver\":5, \"n\":\"voltage\",\"u\":\"V\",\"v\":120.1}, {\"n\":\"current\",\"t\":-5,\"v\":1.2}, {\"n\":\"current\",\"t\":-4,\"v\":1.3}]' 69590b3a-9d76-4baa-adae-9b5fec0ea14f In the Mainflux system terminal you should see following logs: mainflux-things | {\"level\":\"info\",\"message\":\"Method can_access for channel b7bfc4b6-c18d-47c5-b343-98235c5acc19 and thing 513d02d2-16c1-4f23-98be-9e12f8fee898 took 1.410194ms to complete without errors.\",\"ts\":\"2019-01-08T22:19:30.148097648Z\"} mainflux-http | {\"level\":\"info\",\"message\":\"Method publish took 336.685\u00b5s to complete without errors.\",\"ts\":\"2019-01-08T22:19:30.148689601Z\"} This proves that messages have been correctly sent through the system via the protocol adapter ( mainflux-http ).","title":"Getting Started"},{"location":"getting-started/#getting-started","text":"","title":"Getting Started"},{"location":"getting-started/#step-1-run-the-system","text":"Before proceeding, install the following prerequisites: Docker (version 18.09) Docker compose (version 1.24.1) Once everything is installed, execute the following command from project root: make run This will start Mainflux docker composition, which will output the logs from the containers.","title":"Step 1 - Run the System"},{"location":"getting-started/#step-2-install-the-cli","text":"Open a new terminal from which you can interact with the running Mainflux system. The easiest way to do this is by using the Mainflux CLI, which can be downloaded as a tarball from GitHub (here we use release 0.12.1 but be sure to use the latest CLI release ): wget -O- https://github.com/mainflux/mainflux/releases/download/0.12.1/mainflux-cli_0.12.1_linux-amd64.tar.gz | tar xvz -C $GOBIN Make sure that $GOBIN is added to your $PATH so that mainflux-cli command can be accessible system-wide","title":"Step 2 - Install the CLI"},{"location":"getting-started/#build-mainflux-cli","text":"Build mainflux-cli if the pre-built CLI is not compatible with your OS, i.e MacOS. Please see the CLI for further details.","title":"Build mainflux-cli"},{"location":"getting-started/#step-3-provision-the-system","text":"Once installed, you can use the CLI to quick-provision the system for testing: mainflux-cli provision test This command actually creates a temporary testing user, logs it in, then creates two things and two channels on behalf of this user. This quickly provisions a Mainflux system with one simple testing scenario. You can read more about system provisioning in the dedicated Provisioning chapter Output of the command follows this pattern: { \"email\": \"friendly_beaver@email.com\", \"password\": \"12345678\" } \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJleHAiOjE1NDcwMjE3ODAsImlhdCI6MTU0Njk4NTc4MCwiaXNzIjoibWFpbmZsdXgiLCJzdWIiOiJmcmllbmRseV9iZWF2ZXJAZW1haWwuY29tIn0.Tyk31Ae680KqMrDqP895PRZg_GUytLE0IMIR_o3oO7o\" [ { \"id\": \"513d02d2-16c1-4f23-98be-9e12f8fee898\", \"key\": \"69590b3a-9d76-4baa-adae-9b5fec0ea14f\", \"name\": \"d0\", }, { \"id\": \"bf78ca98-2fef-4cfc-9f26-e02da5ecdf67\", \"key\": \"840c1ea1-2e8d-4809-a6d3-3433a5c489d2\", \"name\": \"d1\", } ] [ { \"id\": \"b7bfc4b6-c18d-47c5-b343-98235c5acc19\", \"name\": \"c0\" }, { \"id\": \"378678cd-891b-4a39-b026-869938783f54\", \"name\": \"c1\" } ] In the Mainflux system terminal (where docker compose is running) you should see following logs: mainflux-users | {\"level\":\"info\",\"message\":\"Method register for user friendly_beaver@email.com took 97.573974ms to complete without errors.\",\"ts\":\"2019-01-08T22:16:20.745989495Z\"} mainflux-users | {\"level\":\"info\",\"message\":\"Method login for user friendly_beaver@email.com took 69.308406ms to complete without errors.\",\"ts\":\"2019-01-08T22:16:20.820610461Z\"} mainflux-users | {\"level\":\"info\",\"message\":\"Method identity for client friendly_beaver@email.com took 50.903\u00b5s to complete without errors.\",\"ts\":\"2019-01-08T22:16:20.822208948Z\"} mainflux-things | {\"level\":\"info\",\"message\":\"Method add_thing for token eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJleHAiOjE1NDcwMjE3ODAsImlhdCI6MTU0Njk4NTc4MCwiaXNzIjoibWFpbmZsdXgiLCJzdWIiOiJmcmllbmRseV9iZWF2ZXJAZW1haWwuY29tIn0.Tyk31Ae680KqMrDqP895PRZg_GUytLE0IMIR_o3oO7o and thing 513d02d2-16c1-4f23-98be-9e12f8fee898 took 4.865299ms to complete without errors.\",\"ts\":\"2019-01-08T22:16:20.826786175Z\"} ... This proves that these provisioning commands were sent from the CLI to the Mainflux system.","title":"Step 3 - Provision the System"},{"location":"getting-started/#step-4-send-messages","text":"Once system is provisioned, a thing can start sending messages on a channel : mainflux-cli messages send <channel_id> '[{\"bn\":\"some-base-name:\",\"bt\":1.276020076001e+09, \"bu\":\"A\",\"bver\":5, \"n\":\"voltage\",\"u\":\"V\",\"v\":120.1}, {\"n\":\"current\",\"t\":-5,\"v\":1.2}, {\"n\":\"current\",\"t\":-4,\"v\":1.3}]' <thing_key> For example: mainflux-cli messages send b7bfc4b6-c18d-47c5-b343-98235c5acc19 '[{\"bn\":\"some-base-name:\",\"bt\":1.276020076001e+09, \"bu\":\"A\",\"bver\":5, \"n\":\"voltage\",\"u\":\"V\",\"v\":120.1}, {\"n\":\"current\",\"t\":-5,\"v\":1.2}, {\"n\":\"current\",\"t\":-4,\"v\":1.3}]' 69590b3a-9d76-4baa-adae-9b5fec0ea14f In the Mainflux system terminal you should see following logs: mainflux-things | {\"level\":\"info\",\"message\":\"Method can_access for channel b7bfc4b6-c18d-47c5-b343-98235c5acc19 and thing 513d02d2-16c1-4f23-98be-9e12f8fee898 took 1.410194ms to complete without errors.\",\"ts\":\"2019-01-08T22:19:30.148097648Z\"} mainflux-http | {\"level\":\"info\",\"message\":\"Method publish took 336.685\u00b5s to complete without errors.\",\"ts\":\"2019-01-08T22:19:30.148689601Z\"} This proves that messages have been correctly sent through the system via the protocol adapter ( mainflux-http ).","title":"Step 4 - Send Messages"},{"location":"groups/","text":"Groups # Auth service # For grouping Mainflux entities there are groups object in the auth service. Grouping of entities can be done for things and users but additionally you can use groups for grouping some external entities as well. Groups are organized like a tree, group can have one parent and children. Group with no parent is root of the tree. Create a group # curl -is -S -X POST http://localhost/groups -d '{\"name\":\"<group_name>\",\"description\":\"<group_description>\",\"parent_id\":\"<parent_id>\",\"metadata\":<group_metadata>}' -H 'Content-Type: application/json' -H \"Authorization: Bearer $TOKEN\" HTTP/1.1 201 Created Server: nginx/1.16.0 Date: Fri, 09 Apr 2021 08:02:02 GMT Content-Type: application/json Content-Length: 0 Connection: keep-alive Location: /groups/01F2TTDYGMP6DW083NE6E0DKH2 Access-Control-Expose-Headers: Location group_name - name of the group group_description - description of group up to 1024 characters. parent_id - id of parent group, if not specified created group is tree root. metadata - custom metadata that can be attached to group object for specific application needs. Fetch a group # curl -s -S -X GET http://localhost/groups/01F2TTDYGMP6DW083NE6E0DKH2 -H \"Authorization: Bearer $TOKEN\" Create a child # curl -is -S -X POST http://localhost/groups -d '{\"name\":\"test1\",\"description\":\"<group_description>\",\"parent_id\":\"01F2TTDYGMP6DW083NE6E0DKH2\",\"metadata\":{\"group_attr\":\"attr_value\"}}' -H 'Content-Type: application/json' -H \"Authorization: Bearer $TOKEN\" HTTP/1.1 201 Created Server: nginx/1.16.0 Date: Fri, 09 Apr 2021 08:09:37 GMT Content-Type: application/json Content-Length: 0 Connection: keep-alive Location: /groups/01F2TTVV5NJH63FE5KXMNPWB8P Access-Control-Expose-Headers: Location Fetch a child group # curl -s -S -X GET http://localhost/groups/01F2TTVV5NJH63FE5KXMNPWB8P -H \"Authorization: Bearer $TOKEN\" | jq { \"id\": \"01F2TTVV5NJH63FE5KXMNPWB8P\", \"name\": \"test1\", \"owner_id\": \"8e968002-1b19-4e17-bfb6-f0064888a2d1\", \"parent_id\": \"01F2TTDYGMP6DW083NE6E0DKH2\", \"description\": \"group for test\", \"metadata\": { \"group_attr\": \"attr_value\" }, \"level\": 1, \"created_at\": \"2021-04-09T08:09:37.718Z\", \"updated_at\": \"2021-04-09T08:09:37.718Z\" } owner_id - is attached by service, it is id of the user that issued create request. level - is generated by service, it represents a level in a tree hierarchy created_at - time of creation, generated by service. updated_at - time of update, generated by service. Fetch group hierarchy # To fetch a group hierarchy you can either fetch children for a group or a direct ascendant line Fetch a parent # curl -s -S -X GET http://localhost/groups/<group_id>/parents?tree=true&level=5 -H \"Authorization: Bearer <user_token>\" | jq { \"total\": 2, \"level\": 1, \"name\": \"\", \"groups\": [ { \"id\": \"01F2TTDYGMP6DW083NE6E0DKH2\", \"name\": \"test\", \"owner_id\": \"8e968002-1b19-4e17-bfb6-f0064888a2d1\", \"description\": \"group for test\", \"metadata\": { \"group_attr\": \"attr_value\" }, \"level\": 1, \"path\": \"01F2TTDYGMP6DW083NE6E0DKH2\", \"children\": [ { \"id\": \"01F2TTVV5NJH63FE5KXMNPWB8P\", \"name\": \"test1\", \"owner_id\": \"8e968002-1b19-4e17-bfb6-f0064888a2d1\", \"parent_id\": \"01F2TTDYGMP6DW083NE6E0DKH2\", \"description\": \"group for test\", \"metadata\": { \"group_attr\": \"attr_value\" }, \"level\": 2, \"path\": \"01F2TTDYGMP6DW083NE6E0DKH2.01F2TTVV5NJH63FE5KXMNPWB8P\", \"created_at\": \"2021-04-09T08:09:37.718Z\", \"updated_at\": \"2021-04-09T08:09:37.718Z\" } ], \"created_at\": \"2021-04-09T08:02:02.389Z\", \"updated_at\": \"2021-04-09T08:02:02.389Z\" } ] } tree - if true response is JSON that represent a groups tree structure. If ommited or false than groups will be retrieve as list level - limits the hierarchy to be retrieved. Max level to be fetched in one request is 5. Fetch children # curl -s -S -X GET http://localhost/groups/01F2TTDYGMP6DW083NE6E0DKH2/children\\?tree\\=true\\&level\\=5 -H \"Authorization: Bearer $TOKEN\" | jq { \"total\": 3, \"level\": 5, \"name\": \"\", \"groups\": [ { \"id\": \"01F2TTDYGMP6DW083NE6E0DKH2\", \"name\": \"test\", \"owner_id\": \"8e968002-1b19-4e17-bfb6-f0064888a2d1\", \"description\": \"group for test\", \"metadata\": { \"group_attr\": \"attr_value\" }, \"level\": 1, \"path\": \"01F2TTDYGMP6DW083NE6E0DKH2\", \"children\": [ { \"id\": \"01F2TTVV5NJH63FE5KXMNPWB8P\", \"name\": \"test1\", \"owner_id\": \"8e968002-1b19-4e17-bfb6-f0064888a2d1\", \"parent_id\": \"01F2TTDYGMP6DW083NE6E0DKH2\", \"description\": \"group for test\", \"metadata\": { \"group_attr\": \"attr_value\" }, \"level\": 2, \"path\": \"01F2TTDYGMP6DW083NE6E0DKH2.01F2TTVV5NJH63FE5KXMNPWB8P\", \"created_at\": \"2021-04-09T08:09:37.718Z\", \"updated_at\": \"2021-04-09T08:09:37.718Z\" }, { \"id\": \"01F2TTJPXSZ7K941T57MX3V03M\", \"name\": \"test\", \"owner_id\": \"8e968002-1b19-4e17-bfb6-f0064888a2d1\", \"parent_id\": \"01F2TTDYGMP6DW083NE6E0DKH2\", \"description\": \"group for test\", \"metadata\": { \"group_attr\": \"attr_value\" }, \"level\": 2, \"path\": \"01F2TTDYGMP6DW083NE6E0DKH2.01F2TTJPXSZ7K941T57MX3V03M\", \"created_at\": \"2021-04-09T08:04:38.458Z\", \"updated_at\": \"2021-04-09T08:04:38.458Z\" } ], \"created_at\": \"2021-04-09T08:02:02.389Z\", \"updated_at\": \"2021-04-09T08:02:02.389Z\" } ] } Assign a member to a group # You can assign members to a group by putting entity ids into a group For example assigning users to a group curl -isSX POST http://localhost/groups/<group_id>/members -d '{\"members\":[\"<user_id1>\",...\"<user_idN>\"],\"type\":\"users\"}' -H \"Authorization: Bearer $TOKEN\" -H 'Content-Type: application/json' HTTP/1.1 200 OK Server: nginx/1.16.0 Date: Fri, 09 Apr 2021 09:40:35 GMT Content-Type: application/json Content-Length: 0 Connection: keep-alive Access-Control-Expose-Headers: Location Or assigning things to a group curl -isSX POST http://localhost/groups/01F2TTDYGMP6DW083NE6E0DKH2/members -d '{\"members\":[\"a0b1d516-67c6-4e8d-8ea2-ad4aff444ca3\",\"9a036414-5d47-4122-9e58-b3b6953a2097\"],\"type\":\"things\"}' -H \"Authorization: Bearer $TOKEN\" -H 'Content-Type: application/json' HTTP/1.1 200 OK Server: nginx/1.16.0 Date: Fri, 09 Apr 2021 09:43:24 GMT Content-Type: application/json Content-Length: 0 Connection: keep-alive Access-Control-Expose-Headers: Location To fetch members you can use endpoint on auth , things or users service. Endpoint on auth service only retrieves member ids. Endpoints on things and users will retrieve full objects. Fetching members # Fetching from auth service # curl -sSX GET http://localhost/groups/01F2TTDYGMP6DW083NE6E0DKH2/members -H \"Authorization: Bearer $TOKEN\" | jq { \"limit\": 10, \"total\": 0, \"name\": \"\", \"Members\": [ { \"ID\": \"5ec9f5f4-5221-43f4-a56f-9594ab110efa\", \"Type\": \"users\" }, { \"ID\": \"8e968002-1b19-4e17-bfb6-f0064888a2d1\", \"Type\": \"users\" }, { \"ID\": \"a0b1d516-67c6-4e8d-8ea2-ad4aff444ca3\", \"Type\": \"things\" }, { \"ID\": \"9a036414-5d47-4122-9e58-b3b6953a2097\", \"Type\": \"things\" } ] } You can filter by type curl -sSX GET http://localhost/groups/01F2TTDYGMP6DW083NE6E0DKH2/members\\?type\\='users' -H \"Authorization: Bearer $TOKEN\" | jq { \"limit\": 10, \"total\": 2, \"level\": 0, \"name\": \"\", \"Members\": [ { \"ID\": \"5ec9f5f4-5221-43f4-a56f-9594ab110efa\", \"Type\": \"users\" }, { \"ID\": \"8e968002-1b19-4e17-bfb6-f0064888a2d1\", \"Type\": \"users\" } ] } Fetching from users service # curl -sSX GET http://localhost/groups/users/01F2TTDYGMP6DW083NE6E0DKH2 -H \"Authorization: Bearer $TOKEN\" | jq { \"total\": 2, \"offset\": 0, \"limit\": 10, \"users\": [ { \"id\": \"5ec9f5f4-5221-43f4-a56f-9594ab110efa\", \"email\": \"admin@example.com\" }, { \"id\": \"8e968002-1b19-4e17-bfb6-f0064888a2d1\", \"email\": \"moko@example.com\" } ] } Fetching from things service # curl -sSX GET http://localhost/groups/things/01F2TTDYGMP6DW083NE6E0DKH2 -H \"Authorization: Bearer $TOKEN\" | jq { \"total\": 2, \"offset\": 0, \"limit\": 10, \"order\": \"\", \"direction\": \"\", \"things\": [ { \"id\": \"a0b1d516-67c6-4e8d-8ea2-ad4aff444ca3\", \"key\": \"54dfc44c-3471-4903-a6b5-529dc7a2dd41\" }, { \"id\": \"9a036414-5d47-4122-9e58-b3b6953a2097\", \"key\": \"e874fc75-461d-4ece-ac55-2d7cdc10c20a\" } ] } Fetching membership # For entity that is being put in multiple groups it is possible to retrieve a list of groups it belongs to. curl -sSX GET http://localhost/members/<member_id>/groups -H \"Authorization: Bearer $TOKEN\" | jq { \"total\": 1, \"level\": 0, \"name\": \"\", \"groups\": [ { \"id\": \"01F2TTDYGMP6DW083NE6E0DKH2\", \"name\": \"test\", \"owner_id\": \"8e968002-1b19-4e17-bfb6-f0064888a2d1\", \"description\": \"group for test\", \"metadata\": { \"group_attr\": \"attr_value\" }, \"level\": 0, \"path\": \"\", \"created_at\": \"0001-01-01T00:00:00Z\", \"updated_at\": \"0001-01-01T00:00:00Z\" } ] } member_id - is either thing or user id, or some other enitity if groups are being used for grouping external entities.","title":"Groups"},{"location":"groups/#groups","text":"","title":"Groups"},{"location":"groups/#auth-service","text":"For grouping Mainflux entities there are groups object in the auth service. Grouping of entities can be done for things and users but additionally you can use groups for grouping some external entities as well. Groups are organized like a tree, group can have one parent and children. Group with no parent is root of the tree.","title":"Auth service"},{"location":"groups/#create-a-group","text":"curl -is -S -X POST http://localhost/groups -d '{\"name\":\"<group_name>\",\"description\":\"<group_description>\",\"parent_id\":\"<parent_id>\",\"metadata\":<group_metadata>}' -H 'Content-Type: application/json' -H \"Authorization: Bearer $TOKEN\" HTTP/1.1 201 Created Server: nginx/1.16.0 Date: Fri, 09 Apr 2021 08:02:02 GMT Content-Type: application/json Content-Length: 0 Connection: keep-alive Location: /groups/01F2TTDYGMP6DW083NE6E0DKH2 Access-Control-Expose-Headers: Location group_name - name of the group group_description - description of group up to 1024 characters. parent_id - id of parent group, if not specified created group is tree root. metadata - custom metadata that can be attached to group object for specific application needs.","title":"Create a group"},{"location":"groups/#fetch-a-group","text":"curl -s -S -X GET http://localhost/groups/01F2TTDYGMP6DW083NE6E0DKH2 -H \"Authorization: Bearer $TOKEN\"","title":"Fetch a group"},{"location":"groups/#create-a-child","text":"curl -is -S -X POST http://localhost/groups -d '{\"name\":\"test1\",\"description\":\"<group_description>\",\"parent_id\":\"01F2TTDYGMP6DW083NE6E0DKH2\",\"metadata\":{\"group_attr\":\"attr_value\"}}' -H 'Content-Type: application/json' -H \"Authorization: Bearer $TOKEN\" HTTP/1.1 201 Created Server: nginx/1.16.0 Date: Fri, 09 Apr 2021 08:09:37 GMT Content-Type: application/json Content-Length: 0 Connection: keep-alive Location: /groups/01F2TTVV5NJH63FE5KXMNPWB8P Access-Control-Expose-Headers: Location","title":"Create a child"},{"location":"groups/#fetch-a-child-group","text":"curl -s -S -X GET http://localhost/groups/01F2TTVV5NJH63FE5KXMNPWB8P -H \"Authorization: Bearer $TOKEN\" | jq { \"id\": \"01F2TTVV5NJH63FE5KXMNPWB8P\", \"name\": \"test1\", \"owner_id\": \"8e968002-1b19-4e17-bfb6-f0064888a2d1\", \"parent_id\": \"01F2TTDYGMP6DW083NE6E0DKH2\", \"description\": \"group for test\", \"metadata\": { \"group_attr\": \"attr_value\" }, \"level\": 1, \"created_at\": \"2021-04-09T08:09:37.718Z\", \"updated_at\": \"2021-04-09T08:09:37.718Z\" } owner_id - is attached by service, it is id of the user that issued create request. level - is generated by service, it represents a level in a tree hierarchy created_at - time of creation, generated by service. updated_at - time of update, generated by service.","title":"Fetch a child group"},{"location":"groups/#fetch-group-hierarchy","text":"To fetch a group hierarchy you can either fetch children for a group or a direct ascendant line","title":"Fetch group hierarchy"},{"location":"groups/#fetch-a-parent","text":"curl -s -S -X GET http://localhost/groups/<group_id>/parents?tree=true&level=5 -H \"Authorization: Bearer <user_token>\" | jq { \"total\": 2, \"level\": 1, \"name\": \"\", \"groups\": [ { \"id\": \"01F2TTDYGMP6DW083NE6E0DKH2\", \"name\": \"test\", \"owner_id\": \"8e968002-1b19-4e17-bfb6-f0064888a2d1\", \"description\": \"group for test\", \"metadata\": { \"group_attr\": \"attr_value\" }, \"level\": 1, \"path\": \"01F2TTDYGMP6DW083NE6E0DKH2\", \"children\": [ { \"id\": \"01F2TTVV5NJH63FE5KXMNPWB8P\", \"name\": \"test1\", \"owner_id\": \"8e968002-1b19-4e17-bfb6-f0064888a2d1\", \"parent_id\": \"01F2TTDYGMP6DW083NE6E0DKH2\", \"description\": \"group for test\", \"metadata\": { \"group_attr\": \"attr_value\" }, \"level\": 2, \"path\": \"01F2TTDYGMP6DW083NE6E0DKH2.01F2TTVV5NJH63FE5KXMNPWB8P\", \"created_at\": \"2021-04-09T08:09:37.718Z\", \"updated_at\": \"2021-04-09T08:09:37.718Z\" } ], \"created_at\": \"2021-04-09T08:02:02.389Z\", \"updated_at\": \"2021-04-09T08:02:02.389Z\" } ] } tree - if true response is JSON that represent a groups tree structure. If ommited or false than groups will be retrieve as list level - limits the hierarchy to be retrieved. Max level to be fetched in one request is 5.","title":"Fetch a parent"},{"location":"groups/#fetch-children","text":"curl -s -S -X GET http://localhost/groups/01F2TTDYGMP6DW083NE6E0DKH2/children\\?tree\\=true\\&level\\=5 -H \"Authorization: Bearer $TOKEN\" | jq { \"total\": 3, \"level\": 5, \"name\": \"\", \"groups\": [ { \"id\": \"01F2TTDYGMP6DW083NE6E0DKH2\", \"name\": \"test\", \"owner_id\": \"8e968002-1b19-4e17-bfb6-f0064888a2d1\", \"description\": \"group for test\", \"metadata\": { \"group_attr\": \"attr_value\" }, \"level\": 1, \"path\": \"01F2TTDYGMP6DW083NE6E0DKH2\", \"children\": [ { \"id\": \"01F2TTVV5NJH63FE5KXMNPWB8P\", \"name\": \"test1\", \"owner_id\": \"8e968002-1b19-4e17-bfb6-f0064888a2d1\", \"parent_id\": \"01F2TTDYGMP6DW083NE6E0DKH2\", \"description\": \"group for test\", \"metadata\": { \"group_attr\": \"attr_value\" }, \"level\": 2, \"path\": \"01F2TTDYGMP6DW083NE6E0DKH2.01F2TTVV5NJH63FE5KXMNPWB8P\", \"created_at\": \"2021-04-09T08:09:37.718Z\", \"updated_at\": \"2021-04-09T08:09:37.718Z\" }, { \"id\": \"01F2TTJPXSZ7K941T57MX3V03M\", \"name\": \"test\", \"owner_id\": \"8e968002-1b19-4e17-bfb6-f0064888a2d1\", \"parent_id\": \"01F2TTDYGMP6DW083NE6E0DKH2\", \"description\": \"group for test\", \"metadata\": { \"group_attr\": \"attr_value\" }, \"level\": 2, \"path\": \"01F2TTDYGMP6DW083NE6E0DKH2.01F2TTJPXSZ7K941T57MX3V03M\", \"created_at\": \"2021-04-09T08:04:38.458Z\", \"updated_at\": \"2021-04-09T08:04:38.458Z\" } ], \"created_at\": \"2021-04-09T08:02:02.389Z\", \"updated_at\": \"2021-04-09T08:02:02.389Z\" } ] }","title":"Fetch children"},{"location":"groups/#assign-a-member-to-a-group","text":"You can assign members to a group by putting entity ids into a group For example assigning users to a group curl -isSX POST http://localhost/groups/<group_id>/members -d '{\"members\":[\"<user_id1>\",...\"<user_idN>\"],\"type\":\"users\"}' -H \"Authorization: Bearer $TOKEN\" -H 'Content-Type: application/json' HTTP/1.1 200 OK Server: nginx/1.16.0 Date: Fri, 09 Apr 2021 09:40:35 GMT Content-Type: application/json Content-Length: 0 Connection: keep-alive Access-Control-Expose-Headers: Location Or assigning things to a group curl -isSX POST http://localhost/groups/01F2TTDYGMP6DW083NE6E0DKH2/members -d '{\"members\":[\"a0b1d516-67c6-4e8d-8ea2-ad4aff444ca3\",\"9a036414-5d47-4122-9e58-b3b6953a2097\"],\"type\":\"things\"}' -H \"Authorization: Bearer $TOKEN\" -H 'Content-Type: application/json' HTTP/1.1 200 OK Server: nginx/1.16.0 Date: Fri, 09 Apr 2021 09:43:24 GMT Content-Type: application/json Content-Length: 0 Connection: keep-alive Access-Control-Expose-Headers: Location To fetch members you can use endpoint on auth , things or users service. Endpoint on auth service only retrieves member ids. Endpoints on things and users will retrieve full objects.","title":"Assign a member to a group"},{"location":"groups/#fetching-members","text":"","title":"Fetching members"},{"location":"groups/#fetching-from-auth-service","text":"curl -sSX GET http://localhost/groups/01F2TTDYGMP6DW083NE6E0DKH2/members -H \"Authorization: Bearer $TOKEN\" | jq { \"limit\": 10, \"total\": 0, \"name\": \"\", \"Members\": [ { \"ID\": \"5ec9f5f4-5221-43f4-a56f-9594ab110efa\", \"Type\": \"users\" }, { \"ID\": \"8e968002-1b19-4e17-bfb6-f0064888a2d1\", \"Type\": \"users\" }, { \"ID\": \"a0b1d516-67c6-4e8d-8ea2-ad4aff444ca3\", \"Type\": \"things\" }, { \"ID\": \"9a036414-5d47-4122-9e58-b3b6953a2097\", \"Type\": \"things\" } ] } You can filter by type curl -sSX GET http://localhost/groups/01F2TTDYGMP6DW083NE6E0DKH2/members\\?type\\='users' -H \"Authorization: Bearer $TOKEN\" | jq { \"limit\": 10, \"total\": 2, \"level\": 0, \"name\": \"\", \"Members\": [ { \"ID\": \"5ec9f5f4-5221-43f4-a56f-9594ab110efa\", \"Type\": \"users\" }, { \"ID\": \"8e968002-1b19-4e17-bfb6-f0064888a2d1\", \"Type\": \"users\" } ] }","title":"Fetching from auth service"},{"location":"groups/#fetching-from-users-service","text":"curl -sSX GET http://localhost/groups/users/01F2TTDYGMP6DW083NE6E0DKH2 -H \"Authorization: Bearer $TOKEN\" | jq { \"total\": 2, \"offset\": 0, \"limit\": 10, \"users\": [ { \"id\": \"5ec9f5f4-5221-43f4-a56f-9594ab110efa\", \"email\": \"admin@example.com\" }, { \"id\": \"8e968002-1b19-4e17-bfb6-f0064888a2d1\", \"email\": \"moko@example.com\" } ] }","title":"Fetching from users service"},{"location":"groups/#fetching-from-things-service","text":"curl -sSX GET http://localhost/groups/things/01F2TTDYGMP6DW083NE6E0DKH2 -H \"Authorization: Bearer $TOKEN\" | jq { \"total\": 2, \"offset\": 0, \"limit\": 10, \"order\": \"\", \"direction\": \"\", \"things\": [ { \"id\": \"a0b1d516-67c6-4e8d-8ea2-ad4aff444ca3\", \"key\": \"54dfc44c-3471-4903-a6b5-529dc7a2dd41\" }, { \"id\": \"9a036414-5d47-4122-9e58-b3b6953a2097\", \"key\": \"e874fc75-461d-4ece-ac55-2d7cdc10c20a\" } ] }","title":"Fetching from things service"},{"location":"groups/#fetching-membership","text":"For entity that is being put in multiple groups it is possible to retrieve a list of groups it belongs to. curl -sSX GET http://localhost/members/<member_id>/groups -H \"Authorization: Bearer $TOKEN\" | jq { \"total\": 1, \"level\": 0, \"name\": \"\", \"groups\": [ { \"id\": \"01F2TTDYGMP6DW083NE6E0DKH2\", \"name\": \"test\", \"owner_id\": \"8e968002-1b19-4e17-bfb6-f0064888a2d1\", \"description\": \"group for test\", \"metadata\": { \"group_attr\": \"attr_value\" }, \"level\": 0, \"path\": \"\", \"created_at\": \"0001-01-01T00:00:00Z\", \"updated_at\": \"0001-01-01T00:00:00Z\" } ] } member_id - is either thing or user id, or some other enitity if groups are being used for grouping external entities.","title":"Fetching membership"},{"location":"kubernetes/","text":"Kubernetes # Mainflux can be easily deployed on Kubernetes platform by using Helm Chart from official Mainflux DevOps GitHub repository . Prerequisites # Kubernetes kubectl Helm v3 Stable Helm repository Nginx Ingress Controller Kubernetes # Kubernetes is an open source container orchestration engine for automating deployment, scaling, and management of containerised applications. Install it locally or have access to a cluster. Follow these instructions if you need more information. Kubectl # Kubectl is official Kubernetes command line client. Follow these instructions to install it. Regarding the cluster control with kubectl , default config .yaml file should be ~/.kube/config . Helm v3 # Helm is the package manager for Kubernetes. Follow these instructions to install it. Stable Helm Repository # Add a stable chart repository: helm repo add stable https://charts.helm.sh/stable Add a bitnami chart repository: helm repo add bitnami https://charts.bitnami.com/bitnami Nginx Ingress Controller # Follow these instructions to install it or: helm install ingress-nginx ingress-nginx/ingress-nginx --version 3.26.0 --create-namespace -n ingress-nginx Deploying Mainflux # Get Helm charts from Mainflux DevOps GitHub repository : git clone https://github.com/mainflux/devops.git cd devops/charts/mainflux Update the on-disk dependencies to mirror Chart.yaml: helm dependency update If you didn't already have namespace created you should do it with: kubectl create namespace mf Deploying release named mainflux in namespace named mf is done with just: helm install mainflux . -n mf Mainflux is now deployed on your Kubernetes. Customizing Installation # You can override default values while installing with --set option. For example, if you want to specify ingress hostname and pull latest tag of users image: helm install mainflux -n mf --set ingress.hostname='example.com' --set users.image.tag='latest' Or if release is already installed, you can update it: helm upgrade mainflux -n mf --set ingress.hostname='example.com' --set users.image.tag='latest' The following table lists the configurable parameters and their default values. Parameter Description Default defaults.logLevel Log level debug defaults.image.pullPolicy Docker Image Pull Policy IfNotPresent defaults.image.repository Docker Image Repository mainflux defaults.image.tag Docker Image Tag 0.11.0 defaults.replicaCount Replicas of MQTT adapter, Things, Envoy and Authn 3 defaults.natsPort NATS port 4222 defaults.jaegerPort Jaeger port 6831 nginxInternal.mtls.tls TLS secret which contains the server cert/key nginxInternal.mtls.intermediateCrt Generic secret which contains the intermediate cert used to verify clients ingress.enabled Should the Nginx Ingress be created true ingress.hostname Hostname for the Nginx Ingress ingress.tls.hostname Hostname of the Nginx Ingress certificate ingress.tls.secret TLS secret for the Nginx Ingress nats.maxPayload Maximum payload size in bytes that the NATS server will accept 268435456 nats.replicaCount NATS replicas 3 authn.dbPort AuthN service DB port 5432 authn.grpcPort AuthN service gRPC port 8181 authn.httpPort AuthN service HTTP port 8189 users.dbPort Users service DB port 5432 users.httpPort Users service HTTP port 8180 things.dbPort Things service DB port 5432 things.httpPort Things service HTTP port 8182 things.authGrpcPort Things service Auth gRPC port 8183 things.authHttpPort Things service Auth HTTP port 8989 things.redisESPort Things service Redis Event Store port 6379 things.redisCachePort Things service Redis Auth Cache port 6379 adapter_http.httpPort HTTP adapter port 8185 mqtt.proxy.mqttPort MQTT adapter proxy port 1884 mqtt.proxy.wsPort MQTT adapter proxy WS port 8081 mqtt.broker.mqttPort MQTT adapter broker port 1883 mqtt.broker.wsPort MQTT adapter broker WS port 8080 mqtt.broker.persistentVolume.size MQTT adapter broker data Persistent Volume size 5Gi mqtt.redisESPort MQTT adapter Event Store port 6379 mqtt.redisCachePort MQTT adapter Redis Auth Cache port 6379 adapter_coap.udpPort CoAP adapter UDP port 5683 ui.port UI port 3000 bootstrap.enabled Enable bootstrap service false bootstrap.dbPort Bootstrap service DB port 5432 bootstrap.httpPort Bootstrap service HTTP port 8182 bootstrap.redisESPort Bootstrap service Redis Event Store port 6379 influxdb.enabled Enable InfluxDB reader & writer false influxdb.dbPort InfluxDB port 8086 influxdb.writer.httpPort InfluxDB writer HTTP port 8900 influxdb.reader.httpPort InfluxDB reader HTTP port 8905 adapter_opcua.enabled Enable OPC-UA adapter false adapter_opcua.httpPort OPC-UA adapter HTTP port 8188 adapter_opcua.redisRouteMapPort OPC-UA adapter Redis Auth Cache port 6379 adapter_lora.enabled Enable LoRa adapter false adapter_lora.httpPort LoRa adapter HTTP port 8187 adapter_lora.redisRouteMapPort LoRa adapter Redis Auth Cache port 6379 twins.enabled Enable twins service false twins.dbPort Twins service DB port 27017 twins.httpPort Twins service HTTP port 9021 twins.redisCachePort Twins service Redis Cache port 6379 All Mainflux services (both core and add-ons) can have their logLevel , image.pullPolicy , image.repository and image.tag overridden. Mainflux Core is a minimalistic set of required Mainflux services. They are all installed by default: authn users things adapter_http adapter_mqtt adapter_coap ui Mainflux Add-ons are optional services that are disabled by default. Find in Configuration table parameters for enabling them, i.e. to enable influxdb reader & writer you should run helm install with --set influxdb=true . List of add-ons services in charts: bootstrap influxdb.writer influxdb.reader adapter_opcua adapter_lora twins By default scale of MQTT adapter, Things, Envoy, Authn and NATS will be set to 3. It's recommended that you set this values to number of your nodes in Kubernetes cluster, i.e. --set defaults.replicaCount=3 --set nats.replicaCount=3 Additional Steps to Configure Ingress Controller # To send MQTT messages to your host on ports 1883 and 8883 some additional steps are required in configuring NGINX Ingress Controller. NGINX Ingress Controller uses ConfigMap to expose TCP services. That ConfigMap is included in helm chart in ingress.yaml file assuming that location of ConfigMap should be ingress-nginx/tcp-services . If Ingress Controller expects it in some other namespace or with other name you should edit metadata in ingress.yaml . This location was set with --tcp-services-configmap flag and you can check it in deployment of Ingress Controller or add it there in args section for nginx-ingress-controller if it's not already specified. This is explained in NGINX Ingress documentation Also, those two ports need to be exposed in the Service defined for the Ingress. You can do that with command that edit your service: kubectl edit svc -n ingress-nginx nginx-ingress-ingress-nginx-controller and add in spec->ports: - name: mqtt port: 1883 protocol: TCP targetPort: 1883 - name: mqtts port: 8883 protocol: TCP targetPort: 8883 TLS & mTLS # For testing purposes you can generate certificates as explained in detail in authentication chapter of this document. So, you can use this script and after replacing all localhost with your hostname, run: make ca make server_cert make thing_cert KEY=<thing_key> you should get in certs folder these certificates that we will use for setting up TLS and mTLS: ca.crt ca.key ca.srl mainflux-server.crt mainflux-server.key thing.crt thing.key Create kubernetes secrets using those certificates with running commands from (secrets script)[https://github.com/mainflux/devops/blob/master/charts/mainflux/secrets/secrets.sh]. In this example secrets are created in mf namespace: kubectl -n mf create secret tls mainflux-server \\ --key mainflux-server.key \\ --cert mainflux-server.crt kubectl -n mf create secret generic ca \\ --from-file=ca.crt You can check if they are succesfully created: kubectl get secrets -n mf And now set ingress.hostname, ingress.tls.hostname to your hostname and ingress.tls.secret to mainflux-server and after helm update you have secured ingress with TLS certificate. For mTLS you need to set nginx_internal.mtls.tls=\"mainflux-server\" and nginx_internal.mtls.intermediate_crt=\"ca\" . Now you can test sending mqtt message with this parameters: mosquitto_pub -d -L mqtts://<thing_id>:<thing_key>@example.com:8883/channels/<channel_id>/messages --cert thing.crt --key thing.key --cafile ca.crt -m \"test-message\"","title":"Kubernetes"},{"location":"kubernetes/#kubernetes","text":"Mainflux can be easily deployed on Kubernetes platform by using Helm Chart from official Mainflux DevOps GitHub repository .","title":"Kubernetes"},{"location":"kubernetes/#prerequisites","text":"Kubernetes kubectl Helm v3 Stable Helm repository Nginx Ingress Controller","title":"Prerequisites"},{"location":"kubernetes/#kubernetes_1","text":"Kubernetes is an open source container orchestration engine for automating deployment, scaling, and management of containerised applications. Install it locally or have access to a cluster. Follow these instructions if you need more information.","title":"Kubernetes"},{"location":"kubernetes/#kubectl","text":"Kubectl is official Kubernetes command line client. Follow these instructions to install it. Regarding the cluster control with kubectl , default config .yaml file should be ~/.kube/config .","title":"Kubectl"},{"location":"kubernetes/#helm-v3","text":"Helm is the package manager for Kubernetes. Follow these instructions to install it.","title":"Helm v3"},{"location":"kubernetes/#stable-helm-repository","text":"Add a stable chart repository: helm repo add stable https://charts.helm.sh/stable Add a bitnami chart repository: helm repo add bitnami https://charts.bitnami.com/bitnami","title":"Stable Helm Repository"},{"location":"kubernetes/#nginx-ingress-controller","text":"Follow these instructions to install it or: helm install ingress-nginx ingress-nginx/ingress-nginx --version 3.26.0 --create-namespace -n ingress-nginx","title":"Nginx Ingress Controller"},{"location":"kubernetes/#deploying-mainflux","text":"Get Helm charts from Mainflux DevOps GitHub repository : git clone https://github.com/mainflux/devops.git cd devops/charts/mainflux Update the on-disk dependencies to mirror Chart.yaml: helm dependency update If you didn't already have namespace created you should do it with: kubectl create namespace mf Deploying release named mainflux in namespace named mf is done with just: helm install mainflux . -n mf Mainflux is now deployed on your Kubernetes.","title":"Deploying Mainflux"},{"location":"kubernetes/#customizing-installation","text":"You can override default values while installing with --set option. For example, if you want to specify ingress hostname and pull latest tag of users image: helm install mainflux -n mf --set ingress.hostname='example.com' --set users.image.tag='latest' Or if release is already installed, you can update it: helm upgrade mainflux -n mf --set ingress.hostname='example.com' --set users.image.tag='latest' The following table lists the configurable parameters and their default values. Parameter Description Default defaults.logLevel Log level debug defaults.image.pullPolicy Docker Image Pull Policy IfNotPresent defaults.image.repository Docker Image Repository mainflux defaults.image.tag Docker Image Tag 0.11.0 defaults.replicaCount Replicas of MQTT adapter, Things, Envoy and Authn 3 defaults.natsPort NATS port 4222 defaults.jaegerPort Jaeger port 6831 nginxInternal.mtls.tls TLS secret which contains the server cert/key nginxInternal.mtls.intermediateCrt Generic secret which contains the intermediate cert used to verify clients ingress.enabled Should the Nginx Ingress be created true ingress.hostname Hostname for the Nginx Ingress ingress.tls.hostname Hostname of the Nginx Ingress certificate ingress.tls.secret TLS secret for the Nginx Ingress nats.maxPayload Maximum payload size in bytes that the NATS server will accept 268435456 nats.replicaCount NATS replicas 3 authn.dbPort AuthN service DB port 5432 authn.grpcPort AuthN service gRPC port 8181 authn.httpPort AuthN service HTTP port 8189 users.dbPort Users service DB port 5432 users.httpPort Users service HTTP port 8180 things.dbPort Things service DB port 5432 things.httpPort Things service HTTP port 8182 things.authGrpcPort Things service Auth gRPC port 8183 things.authHttpPort Things service Auth HTTP port 8989 things.redisESPort Things service Redis Event Store port 6379 things.redisCachePort Things service Redis Auth Cache port 6379 adapter_http.httpPort HTTP adapter port 8185 mqtt.proxy.mqttPort MQTT adapter proxy port 1884 mqtt.proxy.wsPort MQTT adapter proxy WS port 8081 mqtt.broker.mqttPort MQTT adapter broker port 1883 mqtt.broker.wsPort MQTT adapter broker WS port 8080 mqtt.broker.persistentVolume.size MQTT adapter broker data Persistent Volume size 5Gi mqtt.redisESPort MQTT adapter Event Store port 6379 mqtt.redisCachePort MQTT adapter Redis Auth Cache port 6379 adapter_coap.udpPort CoAP adapter UDP port 5683 ui.port UI port 3000 bootstrap.enabled Enable bootstrap service false bootstrap.dbPort Bootstrap service DB port 5432 bootstrap.httpPort Bootstrap service HTTP port 8182 bootstrap.redisESPort Bootstrap service Redis Event Store port 6379 influxdb.enabled Enable InfluxDB reader & writer false influxdb.dbPort InfluxDB port 8086 influxdb.writer.httpPort InfluxDB writer HTTP port 8900 influxdb.reader.httpPort InfluxDB reader HTTP port 8905 adapter_opcua.enabled Enable OPC-UA adapter false adapter_opcua.httpPort OPC-UA adapter HTTP port 8188 adapter_opcua.redisRouteMapPort OPC-UA adapter Redis Auth Cache port 6379 adapter_lora.enabled Enable LoRa adapter false adapter_lora.httpPort LoRa adapter HTTP port 8187 adapter_lora.redisRouteMapPort LoRa adapter Redis Auth Cache port 6379 twins.enabled Enable twins service false twins.dbPort Twins service DB port 27017 twins.httpPort Twins service HTTP port 9021 twins.redisCachePort Twins service Redis Cache port 6379 All Mainflux services (both core and add-ons) can have their logLevel , image.pullPolicy , image.repository and image.tag overridden. Mainflux Core is a minimalistic set of required Mainflux services. They are all installed by default: authn users things adapter_http adapter_mqtt adapter_coap ui Mainflux Add-ons are optional services that are disabled by default. Find in Configuration table parameters for enabling them, i.e. to enable influxdb reader & writer you should run helm install with --set influxdb=true . List of add-ons services in charts: bootstrap influxdb.writer influxdb.reader adapter_opcua adapter_lora twins By default scale of MQTT adapter, Things, Envoy, Authn and NATS will be set to 3. It's recommended that you set this values to number of your nodes in Kubernetes cluster, i.e. --set defaults.replicaCount=3 --set nats.replicaCount=3","title":"Customizing Installation"},{"location":"kubernetes/#additional-steps-to-configure-ingress-controller","text":"To send MQTT messages to your host on ports 1883 and 8883 some additional steps are required in configuring NGINX Ingress Controller. NGINX Ingress Controller uses ConfigMap to expose TCP services. That ConfigMap is included in helm chart in ingress.yaml file assuming that location of ConfigMap should be ingress-nginx/tcp-services . If Ingress Controller expects it in some other namespace or with other name you should edit metadata in ingress.yaml . This location was set with --tcp-services-configmap flag and you can check it in deployment of Ingress Controller or add it there in args section for nginx-ingress-controller if it's not already specified. This is explained in NGINX Ingress documentation Also, those two ports need to be exposed in the Service defined for the Ingress. You can do that with command that edit your service: kubectl edit svc -n ingress-nginx nginx-ingress-ingress-nginx-controller and add in spec->ports: - name: mqtt port: 1883 protocol: TCP targetPort: 1883 - name: mqtts port: 8883 protocol: TCP targetPort: 8883","title":"Additional Steps to Configure Ingress Controller"},{"location":"kubernetes/#tls-mtls","text":"For testing purposes you can generate certificates as explained in detail in authentication chapter of this document. So, you can use this script and after replacing all localhost with your hostname, run: make ca make server_cert make thing_cert KEY=<thing_key> you should get in certs folder these certificates that we will use for setting up TLS and mTLS: ca.crt ca.key ca.srl mainflux-server.crt mainflux-server.key thing.crt thing.key Create kubernetes secrets using those certificates with running commands from (secrets script)[https://github.com/mainflux/devops/blob/master/charts/mainflux/secrets/secrets.sh]. In this example secrets are created in mf namespace: kubectl -n mf create secret tls mainflux-server \\ --key mainflux-server.key \\ --cert mainflux-server.crt kubectl -n mf create secret generic ca \\ --from-file=ca.crt You can check if they are succesfully created: kubectl get secrets -n mf And now set ingress.hostname, ingress.tls.hostname to your hostname and ingress.tls.secret to mainflux-server and after helm update you have secured ingress with TLS certificate. For mTLS you need to set nginx_internal.mtls.tls=\"mainflux-server\" and nginx_internal.mtls.intermediate_crt=\"ca\" . Now you can test sending mqtt message with this parameters: mosquitto_pub -d -L mqtts://<thing_id>:<thing_key>@example.com:8883/channels/<channel_id>/messages --cert thing.crt --key thing.key --cafile ca.crt -m \"test-message\"","title":"TLS &amp; mTLS"},{"location":"lora/","text":"LoRa # Bridging with LoRaWAN Networks can be done over the lora-adapter . This service sits between Mainflux and LoRa Server and just forwards the messages from one system to another via MQTT protocol, using the adequate MQTT topics and in the good message format (JSON and SenML), i.e. respecting the APIs of both systems. LoRa Server is used for connectivity layer. Specially for the LoRa Gateway Bridge service, which abstracts the SemTech packet-forwarder UDP protocol into JSON over MQTT. But also for the LoRa Server service, responsible of the de-duplication and handling of uplink frames received by the gateway(s), handling of the LoRaWAN mac-layer and scheduling of downlink data transmissions. Finally the Lora App Server services is used to interact with the system. Run Lora Server # Before to run the lora-adapter you must install and run LoRa Server. First, execute the following command: go get github.com/brocaar/loraserver-docker Once everything is installed, execute the following command from the LoRa Server project root: docker-compose up Troubleshouting: Mainflux and LoRa Server use their own MQTT brokers which by default occupy MQTT port 1883 . If both are ran on the same machine different ports must be used. You can fix this on Mainflux side by configuring the environment variable MF_MQTT_ADAPTER_MQTT_PORT . Setup LoRa Server # Now that both systems are running you must provision LoRa Server, which offers for integration with external services, a RESTful and gRPC API. You can do it as well over the LoRa App Server , which is good example of integration. Create an Organization: To add your own Gateways to the network you must have an Organization. Create a Network: Set the address of your Network-Server API that is used by LoRa App Server or other custom components interacting with LoRa Server (by default loraserver:8000). Create a Gateways-Profile: In this profile you can select the radio LoRa channels and the LoRa Network Server to use. Create a Service-profile: A service-profile connects an organization to a network-server and defines the features that an organization can use on this Network-Server. Create a Gateway: You must set proper ID in order to be discovered by LoRa Server. Create an Application: This will allows you to create Devices by connecting them to this application. This is equivalent to Devices connected to channels in Mainflux. Create a Device-Profile: Before creating Device you must create Device profile where you will define some parameter as LoRaWAN MAC version (format of the device address) and the LoRaWAN regional parameter (frequency band). This will allow you to create many devices using this profile. Create a Device: Finally, you can create a Device. You must configure the network session key and application session key of your Device. You can generate and copy them on your device configuration or you can use your own pre generated keys and set them using the LoRa App Server UI. Device connect through OTAA. Make sure that loraserver device-profile is using same release as device. If MAC version is 1.0.X, application key = app_key and app_eui = deviceEUI . If MAC version is 1.1 or ABP both parameters will be needed, APP_key and Network key. Mainflux and LoRa Server # Once everything is running and the LoRa Server is provisioned, execute the following command from Mainflux project root to run the lora-adapter: docker-compose -f docker/addons/lora-adapter/docker-compose.yml up -d Troubleshouting: The lora-adapter subscribes to the LoRa Server MQTT broker and will fail if the connection is not established. You must ensure that the environment variable MF_LORA_ADAPTER_MESSAGES_URL is propertly configured. Remark: By defaut, MF_LORA_ADAPTER_MESSAGES_URL is set as tcp://lora.mqtt.mainflux.io:1883 in the docker-compose.yml file of the adapter. If you run the composition without configure this variable you will start to receive messages from our demo server. Route Map # The lora-adapter use Redis database to create a route map between both systems. As in Mainflux we use Channels to connect Things, LoRa Server uses Applications to connect Devices. The lora-adapter uses the matadata of provision events emitted by Mainflux system to update his route map. For that, you must provision Mainflux Channels and Things with an extra metadata key in the JSON Body of the HTTP request. It must be a JSON object with key lora which value is another JSON object. This nested JSON object should contain app_id or dev_eui field. In this case app_id or dev_eui must be an existent Lora application ID or device EUI: Channel structure: { \"name\": \"<channel name>\", \"metadata:\": { \"lora\": { \"app_id\": \"<application ID>\" } } } Thing structure: { \"type\": \"device\", \"name\": \"<thing name>\", \"metadata:\": { \"lora\": { \"dev_eui\": \"<device EUI>\" } } } Messaging # To forward LoRa messages the lora-adapter subscribes to topics applications/+/devices/+ of the LoRa Server MQTT broker. It verifies the app_id and the dev_eui of received messages. If the mapping exists it uses corresponding Channel ID and Thing ID to sign and forwards the content of the LoRa message to the Mainflux message broker.","title":"LoRa"},{"location":"lora/#lora","text":"Bridging with LoRaWAN Networks can be done over the lora-adapter . This service sits between Mainflux and LoRa Server and just forwards the messages from one system to another via MQTT protocol, using the adequate MQTT topics and in the good message format (JSON and SenML), i.e. respecting the APIs of both systems. LoRa Server is used for connectivity layer. Specially for the LoRa Gateway Bridge service, which abstracts the SemTech packet-forwarder UDP protocol into JSON over MQTT. But also for the LoRa Server service, responsible of the de-duplication and handling of uplink frames received by the gateway(s), handling of the LoRaWAN mac-layer and scheduling of downlink data transmissions. Finally the Lora App Server services is used to interact with the system.","title":"LoRa"},{"location":"lora/#run-lora-server","text":"Before to run the lora-adapter you must install and run LoRa Server. First, execute the following command: go get github.com/brocaar/loraserver-docker Once everything is installed, execute the following command from the LoRa Server project root: docker-compose up Troubleshouting: Mainflux and LoRa Server use their own MQTT brokers which by default occupy MQTT port 1883 . If both are ran on the same machine different ports must be used. You can fix this on Mainflux side by configuring the environment variable MF_MQTT_ADAPTER_MQTT_PORT .","title":"Run Lora Server"},{"location":"lora/#setup-lora-server","text":"Now that both systems are running you must provision LoRa Server, which offers for integration with external services, a RESTful and gRPC API. You can do it as well over the LoRa App Server , which is good example of integration. Create an Organization: To add your own Gateways to the network you must have an Organization. Create a Network: Set the address of your Network-Server API that is used by LoRa App Server or other custom components interacting with LoRa Server (by default loraserver:8000). Create a Gateways-Profile: In this profile you can select the radio LoRa channels and the LoRa Network Server to use. Create a Service-profile: A service-profile connects an organization to a network-server and defines the features that an organization can use on this Network-Server. Create a Gateway: You must set proper ID in order to be discovered by LoRa Server. Create an Application: This will allows you to create Devices by connecting them to this application. This is equivalent to Devices connected to channels in Mainflux. Create a Device-Profile: Before creating Device you must create Device profile where you will define some parameter as LoRaWAN MAC version (format of the device address) and the LoRaWAN regional parameter (frequency band). This will allow you to create many devices using this profile. Create a Device: Finally, you can create a Device. You must configure the network session key and application session key of your Device. You can generate and copy them on your device configuration or you can use your own pre generated keys and set them using the LoRa App Server UI. Device connect through OTAA. Make sure that loraserver device-profile is using same release as device. If MAC version is 1.0.X, application key = app_key and app_eui = deviceEUI . If MAC version is 1.1 or ABP both parameters will be needed, APP_key and Network key.","title":"Setup LoRa Server"},{"location":"lora/#mainflux-and-lora-server","text":"Once everything is running and the LoRa Server is provisioned, execute the following command from Mainflux project root to run the lora-adapter: docker-compose -f docker/addons/lora-adapter/docker-compose.yml up -d Troubleshouting: The lora-adapter subscribes to the LoRa Server MQTT broker and will fail if the connection is not established. You must ensure that the environment variable MF_LORA_ADAPTER_MESSAGES_URL is propertly configured. Remark: By defaut, MF_LORA_ADAPTER_MESSAGES_URL is set as tcp://lora.mqtt.mainflux.io:1883 in the docker-compose.yml file of the adapter. If you run the composition without configure this variable you will start to receive messages from our demo server.","title":"Mainflux and LoRa Server"},{"location":"lora/#route-map","text":"The lora-adapter use Redis database to create a route map between both systems. As in Mainflux we use Channels to connect Things, LoRa Server uses Applications to connect Devices. The lora-adapter uses the matadata of provision events emitted by Mainflux system to update his route map. For that, you must provision Mainflux Channels and Things with an extra metadata key in the JSON Body of the HTTP request. It must be a JSON object with key lora which value is another JSON object. This nested JSON object should contain app_id or dev_eui field. In this case app_id or dev_eui must be an existent Lora application ID or device EUI: Channel structure: { \"name\": \"<channel name>\", \"metadata:\": { \"lora\": { \"app_id\": \"<application ID>\" } } } Thing structure: { \"type\": \"device\", \"name\": \"<thing name>\", \"metadata:\": { \"lora\": { \"dev_eui\": \"<device EUI>\" } } }","title":"Route Map"},{"location":"lora/#messaging","text":"To forward LoRa messages the lora-adapter subscribes to topics applications/+/devices/+ of the LoRa Server MQTT broker. It verifies the app_id and the dev_eui of received messages. If the mapping exists it uses corresponding Channel ID and Thing ID to sign and forwards the content of the LoRa message to the Mainflux message broker.","title":"Messaging"},{"location":"messaging/","text":"Messaging # Once a channel is provisioned and thing is connected to it, it can start to publish messages on the channel. The following sections will provide an example of message publishing for each of the supported protocols. HTTP # To publish message over channel, thing should send following request: curl -s -S -i --cacert docker/ssl/certs/ca.crt -X POST -H \"Authorization: Thing <thing_key>\" https://localhost/http/channels/<channel_id>/messages -d '[{\"bn\":\"some-base-name:\",\"bt\":1.276020076001e+09, \"bu\":\"A\",\"bver\":5, \"n\":\"voltage\",\"u\":\"V\",\"v\":120.1}, {\"n\":\"current\",\"t\":-5,\"v\":1.2}, {\"n\":\"current\",\"t\":-4,\"v\":1.3}]' Note that if you're going to use senml message format, you should always send messages as an array. For more information about the HTTP messaging service API, please check out the API documentation . MQTT # To send and receive messages over MQTT you could use Mosquitto tools , or Paho if you want to use MQTT over WebSocket. To publish message over channel, thing should call following command: mosquitto_pub -u <thing_id> -P <thing_key> -t channels/<channel_id>/messages -h localhost -m '[{\"bn\":\"some-base-name:\",\"bt\":1.276020076001e+09, \"bu\":\"A\",\"bver\":5, \"n\":\"voltage\",\"u\":\"V\",\"v\":120.1}, {\"n\":\"current\",\"t\":-5,\"v\":1.2}, {\"n\":\"current\",\"t\":-4,\"v\":1.3}]' To subscribe to channel, thing should call following command: mosquitto_sub -u <thing_id> -P <thing_key> -t channels/<channel_id>/messages -h localhost If you want to use standard topic such as channels/<channel_id>/messages with SenML content type (JSON or CBOR), you should use following topic channels/<channel_id>/messages . If you are using TLS to secure MQTT connection, add --cafile docker/ssl/certs/ca.crt to every command. CoAP # CoAP adapter implements CoAP protocol using underlying UDP and according to RFC 7252 . To send and receive messages over CoAP, you can use CoAP CLI . To set the add-on, please follow the installation instructions provided here . # Examples: coap-cli get channels/0bb5ba61-a66e-4972-bab6-26f19962678f/messages/subtopic -auth 1e1017e6-dee7-45b4-8a13-00e6afeb66eb -o coap-cli post channels/0bb5ba61-a66e-4972-bab6-26f19962678f/messages/subtopic -auth 1e1017e6-dee7-45b4-8a13-00e6afeb66eb -d \"hello world\" coap-cli post channels/0bb5ba61-a66e-4972-bab6-26f19962678f/messages/subtopic -auth 1e1017e6-dee7-45b4-8a13-00e6afeb66eb -d \"hello world\" -h 0.0.0.0 -p 1234 To send a message, use POST request. To subscribe, send GET request with Observe option (flag o ) set to false. There are two ways to unsubscribe: 1) Send GET request with Observe option set to true. 2) Forget the token and send RST message as a response to CONF message received by the server. The most of the notifications received from the Adapter are non-confirmable. By RFC 7641 : Server must send a notification in a confirmable message instead of a non-confirmable message at least every 24 hours. This prevents a client that went away or is no longer interested from remaining in the list of observers indefinitely. CoAP Adapter sends these notifications every 12 hours. To configure this period, please check adapter documentation If the client is no longer interested in receiving notifications, the second scenario described above can be used to unsubscribe. WS # Mainflux supports MQTT-over-WS , rather than pure WS protocol. this bring numerous benefits for IoT applications that are derived from the properties of MQTT - like QoS and PUB/SUB features. There are 2 reccomended Javascript libraries for implementing browser support for Mainflux MQTT-over-WS connectivity: Eclipse Paho JavaScript Client MQTT.js As WS is an extension of HTTP protocol, Mainflux exposes it on port 80 , so it's usage is practically transparent. Additionally, please notice that since same port as for HTTP is used ( 80 ), and extension URL /mqtt should be used - i.e. connection URL should be ws://<host_addr>/mqtt . For quick testing you can use HiveMQ UI tool . Here is an example of a browser application connecting to Mainflux server and sending and receiving messages over WebSocket using MQTT.js library: <script src=\"https://unpkg.com/mqtt/dist/mqtt.min.js\"></script> <script> // Initialize a mqtt variable globally console.log(mqtt) // connection option const options = { clean: true, // retain session connectTimeout: 4000, // Timeout period // Authentication information clientId: '14d6c682-fb5a-4d28-b670-ee565ab5866c', username: '14d6c682-fb5a-4d28-b670-ee565ab5866c', password: 'ec82f341-d4b5-4c77-ae05-34877a62428f', } var channelId = '08676a76-101d-439c-b62e-d4bb3b014337' var topic = 'channels/' + channelId + '/messages' // Connect string, and specify the connection method by the protocol // ws Unencrypted WebSocket connection // wss Encrypted WebSocket connection const connectUrl = 'ws://localhost/mqtt' const client = mqtt.connect(connectUrl, options) client.on('reconnect', (error) => { console.log('reconnecting:', error) }) client.on('error', (error) => { console.log('Connection failed:', error) }) client.on('connect', function () { console.log('client connected:' + options.clientId) client.subscribe(topic, { qos: 0 }) client.publish(topic, 'WS connection demo!', { qos: 0, retain: false }) }) client.on('message', function (topic, message, packet) { console.log('Received Message:= ' + message.toString() + '\\nOn topic:= ' + topic) }) client.on('close', function () { console.log(options.clientId + ' disconnected') }) </script> N.B. Eclipse Paho lib adds sub-URL /mqtt automaticlly, so procedure for connecting to the server can be something like this: var loc = { hostname: 'localhost', port: 80 } // Create a client instance client = new Paho.MQTT.Client(loc.hostname, Number(loc.port), \"clientId\") // Connect the client client.connect({onSuccess:onConnect}); Subtopics # In order to use subtopics and give more meaning to your pub/sub channel, you can simply add any suffix to base /channels/<channel_id>/messages topic. Example subtopic publish/subscribe for bedroom temperature would be channels/<channel_id>/messages/bedroom/temperature . Subtopics are generic and multilevel. You can use almost any suffix with any depth. Topics with subtopics are propagated to NATS broker in the following format channels.<channel_id>.<optional_subtopic> . Our example topic channels/<channel_id>/messages/bedroom/temperature will be translated to appropriate NATS topic channels.<channel_id>.bedroom.temperature . You can use multilevel subtopics, that have multiple parts. These parts are separated by . or / separators. When you use combination of these two, have in mind that behind the scene, / separator will be replaced with . . Every empty part of subtopic will be removed. What this means is that subtopic a///b is equivalent to a/b . When you want to subscribe, you can use NATS wildcards * and > . Every subtopic part can have * or > as it's value, but if there is any other character beside these wildcards, subtopic will be invalid. What this means is that subtopics such as a.b*c.d will be invalid, while a.b.*.c.d will be valid. Authorization is done on channel level, so you only have to have access to channel in order to have access to it's subtopics. Note: When using MQTT, it's recommended that you use standard MQTT wildcards + and # . For more information and examples checkout official nats.io documentation","title":"Messaging"},{"location":"messaging/#messaging","text":"Once a channel is provisioned and thing is connected to it, it can start to publish messages on the channel. The following sections will provide an example of message publishing for each of the supported protocols.","title":"Messaging"},{"location":"messaging/#http","text":"To publish message over channel, thing should send following request: curl -s -S -i --cacert docker/ssl/certs/ca.crt -X POST -H \"Authorization: Thing <thing_key>\" https://localhost/http/channels/<channel_id>/messages -d '[{\"bn\":\"some-base-name:\",\"bt\":1.276020076001e+09, \"bu\":\"A\",\"bver\":5, \"n\":\"voltage\",\"u\":\"V\",\"v\":120.1}, {\"n\":\"current\",\"t\":-5,\"v\":1.2}, {\"n\":\"current\",\"t\":-4,\"v\":1.3}]' Note that if you're going to use senml message format, you should always send messages as an array. For more information about the HTTP messaging service API, please check out the API documentation .","title":"HTTP"},{"location":"messaging/#mqtt","text":"To send and receive messages over MQTT you could use Mosquitto tools , or Paho if you want to use MQTT over WebSocket. To publish message over channel, thing should call following command: mosquitto_pub -u <thing_id> -P <thing_key> -t channels/<channel_id>/messages -h localhost -m '[{\"bn\":\"some-base-name:\",\"bt\":1.276020076001e+09, \"bu\":\"A\",\"bver\":5, \"n\":\"voltage\",\"u\":\"V\",\"v\":120.1}, {\"n\":\"current\",\"t\":-5,\"v\":1.2}, {\"n\":\"current\",\"t\":-4,\"v\":1.3}]' To subscribe to channel, thing should call following command: mosquitto_sub -u <thing_id> -P <thing_key> -t channels/<channel_id>/messages -h localhost If you want to use standard topic such as channels/<channel_id>/messages with SenML content type (JSON or CBOR), you should use following topic channels/<channel_id>/messages . If you are using TLS to secure MQTT connection, add --cafile docker/ssl/certs/ca.crt to every command.","title":"MQTT"},{"location":"messaging/#coap","text":"CoAP adapter implements CoAP protocol using underlying UDP and according to RFC 7252 . To send and receive messages over CoAP, you can use CoAP CLI . To set the add-on, please follow the installation instructions provided here .","title":"CoAP"},{"location":"messaging/#_1","text":"Examples: coap-cli get channels/0bb5ba61-a66e-4972-bab6-26f19962678f/messages/subtopic -auth 1e1017e6-dee7-45b4-8a13-00e6afeb66eb -o coap-cli post channels/0bb5ba61-a66e-4972-bab6-26f19962678f/messages/subtopic -auth 1e1017e6-dee7-45b4-8a13-00e6afeb66eb -d \"hello world\" coap-cli post channels/0bb5ba61-a66e-4972-bab6-26f19962678f/messages/subtopic -auth 1e1017e6-dee7-45b4-8a13-00e6afeb66eb -d \"hello world\" -h 0.0.0.0 -p 1234 To send a message, use POST request. To subscribe, send GET request with Observe option (flag o ) set to false. There are two ways to unsubscribe: 1) Send GET request with Observe option set to true. 2) Forget the token and send RST message as a response to CONF message received by the server. The most of the notifications received from the Adapter are non-confirmable. By RFC 7641 : Server must send a notification in a confirmable message instead of a non-confirmable message at least every 24 hours. This prevents a client that went away or is no longer interested from remaining in the list of observers indefinitely. CoAP Adapter sends these notifications every 12 hours. To configure this period, please check adapter documentation If the client is no longer interested in receiving notifications, the second scenario described above can be used to unsubscribe.","title":""},{"location":"messaging/#ws","text":"Mainflux supports MQTT-over-WS , rather than pure WS protocol. this bring numerous benefits for IoT applications that are derived from the properties of MQTT - like QoS and PUB/SUB features. There are 2 reccomended Javascript libraries for implementing browser support for Mainflux MQTT-over-WS connectivity: Eclipse Paho JavaScript Client MQTT.js As WS is an extension of HTTP protocol, Mainflux exposes it on port 80 , so it's usage is practically transparent. Additionally, please notice that since same port as for HTTP is used ( 80 ), and extension URL /mqtt should be used - i.e. connection URL should be ws://<host_addr>/mqtt . For quick testing you can use HiveMQ UI tool . Here is an example of a browser application connecting to Mainflux server and sending and receiving messages over WebSocket using MQTT.js library: <script src=\"https://unpkg.com/mqtt/dist/mqtt.min.js\"></script> <script> // Initialize a mqtt variable globally console.log(mqtt) // connection option const options = { clean: true, // retain session connectTimeout: 4000, // Timeout period // Authentication information clientId: '14d6c682-fb5a-4d28-b670-ee565ab5866c', username: '14d6c682-fb5a-4d28-b670-ee565ab5866c', password: 'ec82f341-d4b5-4c77-ae05-34877a62428f', } var channelId = '08676a76-101d-439c-b62e-d4bb3b014337' var topic = 'channels/' + channelId + '/messages' // Connect string, and specify the connection method by the protocol // ws Unencrypted WebSocket connection // wss Encrypted WebSocket connection const connectUrl = 'ws://localhost/mqtt' const client = mqtt.connect(connectUrl, options) client.on('reconnect', (error) => { console.log('reconnecting:', error) }) client.on('error', (error) => { console.log('Connection failed:', error) }) client.on('connect', function () { console.log('client connected:' + options.clientId) client.subscribe(topic, { qos: 0 }) client.publish(topic, 'WS connection demo!', { qos: 0, retain: false }) }) client.on('message', function (topic, message, packet) { console.log('Received Message:= ' + message.toString() + '\\nOn topic:= ' + topic) }) client.on('close', function () { console.log(options.clientId + ' disconnected') }) </script> N.B. Eclipse Paho lib adds sub-URL /mqtt automaticlly, so procedure for connecting to the server can be something like this: var loc = { hostname: 'localhost', port: 80 } // Create a client instance client = new Paho.MQTT.Client(loc.hostname, Number(loc.port), \"clientId\") // Connect the client client.connect({onSuccess:onConnect});","title":"WS"},{"location":"messaging/#subtopics","text":"In order to use subtopics and give more meaning to your pub/sub channel, you can simply add any suffix to base /channels/<channel_id>/messages topic. Example subtopic publish/subscribe for bedroom temperature would be channels/<channel_id>/messages/bedroom/temperature . Subtopics are generic and multilevel. You can use almost any suffix with any depth. Topics with subtopics are propagated to NATS broker in the following format channels.<channel_id>.<optional_subtopic> . Our example topic channels/<channel_id>/messages/bedroom/temperature will be translated to appropriate NATS topic channels.<channel_id>.bedroom.temperature . You can use multilevel subtopics, that have multiple parts. These parts are separated by . or / separators. When you use combination of these two, have in mind that behind the scene, / separator will be replaced with . . Every empty part of subtopic will be removed. What this means is that subtopic a///b is equivalent to a/b . When you want to subscribe, you can use NATS wildcards * and > . Every subtopic part can have * or > as it's value, but if there is any other character beside these wildcards, subtopic will be invalid. What this means is that subtopics such as a.b*c.d will be invalid, while a.b.*.c.d will be valid. Authorization is done on channel level, so you only have to have access to channel in order to have access to it's subtopics. Note: When using MQTT, it's recommended that you use standard MQTT wildcards + and # . For more information and examples checkout official nats.io documentation","title":"Subtopics"},{"location":"opcua/","text":"OPC-UA # Bridging with an OPC-UA Server can be done over the opcua-adapter . This service sits between Mainflux and an OPC-UA Server and just forwards the messages from one system to another. Run OPC-UA Server # The OPC-UA Server is used for connectivity layer. It allows various methods to read information from the OPC-UA server and its nodes. The current version of the opcua-adapter still experimental and only Browse and Subscribe methods are implemented. Public OPC-UA test servers are available for testing of OPC-UA clients and can be used for development and test purposes. Mainflux OPC-UA Adapter # Execute the following command from Mainflux project root to run the opcua-adapter: docker-compose -f docker/addons/opcua-adapter/docker-compose.yml up -d Route Map # The opcua-adapter use Redis database to create a route-map between Mainflux and an OPC-UA Server. As Mainflux use Things and Channels IDs to sign messages, OPC-UA use node ID (node namespace and node identifier combination) and server URI. The adapter route-map associate a Thing ID with a Node ID and a Channel ID with a Server URI . The opcua-adapter uses the matadata of provision events emitted by Mainflux system to update its route map. For that, you must provision Mainflux Channels and Things with an extra metadata key in the JSON Body of the HTTP request. It must be a JSON object with key opcua which value is another JSON object. This nested JSON object should contain node_id or server_uri that correspond to an existent OPC-UA Node ID or Server URI : Channel structure: { \"name\": \"<channel name>\", \"metadata:\": { \"opcua\": { \"server_uri\": \"<Server URI>\" } } } Thing structure: { \"name\": \"<thing name>\", \"metadata:\": { \"opcua\": { \"node_id\": \"<Node ID>\", } } } Browse # The opcua-adapter exposes a /browse HTTP endpoint accessible with method GET and configurable throw HTTP query parameters server , namespace and identifier . The server URI, the node namespace and the node identifier represent the parent node and are used to fetch the list of available children nodes starting from the given one. By default the root node ID (node namespace and node identifier combination) of an OPC-UA server is ns=0;i=84 . It's also the default value used by the opcua-adapter to do the browsing if only the server URI is specified in the HTTP query. Subscribe # To create an OPC-UA subscription, user should connect the Thing to the Channel. This will automatically create the connection, enable the redis route-map and run a subscription to the server_uri and node_id defined in the Thing and Channel metadata. Messaging # To forward OPC-UA messages the opcua-adapter subscribes to the Node ID of an OPC-UA Server URI. It verifies the server_uri and the node_id of received messages. If the mapping exists it uses corresponding Channel ID and Thing ID to sign and forwards the content of the OPC-UA message to the Mainflux message broker. If the mapping or the connection between the Thing and the Channel don't exist the subscription stops.","title":"OPC-UA"},{"location":"opcua/#opc-ua","text":"Bridging with an OPC-UA Server can be done over the opcua-adapter . This service sits between Mainflux and an OPC-UA Server and just forwards the messages from one system to another.","title":"OPC-UA"},{"location":"opcua/#run-opc-ua-server","text":"The OPC-UA Server is used for connectivity layer. It allows various methods to read information from the OPC-UA server and its nodes. The current version of the opcua-adapter still experimental and only Browse and Subscribe methods are implemented. Public OPC-UA test servers are available for testing of OPC-UA clients and can be used for development and test purposes.","title":"Run OPC-UA Server"},{"location":"opcua/#mainflux-opc-ua-adapter","text":"Execute the following command from Mainflux project root to run the opcua-adapter: docker-compose -f docker/addons/opcua-adapter/docker-compose.yml up -d","title":"Mainflux OPC-UA Adapter"},{"location":"opcua/#route-map","text":"The opcua-adapter use Redis database to create a route-map between Mainflux and an OPC-UA Server. As Mainflux use Things and Channels IDs to sign messages, OPC-UA use node ID (node namespace and node identifier combination) and server URI. The adapter route-map associate a Thing ID with a Node ID and a Channel ID with a Server URI . The opcua-adapter uses the matadata of provision events emitted by Mainflux system to update its route map. For that, you must provision Mainflux Channels and Things with an extra metadata key in the JSON Body of the HTTP request. It must be a JSON object with key opcua which value is another JSON object. This nested JSON object should contain node_id or server_uri that correspond to an existent OPC-UA Node ID or Server URI : Channel structure: { \"name\": \"<channel name>\", \"metadata:\": { \"opcua\": { \"server_uri\": \"<Server URI>\" } } } Thing structure: { \"name\": \"<thing name>\", \"metadata:\": { \"opcua\": { \"node_id\": \"<Node ID>\", } } }","title":"Route Map"},{"location":"opcua/#browse","text":"The opcua-adapter exposes a /browse HTTP endpoint accessible with method GET and configurable throw HTTP query parameters server , namespace and identifier . The server URI, the node namespace and the node identifier represent the parent node and are used to fetch the list of available children nodes starting from the given one. By default the root node ID (node namespace and node identifier combination) of an OPC-UA server is ns=0;i=84 . It's also the default value used by the opcua-adapter to do the browsing if only the server URI is specified in the HTTP query.","title":"Browse"},{"location":"opcua/#subscribe","text":"To create an OPC-UA subscription, user should connect the Thing to the Channel. This will automatically create the connection, enable the redis route-map and run a subscription to the server_uri and node_id defined in the Thing and Channel metadata.","title":"Subscribe"},{"location":"opcua/#messaging","text":"To forward OPC-UA messages the opcua-adapter subscribes to the Node ID of an OPC-UA Server URI. It verifies the server_uri and the node_id of received messages. If the mapping exists it uses corresponding Channel ID and Thing ID to sign and forwards the content of the OPC-UA message to the Mainflux message broker. If the mapping or the connection between the Thing and the Channel don't exist the subscription stops.","title":"Messaging"},{"location":"provision/","text":"Provision # Provisioning is a process of configuration of an IoT platform in which system operator creates and sets-up different entities used in the platform - users, channels and things. Platform management # Users Management # Account Creation # Use the Mainflux API to create user account: curl -s -S -i --cacert docker/ssl/certs/ca.crt -X POST -H \"Content-Type: application/json\" https://localhost/users -d '{\"email\":\"john.doe@email.com\", \"password\":\"12345678\"}' Note that when using official docker-compose , all services are behind nginx proxy and all traffic is TLS encrypted. Obtaining an Authorization Key # In order for this user to be able to authenticate to the system, you will have to create an authorization token for him: curl -s -S -i --cacert docker/ssl/certs/ca.crt -X POST -H \"Content-Type: application/json\" https://localhost/tokens -d '{\"email\":\"john.doe@email.com\", \"password\":\"12345678\"}' Response should look like this: { \"token\": \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJleHAiOjE1MjMzODg0NzcsImlhdCI6MTUyMzM1MjQ3NywiaXNzIjoibWFpbmZsdXgiLCJzdWIiOiJqb2huLmRvZUBlbWFpbC5jb20ifQ.cygz9zoqD7Rd8f88hpQNilTCAS1DrLLgLg4PRcH-iAI\" } For more information about the Users service API, please check out the API documentation . System Provisioning # Before proceeding, make sure that you have created a new account and obtained an authorization key. Provisioning Things # This endpoint will be depreciated in 0.11.0. It will be replaced with the bulk endpoint currently found at /things/bulk. Things are created by executing request POST /things with a JSON payload. Note that you will also need user_token in order to create things that belong to this particular user. curl -s -S -i --cacert docker/ssl/certs/ca.crt -X POST -H \"Content-Type: application/json\" -H \"Authorization: Bearer <user_token>\" https://localhost/things -d '{\"name\":\"weio\"}' Response will contain Location header whose value represents path to newly created thing: HTTP/1.1 201 Created Content-Type: application/json Location: /things/81380742-7116-4f6f-9800-14fe464f6773 Date: Tue, 10 Apr 2018 10:02:59 GMT Content-Length: 0 Bulk Provisioning Things # Multiple things can be created by executing a POST /things/bulk request with a JSON payload. The payload should contain a JSON array of the things to be created. If there is an error any of the things, none of the things will be created. curl -s -S -i --cacert docker/ssl/certs/ca.crt -X POST -H \"Content-Type: application/json\" -H \"Authorization: Bearer <user_token>\" https://localhost/things/bulk -d '[{\"name\":\"weio\"},{\"name\":\"bob\"}]' The response's body will contain a list of the created things. HTTP/2 201 server: nginx/1.16.0 date: Tue, 22 Oct 2019 02:19:15 GMT content-type: application/json content-length: 222 access-control-expose-headers: Location {\"things\":[{\"id\":\"8909adbf-312f-41eb-8cfc-ccc8c4e3655e\",\"name\":\"weio\",\"key\":\"4ef103cc-964a-41b5-b75b-b7415c3a3619\"},{\"id\":\"2fcd2349-38f7-4b5c-8a29-9607b2ca8ff5\",\"name\":\"bob\",\"key\":\"ff0d1490-355c-4dcf-b322-a4c536c8c3bf\"}]} Retrieving Provisioned Things # In order to retrieve data of provisioned things that is written in database, you can send following request: curl -s -S -i --cacert docker/ssl/certs/ca.crt -H \"Authorization: Bearer <user_token>\" https://localhost/things Notice that you will receive only those things that were provisioned by user_token owner. HTTP/1.1 200 OK Content-Type: application/json Date: Tue, 10 Apr 2018 10:50:12 GMT Content-Length: 1105 { \"total\": 2, \"offset\": 0, \"limit\": 10, \"things\": [ { \"id\": \"81380742-7116-4f6f-9800-14fe464f6773\", \"name\": \"weio\", \"key\": \"7aa91f7a-cbea-4fed-b427-07e029577590\" }, { \"id\": \"cb63f852-2d48-44f0-a0cf-e450496c6c92\", \"name\": \"myapp\", \"key\": \"cbf02d60-72f2-4180-9f82-2c957db929d1\" } ] } You can specify offset and limit parameters in order to fetch a specific group of things. In that case, your request should look like: curl -s -S -i --cacert docker/ssl/certs/ca.crt -H \"Authorization: Bearer <user_token>\" https://localhost/things?offset=0&limit=5 You can specify name and/or metadata parameters in order to fetch specific group of things. When specifying metadata you can specify just a part of the metadata JSON you want to match. curl -s -S -i --cacert docker/ssl/certs/ca.crt -H \"Authorization: Bearer <user_token>\" https://localhost/things?offset=0&limit=5&metadata=\"\\{\\\"serial\\\":\\\"123456\\\"\\}\" If you don't provide them, default values will be used instead: 0 for offset and 10 for limit . Note that limit cannot be set to values greater than 100. Providing invalid values will be considered malformed request. Searching Provisioned Things # In order to search things with specific name and/or metadata, you can send following request: curl -s -S -i --cacert docker/ssl/certs/ca.crt -X POST -H \"Content-Type: application/json\" -H \"Authorization: Bearer <user_token>\" https://localhost/things/search -d '{\"metadata\":{\"foo\":\"bar\"}, \"name\":\"bob\", \"limit\": 10, \"offset\":0, \"order\":\"name\", \"dir\":\"desc\"}' You can specify offset and limit parameters in order to fetch a specific set of things. Also, you can specify ordering with direction through parameters order and dir . Ordering values can be name or id of things, order direction can be asc or desc . If you don't provide them, default values will be used instead: 0 for offset and 10 for limit . Note that limit cannot be set to values greater than 100. Providing invalid values will be considered malformed request. The response's body will contain a list of the things filtered by name and/or metadata: HTTP/2 200 server: nginx/1.16.0 date: Mon, 15 Mar 2021 18:34:10 GMT content-type: application/json content-length: 208 access-control-expose-headers: Location { \"total\": 1, \"offset\": 0, \"limit\": 10, \"order\": \"name\", \"direction\": \"desc\", \"things\": [ { \"id\": \"1b86eea5-94b6-41fa-be9f-d10c85a8994d\", \"name\": \"bob\", \"key\": \"d72de10f-4963-4bf1-a454-874a39bb498e\", \"metadata\": { \"foo\": \"bar\" } } ] } Removing Things # In order to remove you own thing you can send following request: curl -s -S -i --cacert docker/ssl/certs/ca.crt -X DELETE -H \"Authorization: Bearer <user_token>\" https://localhost/things/<thing_id> Provisioning Channels # This endpoint will be depreciated in 0.11.0. It will be replaced with the bulk endpoint currently found at /channels/bulk. Channels are created by executing request POST /channels : curl -s -S -i --cacert docker/ssl/certs/ca.crt -X POST -H \"Content-Type: application/json\" -H \"Authorization: Bearer <user_token>\" https://localhost/channels -d '{\"name\":\"mychan\"}' After sending request you should receive response with Location header that contains path to newly created channel: HTTP/1.1 201 Created Content-Type: application/json Location: /channels/19daa7a8-a489-4571-8714-ef1a214ed914 Date: Tue, 10 Apr 2018 11:30:07 GMT Content-Length: 0 Bulk Provisioning Channels # Multiple channels can be created by executing a POST /things/bulk request with a JSON payload. The payload should contain a JSON array of the channels to be created. If there is an error any of the channels, none of the channels will be created. curl -s -S -i --cacert docker/ssl/certs/ca.crt -X POST -H \"Content-Type: application/json\" -H \"Authorization: Bearer <user_token>\" https://localhost/channels/bulk -d '[{\"name\":\"joe\"},{\"name\":\"betty\"}]' The response's body will contain a list of the created channels. HTTP/2 201 server: nginx/1.16.0 date: Tue, 22 Oct 2019 02:14:41 GMT content-type: application/json content-length: 135 access-control-expose-headers: Location {\"channels\":[{\"id\":\"5a21bbcb-4c9a-4bb4-af31-9982d00f7a6e\",\"name\":\"joe\"},{\"id\":\"d74b119b-2eea-4285-a999-9f747869bb45\",\"name\":\"betty\"}]} Retrieving Provisioned Channels # To retreve provisioned channels you should send request to /channels with authorization token in Authorization header: curl -s -S -i --cacert docker/ssl/certs/ca.crt -H \"Authorization: Bearer <user_token>\" https://localhost/channels Note that you will receive only those channels that were created by authorization token's owner. HTTP/1.1 200 OK Content-Type: application/json Date: Tue, 10 Apr 2018 11:38:06 GMT Content-Length: 139 { \"total\": 1, \"offset\": 0, \"limit\": 10, \"channels\": [ { \"id\": \"19daa7a8-a489-4571-8714-ef1a214ed914\", \"name\": \"mychan\" } ] } You can specify offset and limit parameters in order to fetch specific group of channels. In that case, your request should look like: curl -s -S -i --cacert docker/ssl/certs/ca.crt -H \"Authorization: Bearer <user_token>\" https://localhost/channels?offset=0&limit=5 If you don't provide them, default values will be used instead: 0 for offset and 10 for limit . Note that limit cannot be set to values greater than 100. Providing invalid values will be considered malformed request. Removing Channels # In order to remove specific channel you should send following request: curl -s -S -i --cacert docker/ssl/certs/ca.crt -X DELETE -H \"Authorization: Bearer <user_token>\" https://localhost/channels/<channel_id> Access Control # Channel can be observed as a communication group of things. Only things that are connected to the channel can send and receive messages from other things in this channel. Things that are not connected to this channel are not allowed to communicate over it. Only user, who is the owner of a channel and of the things, can connect the things to the channel (which is equivalent of giving permissions to these things to communicate over given communication group). To connect a thing to the channel you should send following request: This endpoint will be depreciated in 0.11.0. It will be replaced with the bulk endpoint found at /connect. curl -s -S -i --cacert docker/ssl/certs/ca.crt -X PUT -H \"Authorization: Bearer <user_token>\" https://localhost/channels/<channel_id>/things/<thing_id> To connect multiple things to a channel, you can send the following request: curl -s -S -i --cacert docker/ssl/certs/ca.crt -X POST -H \"Content-Type: application/json\" -H \"Authorization: Bearer <user_token>\" https://localhost/connect -d '{\"channel_ids\":[\"<channel_id>\", \"<channel_id>\"],\"thing_ids\":[\"<thing_id>\", \"<thing_id>\"]}' You can observe which things are connected to specific channel: curl -s -S -i --cacert docker/ssl/certs/ca.crt -H \"Authorization: Bearer <user_token>\" https://localhost/channels/<channel_id>/things You can also observe which things are not connected to specific channel by adding a query parameter connected=false to the HTTP request: curl -s -S -i --cacert docker/ssl/certs/ca.crt -H \"Authorization: Bearer <user_token>\" https://localhost/channels/<channel_id>/things?connected=false Response that you'll get should look like this: { \"total\": 2, \"offset\": 0, \"limit\": 10, \"things\": [ { \"id\": \"3ffb3880-d1e6-4edd-acd9-4294d013f35b\", \"name\": \"d0\", \"key\": \"b1996995-237a-4552-94b2-83ec2e92a040\", \"metadata\": \"{}\" }, { \"id\": \"94d166d6-6477-43dc-93b7-5c3707dbef1e\", \"name\": \"d1\", \"key\": \"e4588a68-6028-4740-9f12-c356796aebe8\", \"metadata\": \"{}\" } ] } You can observe to which channels is specified thing connected: curl -s -S -i --cacert docker/ssl/certs/ca.crt -H \"Authorization: Bearer <user_token>\" https://localhost/things/<thing_id>/channels You can also observe to which channels is specified thing not connected by adding a query parameter connected=false to the HTTP request: curl -s -S -i --cacert docker/ssl/certs/ca.crt -H \"Authorization: Bearer <user_token>\" https://localhost/things/<thing_id>/channels?connected=false Response that you'll get should look like this: { \"total\": 2, \"offset\": 0, \"limit\": 10, \"channels\": [ { \"id\": \"5e62eb13-2695-4860-8d87-85b8a2f80fd4\", \"name\": \"c1\", \"metadata\": \"{}\" }, { \"id\": \"c4b5e19a-7ffe-4172-b2c5-c8b9d570a165\", \"name\": \"c0\", \"metadata\":\"{}\" } ] } If you want to disconnect your thing from the channel, send following request: curl -s -S -i --cacert docker/ssl/certs/ca.crt -X DELETE -H \"Authorization: Bearer <user_token>\" https://localhost/channels/<channel_id>/things/<thing_id> For more information about the Things service API, please check out the API documentation . Provision Service # Provisioning is a process of configuration of an IoT platform in which system operator creates and sets-up different entities used in the platform - users, channels and things. It is part of process of setting up IoT applications where we connect devices on edge with platform in cloud. For provisioning we can use Mainflux CLI for creating users and for each node in the edge (eg. gateway) required number of things, channels, connecting them and creating certificates if needed. Provision service is used to set up initial application configuration once user is created. Provision service creates things, channels, connections and certificates. Once user is created we can use provision to create a setup for edge node in one HTTP request instead of issuing several CLI commands. Provision service provides an HTTP API to interact with Mainflux . For gateways to communicate with Mainflux configuration is required (MQTT host, thing, channels, certificates...). Gateway will send a request to Bootstrap service providing <external_id> and <external_key> in HTTP request to get the configuration. To make a request to Bootstrap service you can use Agent service on a gateway. To create bootstrap configuration you can use Bootstrap or Provision service. Mainflux UI uses Bootstrap service for creating gateway configurations. Provision service should provide an easy way of provisioning your gateways i.e creating bootstrap configuration and as many things and channels that your setup requires. Also, you may use provision service to create certificates for each thing. Each service running on gateway may require more than one thing and channel for communication. If, for example, you are using services Agent and Export on a gateway you will need two channels for Agent ( data and control ) and one thing for Export . Additionally, if you enabled mTLS each service will need its own thing and certificate for access to Mainflux . Your setup could require any number of things and channels, this kind of setup we can call provision layout . Provision service provides a way of specifying this provision layout and creating a setup according to that layout by serving requests on /mapping endpoint. Provision layout is configured in config.toml . Configuration # The service is configured using the environment variables presented in the following table . Note that any unset variables will be replaced with their default values. By default, call to /mapping endpoint will create one thing and two channels ( control and data ) and connect it as this is typical setup required by Agent . If there is a requirement for different provision layout we can use config file in addition to environment variables. For the purposes of running provision as an add-on in docker composition environment variables seems more suitable. Environment variables are set in .env . Configuration can be specified in config.toml . Config file can specify all the settings that environment variables can configure and in addition /mapping endpoint provision layout can be configured. In config.toml we can enlist an array of things and channels that we want to create and make connections between them which we call provision layout. Things Metadata can be whatever suits your needs. Thing that has metadata with external_id will have bootstrap configuration created, external_id value will be populated with value from request ). Bootstrap configuration can be fetched with Agent . For channel's metadata type is reserved for control and data which we use with Agent . Example of provision layout below [bootstrap] [bootstrap.content] [bootstrap.content.agent.edgex] url = \"http://localhost:48090/api/v1/\" [bootstrap.content.agent.log] level = \"info\" [bootstrap.content.agent.mqtt] mtls = false qos = 0 retain = false skip_tls_ver = true url = \"localhost:1883\" [bootstrap.content.agent.server] nats_url = \"localhost:4222\" port = \"9000\" [bootstrap.content.agent.heartbeat] interval = \"30s\" [bootstrap.content.agent.terminal] session_timeout = \"30s\" [bootstrap.content.export.exp] log_level = \"debug\" nats = \"nats://localhost:4222\" port = \"8172\" cache_url = \"localhost:6379\" cache_pass = \"\" cache_db = \"0\" [bootstrap.content.export.mqtt] ca_path = \"ca.crt\" cert_path = \"thing.crt\" channel = \"\" host = \"tcp://localhost:1883\" mtls = false password = \"\" priv_key_path = \"thing.key\" qos = 0 retain = false skip_tls_ver = false username = \"\" [[bootstrap.content.export.routes]] mqtt_topic = \"\" nats_topic = \"channels\" subtopic = \"\" type = \"mfx\" workers = 10 [[bootstrap.content.export.routes]] mqtt_topic = \"\" nats_topic = \"export\" subtopic = \"\" type = \"default\" workers = 10 [[things]] name = \"thing\" [things.metadata] external_id = \"xxxxxx\" [[channels]] name = \"control-channel\" [channels.metadata] type = \"control\" [[channels]] name = \"data-channel\" [channels.metadata] type = \"data\" [[channels]] name = \"export-channel\" [channels.metadata] type = \"export\" [bootstrap.content] will be marshalled and saved into content field in bootstrap configs when request to /mappings is made, content field from bootstrap config is used to create Agent and Export configuration files upon Agent fetching bootstrap configuration. Authentication # In order to create necessary entities provision service needs to authenticate against Mainflux. To provide authentication credentials to the provision service you can pass it in as an environment variable or in a config file as Mainflux user and password or as API token (that can be issued on /users or /keys endpoint of auth . Additionally, users or API token can be passed in Authorization header, this authentication takes precedence over others. username , password - ( MF_PROVISION_USER , MF_PROVISION_PASSWORD in .env , mf_user , mf_pass in config.toml API Key - ( MF_PROVISION_API_KEY in .env or config.toml Authorization: Bearer Token|ApiKey - request authorization header containing either users token or API key. Check auth . Running # Provision service can be run as a standalone or in docker composition as addon to the core docker composition. Standalone: MF_PROVISION_BS_SVC_URL=http://localhost:8202/things \\ MF_PROVISION_THINGS_LOCATION=http://localhost:8182 \\ MF_PROVISION_USERS_LOCATION=http://localhost:8180 \\ MF_PROVISION_CONFIG_FILE=docker/addons/provision/configs/config.toml \\ build/mainflux-provision Docker composition: docker-compose -f docker/addons/provision/docker-compose.yml up Provision # For the case that credentials or API token is passed in configuration file or environment variables, call to /mapping endpoint doesn't require Authentication header: curl -s -S -X POST http://localhost:8888/mapping -H 'Content-Type: application/json' -d '{\"external_id\": \"33:52:77:99:43\", \"external_key\": \"223334fw2\"}' In the case that provision service is not deployed with credentials or API key or you want to use user other than one being set in environment (or config file): curl -s -S -X POST http://localhost:8091/mapping -H \"Authorization: Bearer <token|api_key>\" -H 'Content-Type: application/json' -d '{\"external_id\": \"<external_id>\", \"external_key\": \"<external_key>\"}' Or if you want to specify a name for thing different than in config.toml you can specify post data as: {\"name\": \"<name>\", \"external_id\": \"<external_id>\", \"external_key\": \"<external_key>\"} Response contains created things, channels and certificates if any: { \"things\": [ { \"id\": \"c22b0c0f-8c03-40da-a06b-37ed3a72c8d1\", \"name\": \"thing\", \"key\": \"007cce56-e0eb-40d6-b2b9-ed348a97d1eb\", \"metadata\": { \"external_id\": \"33:52:79:C3:43\" } } ], \"channels\": [ { \"id\": \"064c680e-181b-4b58-975e-6983313a5170\", \"name\": \"control-channel\", \"metadata\": { \"type\": \"control\" } }, { \"id\": \"579da92d-6078-4801-a18a-dd1cfa2aa44f\", \"name\": \"data-channel\", \"metadata\": { \"type\": \"data\" } } ], \"whitelisted\": { \"c22b0c0f-8c03-40da-a06b-37ed3a72c8d1\": true } } Example # Deploy Mainflux UI docker composition as it contains all the required services for provisioning to work ( certs , bootstrap and Mainflux core) git clone https://github.com/mainflux/ui cd ui docker-compose -f docker/docker-compose.yml up Create user and obtain access token mainflux-cli -m https://mainflux.com users create john.doe@email.com 12345678 # Retrieve token mainflux-cli -m https://mainflux.com users token john.doe@email.com 12345678 created: eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJleHAiOjE1OTY1ODU3MDUsImlhdCI6MTU5NjU0OTcwNSwiaXNzIjoibWFpbmZsdXguYXV0aG4iLCJzdWIiOiJtaXJrYXNoQGdtYWlsLmNvbSIsInR5cGUiOjB9._vq0zJzFc9tQqc8x74kpn7dXYefUtG9IB0Cb-X2KMK8 Put a value of token into environment variable TOKEN=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJleHAiOjE1OTY1ODU3MDUsImlhdCI6MTU5NjU0OTcwNSwiaXNzIjoibWFpbmZsdXguYXV0aG4iLCJzdWIiOiJtaXJrYXNoQGdtYWlsLmNvbSIsInR5cGUiOjB9._vq0zJzFc9tQqc8x74kpn7dXYefUtG9IB0Cb-X2KMK8 Make a call to provision endpoint curl -s -S -X POST http://mainflux.com:8190/mapping -H \"Authorization: Bearer $TOKEN\" -H 'Content-Type: application/json' -d '{\"name\":\"edge-gw\", \"external_id\" : \"gateway\", \"external_key\":\"external_key\" }' To check the results you can make a call to bootstrap endpoint curl -s -S -X GET http://mainflux.com:8202/things/bootstrap/gateway -H \"Authorization: Thing external_key\" -H 'Content-Type: application/json' Or you can start Agent with: git clone https://github.com/mainflux/agent cd agent make MF_AGENT_BOOTSTRAP_ID=gateway MF_AGENT_BOOTSTRAP_KEY=external_key MF_AGENT_BOOTSTRAP_URL=http://mainflux.ccom:8202/things/bootstrap build/mainflux-agent Agent will retrieve connections parameters and connect to Mainflux cloud. For more information about the Provision service API, please check out the API documentation . Certs Service # Issues certificates for things. Certs service can create certificates to be used when Mainflux is deployed to support mTLS. Certs service will create certificate for valid thing ID if valid user token is passed and user is owner of the provided thing ID. Certificate service can create certificates in two modes: 1. Development mode - to be used when no PKI is deployed, this works similar to the make thing_cert 2. PKI mode - certificates issued by PKI, when you deploy Vault as PKI certificate management cert service will proxy requests to Vault previously checking access rights and saving info on successfully created certificate. Development mode # If MF_CERTS_VAULT_HOST is empty than Development mode is on. To issue a certificate: TOKEN=`curl -s --insecure -S -X POST http://localhost/tokens -H 'Content-Type: application/json' -d '{\"email\":\"edge@email.com\",\"password\":\"12345678\"}' | jq -r '.token'` curl -s -S -X POST http://localhost:8204/certs -H \"Authorization: Bearer $TOKEN\" -H 'Content-Type: application/json' -d '{\"thing_id\":<thing_id>, \"rsa_bits\":2048, \"key_type\":\"rsa\"}' { \"ThingID\": \"\", \"ClientCert\": \"-----BEGIN CERTIFICATE-----\\nMIIDmTCCAoGgAwIBAgIRANmkAPbTR1UYeYO0Id/4+8gwDQYJKoZIhvcNAQELBQAw\\nVzESMBAGA1UEAwwJbG9jYWxob3N0MREwDwYDVQQKDAhNYWluZmx1eDEMMAoGA1UE\\nCwwDSW9UMSAwHgYJKoZIhvcNAQkBFhFpbmZvQG1haW5mbHV4LmNvbTAeFw0yMDA2\\nMzAxNDIxMDlaFw0yMDA5MjMyMjIxMDlaMFUxETAPBgNVBAoTCE1haW5mbHV4MREw\\nDwYDVQQLEwhtYWluZmx1eDEtMCsGA1UEAxMkYjAwZDBhNzktYjQ2YS00NTk3LTli\\nNGYtMjhkZGJhNTBjYTYyMIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEA\\ntgS2fLUWG3CCQz/l6VRQRJfRvWmdxK0mW6zIXGeeOILYZeaLiuiUnohwMJ4RiMqT\\nuJbInAIuO/Tt5osfrCFFzPEOLYJ5nZBBaJfTIAxqf84Ou1oeMRll4wpzgeKx0rJO\\nXMAARwn1bT9n3uky5QQGSLy4PyyILzSXH/1yCQQctdQB/Ar/UI1TaYoYlGzh7dHT\\nWpcxq1HYgCyAtcrQrGD0rEwUn82UBCrnya+bygNqu0oDzIFQwa1G8jxSgXk0mFS1\\nWrk7rBipsvp8HQhdnvbEVz4k4AAKcQxesH4DkRx/EXmU2UvN3XysvcJ2bL+UzMNI\\njNhAe0pgPbB82F6zkYZ/XQIDAQABo2IwYDAOBgNVHQ8BAf8EBAMCB4AwHQYDVR0l\\nBBYwFAYIKwYBBQUHAwIGCCsGAQUFBwMBMA4GA1UdDgQHBAUBAgMEBjAfBgNVHSME\\nGDAWgBRs4xR91qEjNRGmw391xS7x6Tc+8jANBgkqhkiG9w0BAQsFAAOCAQEAW/dS\\nV4vNLTZwBnPVHUX35pRFxPKvscY+vnnpgyDtITgZHYe0KL+Bs3IHuywtqaezU5x1\\nkZo+frE1OcpRvp7HJtDiT06yz+18qOYZMappCWCeAFWtZkMhlvnm3TqTkgui6Xgl\\nGj5xnPb15AOlsDE2dkv5S6kEwJGHdVX6AOWfB4ubUq5S9e4ABYzXGUty6Hw/ZUmJ\\nhCTRVJ7cQJVTJsl1o7CYT8JBvUUG75LirtoFE4M4JwsfsKZXzrQffTf1ynqI3dN/\\nHWySEbvTSWcRcA3MSmOTxGt5/zwCglHDlWPKMrXtjTW7NPuGL5/P9HSB9HGVVeET\\nDUMdvYwgj0cUCEu3LA==\\n-----END CERTIFICATE-----\\n\", \"IssuingCA\": \"\", \"CAChain\": null, \"ClientKey\": \"-----BEGIN RSA PRIVATE KEY-----\\nMIIEowIBAAKCAQEAtgS2fLUWG3CCQz/l6VRQRJfRvWmdxK0mW6zIXGeeOILYZeaL\\niuiUnohwMJ4RiMqTuJbInAIuO/Tt5osfrCFFzPEOLYJ5nZBBaJfTIAxqf84Ou1oe\\nMRll4wpzgeKx0rJOXMAARwn1bT9n3uky5QQGSLy4PyyILzSXH/1yCQQctdQB/Ar/\\nUI1TaYoYlGzh7dHTWpcxq1HYgCyAtcrQrGD0rEwUn82UBCrnya+bygNqu0oDzIFQ\\nwa1G8jxSgXk0mFS1Wrk7rBipsvp8HQhdnvbEVz4k4AAKcQxesH4DkRx/EXmU2UvN\\n3XysvcJ2bL+UzMNIjNhAe0pgPbB82F6zkYZ/XQIDAQABAoIBAALoal3tqq+/iWU3\\npR2oKiweXMxw3oNg3McEKKNJSH7QoFJob3xFoPIzbc9pBxCvY9LEHepYIpL0o8RW\\nHqhqU6olg7t4ZSb+Qf1Ax6+wYxctnJCjrO3N4RHSfevqSjr6fEQBEUARSal4JNmr\\n0hNUkCEjWrIvrPFMHsn1C5hXR3okJQpGsad4oCGZDp2eZ/NDyvmLBLci9/5CJdRv\\n6roOF5ShWweKcz1+pfy666Q8RiUI7H1zXjPaL4yqkv8eg/WPOO0dYF2Ri2Grk9OY\\n1qTM0W1vi9zfncinZ0DpgtwMTFQezGwhUyJHSYHmjVBA4AaYIyOQAI/2dl5fXM+O\\n9JfXpOUCgYEA10xAtMc/8KOLbHCprpc4pbtOqfchq/M04qPKxQNAjqvLodrWZZgF\\nexa+B3eWWn5MxmQMx18AjBCPwbNDK8Rkd9VqzdWempaSblgZ7y1a0rRNTXzN5DFP\\noiuRQV4wszCuj5XSdPn+lxApaI/4+TQ0oweIZCpGW39XKePPoB5WZiMCgYEA2G3W\\niJncRpmxWwrRPi1W26E9tWOT5s9wYgXWMc+PAVUd/qdDRuMBHpu861Qoghp/MJog\\nBYqt2rQqU0OxvIXlXPrXPHXrCLOFwybRCBVREZrg4BZNnjyDTLOu9C+0M3J9ImCh\\n3vniYqb7S0gRmoDM0R3Zu4+ajfP2QOGLXw1qHH8CgYEAl0EQ7HBW8V5UYzi7XNcM\\nixKOb0YZt83DR74+hC6GujTjeLBfkzw8DX+qvWA8lxLIKVC80YxivAQemryv4h21\\nX6Llx/nd1UkXUsI+ZhP9DK5y6I9XroseIRZuk/fyStFWsbVWB6xiOgq2rKkJBzqw\\nCCEQpx40E6/gsqNDiIAHvvUCgYBkkjXc6FJ55DWMLuyozfzMtpKsVYeG++InSrsM\\nDn1PizQS/7q9mAMPLCOP312rh5CPDy/OI3FCbfI1GwHerwG0QUP/bnQ3aOTBmKoN\\n7YnsemIA/5w16bzBycWE5x3/wjXv4aOWr9vJJ/siMm0rtKp4ijyBcevKBxHpeGWB\\nWAR1FQKBgGIqAxGnBpip9E24gH894BaGHHMpQCwAxARev6sHKUy27eFUd6ipoTva\\n4Wv36iz3gxU4R5B0gyfnxBNiUab/z90cb5+6+FYO13kqjxRRZWffohk5nHlmFN9K\\nea7KQHTfTdRhOLUzW2yVqLi9pzfTfA6Yqf3U1YD3bgnWrp1VQnjo\\n-----END RSA PRIVATE KEY-----\\n\", \"PrivateKeyType\": \"\", \"Serial\": \"\", \"Expire\": \"0001-01-01T00:00:00Z\" } PKI mode # When MF_CERTS_VAULT_HOST is set it is presumed that Vault is installed and certs service will issue certificates using Vault API. First you'll need to set up Vault . To setup Vault follow steps in Build Your Own Certificate Authority (CA) . To setup certs service with Vault following environment variables must be set: MF_CERTS_VAULT_HOST=vault-domain.com MF_CERTS_VAULT_PKI_PATH=<vault_pki_path> MF_CERTS_VAULT_ROLE=<vault_role> MF_CERTS_VAULT_TOKEN=<vault_acces_token> For lab purposes you can use docker-compose and script for setting up PKI in https://github.com/mteodor/vault Issuing certificate is same as in Development mode. In this mode certificates can also be revoked: curl -s -S -X DELETE http://localhost:8204/certs/revoke -H \"Authorization: Bearer $TOKEN\" -H 'Content-Type: application/json' -d '{\"thing_id\":\"c30b8842-507c-4bcd-973c-74008cef3be5\"}' For more information about the Certification service API, please check out the API documentation .","title":"Provision"},{"location":"provision/#provision","text":"Provisioning is a process of configuration of an IoT platform in which system operator creates and sets-up different entities used in the platform - users, channels and things.","title":"Provision"},{"location":"provision/#platform-management","text":"","title":"Platform management"},{"location":"provision/#users-management","text":"","title":"Users Management"},{"location":"provision/#account-creation","text":"Use the Mainflux API to create user account: curl -s -S -i --cacert docker/ssl/certs/ca.crt -X POST -H \"Content-Type: application/json\" https://localhost/users -d '{\"email\":\"john.doe@email.com\", \"password\":\"12345678\"}' Note that when using official docker-compose , all services are behind nginx proxy and all traffic is TLS encrypted.","title":"Account Creation"},{"location":"provision/#obtaining-an-authorization-key","text":"In order for this user to be able to authenticate to the system, you will have to create an authorization token for him: curl -s -S -i --cacert docker/ssl/certs/ca.crt -X POST -H \"Content-Type: application/json\" https://localhost/tokens -d '{\"email\":\"john.doe@email.com\", \"password\":\"12345678\"}' Response should look like this: { \"token\": \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJleHAiOjE1MjMzODg0NzcsImlhdCI6MTUyMzM1MjQ3NywiaXNzIjoibWFpbmZsdXgiLCJzdWIiOiJqb2huLmRvZUBlbWFpbC5jb20ifQ.cygz9zoqD7Rd8f88hpQNilTCAS1DrLLgLg4PRcH-iAI\" } For more information about the Users service API, please check out the API documentation .","title":"Obtaining an Authorization Key"},{"location":"provision/#system-provisioning","text":"Before proceeding, make sure that you have created a new account and obtained an authorization key.","title":"System Provisioning"},{"location":"provision/#provisioning-things","text":"This endpoint will be depreciated in 0.11.0. It will be replaced with the bulk endpoint currently found at /things/bulk. Things are created by executing request POST /things with a JSON payload. Note that you will also need user_token in order to create things that belong to this particular user. curl -s -S -i --cacert docker/ssl/certs/ca.crt -X POST -H \"Content-Type: application/json\" -H \"Authorization: Bearer <user_token>\" https://localhost/things -d '{\"name\":\"weio\"}' Response will contain Location header whose value represents path to newly created thing: HTTP/1.1 201 Created Content-Type: application/json Location: /things/81380742-7116-4f6f-9800-14fe464f6773 Date: Tue, 10 Apr 2018 10:02:59 GMT Content-Length: 0","title":"Provisioning Things"},{"location":"provision/#bulk-provisioning-things","text":"Multiple things can be created by executing a POST /things/bulk request with a JSON payload. The payload should contain a JSON array of the things to be created. If there is an error any of the things, none of the things will be created. curl -s -S -i --cacert docker/ssl/certs/ca.crt -X POST -H \"Content-Type: application/json\" -H \"Authorization: Bearer <user_token>\" https://localhost/things/bulk -d '[{\"name\":\"weio\"},{\"name\":\"bob\"}]' The response's body will contain a list of the created things. HTTP/2 201 server: nginx/1.16.0 date: Tue, 22 Oct 2019 02:19:15 GMT content-type: application/json content-length: 222 access-control-expose-headers: Location {\"things\":[{\"id\":\"8909adbf-312f-41eb-8cfc-ccc8c4e3655e\",\"name\":\"weio\",\"key\":\"4ef103cc-964a-41b5-b75b-b7415c3a3619\"},{\"id\":\"2fcd2349-38f7-4b5c-8a29-9607b2ca8ff5\",\"name\":\"bob\",\"key\":\"ff0d1490-355c-4dcf-b322-a4c536c8c3bf\"}]}","title":"Bulk Provisioning Things"},{"location":"provision/#retrieving-provisioned-things","text":"In order to retrieve data of provisioned things that is written in database, you can send following request: curl -s -S -i --cacert docker/ssl/certs/ca.crt -H \"Authorization: Bearer <user_token>\" https://localhost/things Notice that you will receive only those things that were provisioned by user_token owner. HTTP/1.1 200 OK Content-Type: application/json Date: Tue, 10 Apr 2018 10:50:12 GMT Content-Length: 1105 { \"total\": 2, \"offset\": 0, \"limit\": 10, \"things\": [ { \"id\": \"81380742-7116-4f6f-9800-14fe464f6773\", \"name\": \"weio\", \"key\": \"7aa91f7a-cbea-4fed-b427-07e029577590\" }, { \"id\": \"cb63f852-2d48-44f0-a0cf-e450496c6c92\", \"name\": \"myapp\", \"key\": \"cbf02d60-72f2-4180-9f82-2c957db929d1\" } ] } You can specify offset and limit parameters in order to fetch a specific group of things. In that case, your request should look like: curl -s -S -i --cacert docker/ssl/certs/ca.crt -H \"Authorization: Bearer <user_token>\" https://localhost/things?offset=0&limit=5 You can specify name and/or metadata parameters in order to fetch specific group of things. When specifying metadata you can specify just a part of the metadata JSON you want to match. curl -s -S -i --cacert docker/ssl/certs/ca.crt -H \"Authorization: Bearer <user_token>\" https://localhost/things?offset=0&limit=5&metadata=\"\\{\\\"serial\\\":\\\"123456\\\"\\}\" If you don't provide them, default values will be used instead: 0 for offset and 10 for limit . Note that limit cannot be set to values greater than 100. Providing invalid values will be considered malformed request.","title":"Retrieving Provisioned Things"},{"location":"provision/#searching-provisioned-things","text":"In order to search things with specific name and/or metadata, you can send following request: curl -s -S -i --cacert docker/ssl/certs/ca.crt -X POST -H \"Content-Type: application/json\" -H \"Authorization: Bearer <user_token>\" https://localhost/things/search -d '{\"metadata\":{\"foo\":\"bar\"}, \"name\":\"bob\", \"limit\": 10, \"offset\":0, \"order\":\"name\", \"dir\":\"desc\"}' You can specify offset and limit parameters in order to fetch a specific set of things. Also, you can specify ordering with direction through parameters order and dir . Ordering values can be name or id of things, order direction can be asc or desc . If you don't provide them, default values will be used instead: 0 for offset and 10 for limit . Note that limit cannot be set to values greater than 100. Providing invalid values will be considered malformed request. The response's body will contain a list of the things filtered by name and/or metadata: HTTP/2 200 server: nginx/1.16.0 date: Mon, 15 Mar 2021 18:34:10 GMT content-type: application/json content-length: 208 access-control-expose-headers: Location { \"total\": 1, \"offset\": 0, \"limit\": 10, \"order\": \"name\", \"direction\": \"desc\", \"things\": [ { \"id\": \"1b86eea5-94b6-41fa-be9f-d10c85a8994d\", \"name\": \"bob\", \"key\": \"d72de10f-4963-4bf1-a454-874a39bb498e\", \"metadata\": { \"foo\": \"bar\" } } ] }","title":"Searching Provisioned Things"},{"location":"provision/#removing-things","text":"In order to remove you own thing you can send following request: curl -s -S -i --cacert docker/ssl/certs/ca.crt -X DELETE -H \"Authorization: Bearer <user_token>\" https://localhost/things/<thing_id>","title":"Removing Things"},{"location":"provision/#provisioning-channels","text":"This endpoint will be depreciated in 0.11.0. It will be replaced with the bulk endpoint currently found at /channels/bulk. Channels are created by executing request POST /channels : curl -s -S -i --cacert docker/ssl/certs/ca.crt -X POST -H \"Content-Type: application/json\" -H \"Authorization: Bearer <user_token>\" https://localhost/channels -d '{\"name\":\"mychan\"}' After sending request you should receive response with Location header that contains path to newly created channel: HTTP/1.1 201 Created Content-Type: application/json Location: /channels/19daa7a8-a489-4571-8714-ef1a214ed914 Date: Tue, 10 Apr 2018 11:30:07 GMT Content-Length: 0","title":"Provisioning Channels"},{"location":"provision/#bulk-provisioning-channels","text":"Multiple channels can be created by executing a POST /things/bulk request with a JSON payload. The payload should contain a JSON array of the channels to be created. If there is an error any of the channels, none of the channels will be created. curl -s -S -i --cacert docker/ssl/certs/ca.crt -X POST -H \"Content-Type: application/json\" -H \"Authorization: Bearer <user_token>\" https://localhost/channels/bulk -d '[{\"name\":\"joe\"},{\"name\":\"betty\"}]' The response's body will contain a list of the created channels. HTTP/2 201 server: nginx/1.16.0 date: Tue, 22 Oct 2019 02:14:41 GMT content-type: application/json content-length: 135 access-control-expose-headers: Location {\"channels\":[{\"id\":\"5a21bbcb-4c9a-4bb4-af31-9982d00f7a6e\",\"name\":\"joe\"},{\"id\":\"d74b119b-2eea-4285-a999-9f747869bb45\",\"name\":\"betty\"}]}","title":"Bulk Provisioning Channels"},{"location":"provision/#retrieving-provisioned-channels","text":"To retreve provisioned channels you should send request to /channels with authorization token in Authorization header: curl -s -S -i --cacert docker/ssl/certs/ca.crt -H \"Authorization: Bearer <user_token>\" https://localhost/channels Note that you will receive only those channels that were created by authorization token's owner. HTTP/1.1 200 OK Content-Type: application/json Date: Tue, 10 Apr 2018 11:38:06 GMT Content-Length: 139 { \"total\": 1, \"offset\": 0, \"limit\": 10, \"channels\": [ { \"id\": \"19daa7a8-a489-4571-8714-ef1a214ed914\", \"name\": \"mychan\" } ] } You can specify offset and limit parameters in order to fetch specific group of channels. In that case, your request should look like: curl -s -S -i --cacert docker/ssl/certs/ca.crt -H \"Authorization: Bearer <user_token>\" https://localhost/channels?offset=0&limit=5 If you don't provide them, default values will be used instead: 0 for offset and 10 for limit . Note that limit cannot be set to values greater than 100. Providing invalid values will be considered malformed request.","title":"Retrieving Provisioned Channels"},{"location":"provision/#removing-channels","text":"In order to remove specific channel you should send following request: curl -s -S -i --cacert docker/ssl/certs/ca.crt -X DELETE -H \"Authorization: Bearer <user_token>\" https://localhost/channels/<channel_id>","title":"Removing Channels"},{"location":"provision/#access-control","text":"Channel can be observed as a communication group of things. Only things that are connected to the channel can send and receive messages from other things in this channel. Things that are not connected to this channel are not allowed to communicate over it. Only user, who is the owner of a channel and of the things, can connect the things to the channel (which is equivalent of giving permissions to these things to communicate over given communication group). To connect a thing to the channel you should send following request: This endpoint will be depreciated in 0.11.0. It will be replaced with the bulk endpoint found at /connect. curl -s -S -i --cacert docker/ssl/certs/ca.crt -X PUT -H \"Authorization: Bearer <user_token>\" https://localhost/channels/<channel_id>/things/<thing_id> To connect multiple things to a channel, you can send the following request: curl -s -S -i --cacert docker/ssl/certs/ca.crt -X POST -H \"Content-Type: application/json\" -H \"Authorization: Bearer <user_token>\" https://localhost/connect -d '{\"channel_ids\":[\"<channel_id>\", \"<channel_id>\"],\"thing_ids\":[\"<thing_id>\", \"<thing_id>\"]}' You can observe which things are connected to specific channel: curl -s -S -i --cacert docker/ssl/certs/ca.crt -H \"Authorization: Bearer <user_token>\" https://localhost/channels/<channel_id>/things You can also observe which things are not connected to specific channel by adding a query parameter connected=false to the HTTP request: curl -s -S -i --cacert docker/ssl/certs/ca.crt -H \"Authorization: Bearer <user_token>\" https://localhost/channels/<channel_id>/things?connected=false Response that you'll get should look like this: { \"total\": 2, \"offset\": 0, \"limit\": 10, \"things\": [ { \"id\": \"3ffb3880-d1e6-4edd-acd9-4294d013f35b\", \"name\": \"d0\", \"key\": \"b1996995-237a-4552-94b2-83ec2e92a040\", \"metadata\": \"{}\" }, { \"id\": \"94d166d6-6477-43dc-93b7-5c3707dbef1e\", \"name\": \"d1\", \"key\": \"e4588a68-6028-4740-9f12-c356796aebe8\", \"metadata\": \"{}\" } ] } You can observe to which channels is specified thing connected: curl -s -S -i --cacert docker/ssl/certs/ca.crt -H \"Authorization: Bearer <user_token>\" https://localhost/things/<thing_id>/channels You can also observe to which channels is specified thing not connected by adding a query parameter connected=false to the HTTP request: curl -s -S -i --cacert docker/ssl/certs/ca.crt -H \"Authorization: Bearer <user_token>\" https://localhost/things/<thing_id>/channels?connected=false Response that you'll get should look like this: { \"total\": 2, \"offset\": 0, \"limit\": 10, \"channels\": [ { \"id\": \"5e62eb13-2695-4860-8d87-85b8a2f80fd4\", \"name\": \"c1\", \"metadata\": \"{}\" }, { \"id\": \"c4b5e19a-7ffe-4172-b2c5-c8b9d570a165\", \"name\": \"c0\", \"metadata\":\"{}\" } ] } If you want to disconnect your thing from the channel, send following request: curl -s -S -i --cacert docker/ssl/certs/ca.crt -X DELETE -H \"Authorization: Bearer <user_token>\" https://localhost/channels/<channel_id>/things/<thing_id> For more information about the Things service API, please check out the API documentation .","title":"Access Control"},{"location":"provision/#provision-service","text":"Provisioning is a process of configuration of an IoT platform in which system operator creates and sets-up different entities used in the platform - users, channels and things. It is part of process of setting up IoT applications where we connect devices on edge with platform in cloud. For provisioning we can use Mainflux CLI for creating users and for each node in the edge (eg. gateway) required number of things, channels, connecting them and creating certificates if needed. Provision service is used to set up initial application configuration once user is created. Provision service creates things, channels, connections and certificates. Once user is created we can use provision to create a setup for edge node in one HTTP request instead of issuing several CLI commands. Provision service provides an HTTP API to interact with Mainflux . For gateways to communicate with Mainflux configuration is required (MQTT host, thing, channels, certificates...). Gateway will send a request to Bootstrap service providing <external_id> and <external_key> in HTTP request to get the configuration. To make a request to Bootstrap service you can use Agent service on a gateway. To create bootstrap configuration you can use Bootstrap or Provision service. Mainflux UI uses Bootstrap service for creating gateway configurations. Provision service should provide an easy way of provisioning your gateways i.e creating bootstrap configuration and as many things and channels that your setup requires. Also, you may use provision service to create certificates for each thing. Each service running on gateway may require more than one thing and channel for communication. If, for example, you are using services Agent and Export on a gateway you will need two channels for Agent ( data and control ) and one thing for Export . Additionally, if you enabled mTLS each service will need its own thing and certificate for access to Mainflux . Your setup could require any number of things and channels, this kind of setup we can call provision layout . Provision service provides a way of specifying this provision layout and creating a setup according to that layout by serving requests on /mapping endpoint. Provision layout is configured in config.toml .","title":"Provision Service"},{"location":"provision/#configuration","text":"The service is configured using the environment variables presented in the following table . Note that any unset variables will be replaced with their default values. By default, call to /mapping endpoint will create one thing and two channels ( control and data ) and connect it as this is typical setup required by Agent . If there is a requirement for different provision layout we can use config file in addition to environment variables. For the purposes of running provision as an add-on in docker composition environment variables seems more suitable. Environment variables are set in .env . Configuration can be specified in config.toml . Config file can specify all the settings that environment variables can configure and in addition /mapping endpoint provision layout can be configured. In config.toml we can enlist an array of things and channels that we want to create and make connections between them which we call provision layout. Things Metadata can be whatever suits your needs. Thing that has metadata with external_id will have bootstrap configuration created, external_id value will be populated with value from request ). Bootstrap configuration can be fetched with Agent . For channel's metadata type is reserved for control and data which we use with Agent . Example of provision layout below [bootstrap] [bootstrap.content] [bootstrap.content.agent.edgex] url = \"http://localhost:48090/api/v1/\" [bootstrap.content.agent.log] level = \"info\" [bootstrap.content.agent.mqtt] mtls = false qos = 0 retain = false skip_tls_ver = true url = \"localhost:1883\" [bootstrap.content.agent.server] nats_url = \"localhost:4222\" port = \"9000\" [bootstrap.content.agent.heartbeat] interval = \"30s\" [bootstrap.content.agent.terminal] session_timeout = \"30s\" [bootstrap.content.export.exp] log_level = \"debug\" nats = \"nats://localhost:4222\" port = \"8172\" cache_url = \"localhost:6379\" cache_pass = \"\" cache_db = \"0\" [bootstrap.content.export.mqtt] ca_path = \"ca.crt\" cert_path = \"thing.crt\" channel = \"\" host = \"tcp://localhost:1883\" mtls = false password = \"\" priv_key_path = \"thing.key\" qos = 0 retain = false skip_tls_ver = false username = \"\" [[bootstrap.content.export.routes]] mqtt_topic = \"\" nats_topic = \"channels\" subtopic = \"\" type = \"mfx\" workers = 10 [[bootstrap.content.export.routes]] mqtt_topic = \"\" nats_topic = \"export\" subtopic = \"\" type = \"default\" workers = 10 [[things]] name = \"thing\" [things.metadata] external_id = \"xxxxxx\" [[channels]] name = \"control-channel\" [channels.metadata] type = \"control\" [[channels]] name = \"data-channel\" [channels.metadata] type = \"data\" [[channels]] name = \"export-channel\" [channels.metadata] type = \"export\" [bootstrap.content] will be marshalled and saved into content field in bootstrap configs when request to /mappings is made, content field from bootstrap config is used to create Agent and Export configuration files upon Agent fetching bootstrap configuration.","title":"Configuration"},{"location":"provision/#authentication","text":"In order to create necessary entities provision service needs to authenticate against Mainflux. To provide authentication credentials to the provision service you can pass it in as an environment variable or in a config file as Mainflux user and password or as API token (that can be issued on /users or /keys endpoint of auth . Additionally, users or API token can be passed in Authorization header, this authentication takes precedence over others. username , password - ( MF_PROVISION_USER , MF_PROVISION_PASSWORD in .env , mf_user , mf_pass in config.toml API Key - ( MF_PROVISION_API_KEY in .env or config.toml Authorization: Bearer Token|ApiKey - request authorization header containing either users token or API key. Check auth .","title":"Authentication"},{"location":"provision/#running","text":"Provision service can be run as a standalone or in docker composition as addon to the core docker composition. Standalone: MF_PROVISION_BS_SVC_URL=http://localhost:8202/things \\ MF_PROVISION_THINGS_LOCATION=http://localhost:8182 \\ MF_PROVISION_USERS_LOCATION=http://localhost:8180 \\ MF_PROVISION_CONFIG_FILE=docker/addons/provision/configs/config.toml \\ build/mainflux-provision Docker composition: docker-compose -f docker/addons/provision/docker-compose.yml up","title":"Running"},{"location":"provision/#provision_1","text":"For the case that credentials or API token is passed in configuration file or environment variables, call to /mapping endpoint doesn't require Authentication header: curl -s -S -X POST http://localhost:8888/mapping -H 'Content-Type: application/json' -d '{\"external_id\": \"33:52:77:99:43\", \"external_key\": \"223334fw2\"}' In the case that provision service is not deployed with credentials or API key or you want to use user other than one being set in environment (or config file): curl -s -S -X POST http://localhost:8091/mapping -H \"Authorization: Bearer <token|api_key>\" -H 'Content-Type: application/json' -d '{\"external_id\": \"<external_id>\", \"external_key\": \"<external_key>\"}' Or if you want to specify a name for thing different than in config.toml you can specify post data as: {\"name\": \"<name>\", \"external_id\": \"<external_id>\", \"external_key\": \"<external_key>\"} Response contains created things, channels and certificates if any: { \"things\": [ { \"id\": \"c22b0c0f-8c03-40da-a06b-37ed3a72c8d1\", \"name\": \"thing\", \"key\": \"007cce56-e0eb-40d6-b2b9-ed348a97d1eb\", \"metadata\": { \"external_id\": \"33:52:79:C3:43\" } } ], \"channels\": [ { \"id\": \"064c680e-181b-4b58-975e-6983313a5170\", \"name\": \"control-channel\", \"metadata\": { \"type\": \"control\" } }, { \"id\": \"579da92d-6078-4801-a18a-dd1cfa2aa44f\", \"name\": \"data-channel\", \"metadata\": { \"type\": \"data\" } } ], \"whitelisted\": { \"c22b0c0f-8c03-40da-a06b-37ed3a72c8d1\": true } }","title":"Provision"},{"location":"provision/#example","text":"Deploy Mainflux UI docker composition as it contains all the required services for provisioning to work ( certs , bootstrap and Mainflux core) git clone https://github.com/mainflux/ui cd ui docker-compose -f docker/docker-compose.yml up Create user and obtain access token mainflux-cli -m https://mainflux.com users create john.doe@email.com 12345678 # Retrieve token mainflux-cli -m https://mainflux.com users token john.doe@email.com 12345678 created: eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJleHAiOjE1OTY1ODU3MDUsImlhdCI6MTU5NjU0OTcwNSwiaXNzIjoibWFpbmZsdXguYXV0aG4iLCJzdWIiOiJtaXJrYXNoQGdtYWlsLmNvbSIsInR5cGUiOjB9._vq0zJzFc9tQqc8x74kpn7dXYefUtG9IB0Cb-X2KMK8 Put a value of token into environment variable TOKEN=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJleHAiOjE1OTY1ODU3MDUsImlhdCI6MTU5NjU0OTcwNSwiaXNzIjoibWFpbmZsdXguYXV0aG4iLCJzdWIiOiJtaXJrYXNoQGdtYWlsLmNvbSIsInR5cGUiOjB9._vq0zJzFc9tQqc8x74kpn7dXYefUtG9IB0Cb-X2KMK8 Make a call to provision endpoint curl -s -S -X POST http://mainflux.com:8190/mapping -H \"Authorization: Bearer $TOKEN\" -H 'Content-Type: application/json' -d '{\"name\":\"edge-gw\", \"external_id\" : \"gateway\", \"external_key\":\"external_key\" }' To check the results you can make a call to bootstrap endpoint curl -s -S -X GET http://mainflux.com:8202/things/bootstrap/gateway -H \"Authorization: Thing external_key\" -H 'Content-Type: application/json' Or you can start Agent with: git clone https://github.com/mainflux/agent cd agent make MF_AGENT_BOOTSTRAP_ID=gateway MF_AGENT_BOOTSTRAP_KEY=external_key MF_AGENT_BOOTSTRAP_URL=http://mainflux.ccom:8202/things/bootstrap build/mainflux-agent Agent will retrieve connections parameters and connect to Mainflux cloud. For more information about the Provision service API, please check out the API documentation .","title":"Example"},{"location":"provision/#certs-service","text":"Issues certificates for things. Certs service can create certificates to be used when Mainflux is deployed to support mTLS. Certs service will create certificate for valid thing ID if valid user token is passed and user is owner of the provided thing ID. Certificate service can create certificates in two modes: 1. Development mode - to be used when no PKI is deployed, this works similar to the make thing_cert 2. PKI mode - certificates issued by PKI, when you deploy Vault as PKI certificate management cert service will proxy requests to Vault previously checking access rights and saving info on successfully created certificate.","title":"Certs Service"},{"location":"provision/#development-mode","text":"If MF_CERTS_VAULT_HOST is empty than Development mode is on. To issue a certificate: TOKEN=`curl -s --insecure -S -X POST http://localhost/tokens -H 'Content-Type: application/json' -d '{\"email\":\"edge@email.com\",\"password\":\"12345678\"}' | jq -r '.token'` curl -s -S -X POST http://localhost:8204/certs -H \"Authorization: Bearer $TOKEN\" -H 'Content-Type: application/json' -d '{\"thing_id\":<thing_id>, \"rsa_bits\":2048, \"key_type\":\"rsa\"}' { \"ThingID\": \"\", \"ClientCert\": \"-----BEGIN CERTIFICATE-----\\nMIIDmTCCAoGgAwIBAgIRANmkAPbTR1UYeYO0Id/4+8gwDQYJKoZIhvcNAQELBQAw\\nVzESMBAGA1UEAwwJbG9jYWxob3N0MREwDwYDVQQKDAhNYWluZmx1eDEMMAoGA1UE\\nCwwDSW9UMSAwHgYJKoZIhvcNAQkBFhFpbmZvQG1haW5mbHV4LmNvbTAeFw0yMDA2\\nMzAxNDIxMDlaFw0yMDA5MjMyMjIxMDlaMFUxETAPBgNVBAoTCE1haW5mbHV4MREw\\nDwYDVQQLEwhtYWluZmx1eDEtMCsGA1UEAxMkYjAwZDBhNzktYjQ2YS00NTk3LTli\\nNGYtMjhkZGJhNTBjYTYyMIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEA\\ntgS2fLUWG3CCQz/l6VRQRJfRvWmdxK0mW6zIXGeeOILYZeaLiuiUnohwMJ4RiMqT\\nuJbInAIuO/Tt5osfrCFFzPEOLYJ5nZBBaJfTIAxqf84Ou1oeMRll4wpzgeKx0rJO\\nXMAARwn1bT9n3uky5QQGSLy4PyyILzSXH/1yCQQctdQB/Ar/UI1TaYoYlGzh7dHT\\nWpcxq1HYgCyAtcrQrGD0rEwUn82UBCrnya+bygNqu0oDzIFQwa1G8jxSgXk0mFS1\\nWrk7rBipsvp8HQhdnvbEVz4k4AAKcQxesH4DkRx/EXmU2UvN3XysvcJ2bL+UzMNI\\njNhAe0pgPbB82F6zkYZ/XQIDAQABo2IwYDAOBgNVHQ8BAf8EBAMCB4AwHQYDVR0l\\nBBYwFAYIKwYBBQUHAwIGCCsGAQUFBwMBMA4GA1UdDgQHBAUBAgMEBjAfBgNVHSME\\nGDAWgBRs4xR91qEjNRGmw391xS7x6Tc+8jANBgkqhkiG9w0BAQsFAAOCAQEAW/dS\\nV4vNLTZwBnPVHUX35pRFxPKvscY+vnnpgyDtITgZHYe0KL+Bs3IHuywtqaezU5x1\\nkZo+frE1OcpRvp7HJtDiT06yz+18qOYZMappCWCeAFWtZkMhlvnm3TqTkgui6Xgl\\nGj5xnPb15AOlsDE2dkv5S6kEwJGHdVX6AOWfB4ubUq5S9e4ABYzXGUty6Hw/ZUmJ\\nhCTRVJ7cQJVTJsl1o7CYT8JBvUUG75LirtoFE4M4JwsfsKZXzrQffTf1ynqI3dN/\\nHWySEbvTSWcRcA3MSmOTxGt5/zwCglHDlWPKMrXtjTW7NPuGL5/P9HSB9HGVVeET\\nDUMdvYwgj0cUCEu3LA==\\n-----END CERTIFICATE-----\\n\", \"IssuingCA\": \"\", \"CAChain\": null, \"ClientKey\": \"-----BEGIN RSA PRIVATE KEY-----\\nMIIEowIBAAKCAQEAtgS2fLUWG3CCQz/l6VRQRJfRvWmdxK0mW6zIXGeeOILYZeaL\\niuiUnohwMJ4RiMqTuJbInAIuO/Tt5osfrCFFzPEOLYJ5nZBBaJfTIAxqf84Ou1oe\\nMRll4wpzgeKx0rJOXMAARwn1bT9n3uky5QQGSLy4PyyILzSXH/1yCQQctdQB/Ar/\\nUI1TaYoYlGzh7dHTWpcxq1HYgCyAtcrQrGD0rEwUn82UBCrnya+bygNqu0oDzIFQ\\nwa1G8jxSgXk0mFS1Wrk7rBipsvp8HQhdnvbEVz4k4AAKcQxesH4DkRx/EXmU2UvN\\n3XysvcJ2bL+UzMNIjNhAe0pgPbB82F6zkYZ/XQIDAQABAoIBAALoal3tqq+/iWU3\\npR2oKiweXMxw3oNg3McEKKNJSH7QoFJob3xFoPIzbc9pBxCvY9LEHepYIpL0o8RW\\nHqhqU6olg7t4ZSb+Qf1Ax6+wYxctnJCjrO3N4RHSfevqSjr6fEQBEUARSal4JNmr\\n0hNUkCEjWrIvrPFMHsn1C5hXR3okJQpGsad4oCGZDp2eZ/NDyvmLBLci9/5CJdRv\\n6roOF5ShWweKcz1+pfy666Q8RiUI7H1zXjPaL4yqkv8eg/WPOO0dYF2Ri2Grk9OY\\n1qTM0W1vi9zfncinZ0DpgtwMTFQezGwhUyJHSYHmjVBA4AaYIyOQAI/2dl5fXM+O\\n9JfXpOUCgYEA10xAtMc/8KOLbHCprpc4pbtOqfchq/M04qPKxQNAjqvLodrWZZgF\\nexa+B3eWWn5MxmQMx18AjBCPwbNDK8Rkd9VqzdWempaSblgZ7y1a0rRNTXzN5DFP\\noiuRQV4wszCuj5XSdPn+lxApaI/4+TQ0oweIZCpGW39XKePPoB5WZiMCgYEA2G3W\\niJncRpmxWwrRPi1W26E9tWOT5s9wYgXWMc+PAVUd/qdDRuMBHpu861Qoghp/MJog\\nBYqt2rQqU0OxvIXlXPrXPHXrCLOFwybRCBVREZrg4BZNnjyDTLOu9C+0M3J9ImCh\\n3vniYqb7S0gRmoDM0R3Zu4+ajfP2QOGLXw1qHH8CgYEAl0EQ7HBW8V5UYzi7XNcM\\nixKOb0YZt83DR74+hC6GujTjeLBfkzw8DX+qvWA8lxLIKVC80YxivAQemryv4h21\\nX6Llx/nd1UkXUsI+ZhP9DK5y6I9XroseIRZuk/fyStFWsbVWB6xiOgq2rKkJBzqw\\nCCEQpx40E6/gsqNDiIAHvvUCgYBkkjXc6FJ55DWMLuyozfzMtpKsVYeG++InSrsM\\nDn1PizQS/7q9mAMPLCOP312rh5CPDy/OI3FCbfI1GwHerwG0QUP/bnQ3aOTBmKoN\\n7YnsemIA/5w16bzBycWE5x3/wjXv4aOWr9vJJ/siMm0rtKp4ijyBcevKBxHpeGWB\\nWAR1FQKBgGIqAxGnBpip9E24gH894BaGHHMpQCwAxARev6sHKUy27eFUd6ipoTva\\n4Wv36iz3gxU4R5B0gyfnxBNiUab/z90cb5+6+FYO13kqjxRRZWffohk5nHlmFN9K\\nea7KQHTfTdRhOLUzW2yVqLi9pzfTfA6Yqf3U1YD3bgnWrp1VQnjo\\n-----END RSA PRIVATE KEY-----\\n\", \"PrivateKeyType\": \"\", \"Serial\": \"\", \"Expire\": \"0001-01-01T00:00:00Z\" }","title":"Development mode"},{"location":"provision/#pki-mode","text":"When MF_CERTS_VAULT_HOST is set it is presumed that Vault is installed and certs service will issue certificates using Vault API. First you'll need to set up Vault . To setup Vault follow steps in Build Your Own Certificate Authority (CA) . To setup certs service with Vault following environment variables must be set: MF_CERTS_VAULT_HOST=vault-domain.com MF_CERTS_VAULT_PKI_PATH=<vault_pki_path> MF_CERTS_VAULT_ROLE=<vault_role> MF_CERTS_VAULT_TOKEN=<vault_acces_token> For lab purposes you can use docker-compose and script for setting up PKI in https://github.com/mteodor/vault Issuing certificate is same as in Development mode. In this mode certificates can also be revoked: curl -s -S -X DELETE http://localhost:8204/certs/revoke -H \"Authorization: Bearer $TOKEN\" -H 'Content-Type: application/json' -d '{\"thing_id\":\"c30b8842-507c-4bcd-973c-74008cef3be5\"}' For more information about the Certification service API, please check out the API documentation .","title":"PKI mode"},{"location":"security/","text":"Security # Server Configuration # Users # If either the cert or key is not set, the server will use insecure transport. MF_USERS_SERVER_CERT the path to server certificate in pem format. MF_USERS_SERVER_KEY the path to the server key in pem format. Things # If either the cert or key is not set, the server will use insecure transport. MF_THINGS_SERVER_CERT the path to server certificate in pem format. MF_THINGS_SERVER_KEY the path to the server key in pem format. Standalone mode # Sometimes it makes sense to run Things as a standalone service to reduce network traffic or simplify deployment. This means that Things service operates only using a single user and is able to authorize it without gRPC communication with Auth service. When running Things in the standalone mode, Auth and Users services can be omitted from the deployment. To run service in a standalone mode, set MF_THINGS_STANDALONE_EMAIL and MF_THINGS_STANDALONE_TOKEN . Client Configuration # If you wish to secure the gRPC connection to Things and Users services you must define the CAs that you trust. This does not support mutual certificate authentication. Adapter Configuration # MF_HTTP_ADAPTER_CA_CERTS , MF_MQTT_ADAPTER_CA_CERTS , MF_WS_ADAPTER_CA_CERTS , MF_COAP_ADAPTER_CA_CERTS - the path to a file that contains the CAs in PEM format. If not set, the default connection will be insecure. If it fails to read the file, the adapter will fail to start up. Things # MF_THINGS_CA_CERTS - the path to a file that contains the CAs in PEM format. If not set, the default connection will be insecure. If it fails to read the file, the service will fail to start up. Securing PostgreSQL Connections # By default, Mainflux will connect to Postgres using insecure transport. If a secured connection is required, you can select the SSL mode and set paths to any extra certificates and keys needed. MF_USERS_DB_SSL_MODE the SSL connection mode for Users. MF_USERS_DB_SSL_CERT the path to the certificate file for Users. MF_USERS_DB_SSL_KEY the path to the key file for Users. MF_USERS_DB_SSL_ROOT_CERT the path to the root certificate file for Users. MF_THINGS_DB_SSL_MODE the SSL connection mode for Things. MF_THINGS_DB_SSL_CERT the path to the certificate file for Things. MF_THINGS_DB_SSL_KEY the path to the key file for Things. MF_THINGS_DB_SSL_ROOT_CERT the path to the root certificate file for Things. Supported database connection modes are: disabled (default), required , verify-ca and verify-full . Securing gRPC # By default gRPC communication is not secure as Mainflux system is most often run in a private network behind the reverse proxy. However, TLS can be activated and configured.","title":"Security"},{"location":"security/#security","text":"","title":"Security"},{"location":"security/#server-configuration","text":"","title":"Server Configuration"},{"location":"security/#users","text":"If either the cert or key is not set, the server will use insecure transport. MF_USERS_SERVER_CERT the path to server certificate in pem format. MF_USERS_SERVER_KEY the path to the server key in pem format.","title":"Users"},{"location":"security/#things","text":"If either the cert or key is not set, the server will use insecure transport. MF_THINGS_SERVER_CERT the path to server certificate in pem format. MF_THINGS_SERVER_KEY the path to the server key in pem format.","title":"Things"},{"location":"security/#standalone-mode","text":"Sometimes it makes sense to run Things as a standalone service to reduce network traffic or simplify deployment. This means that Things service operates only using a single user and is able to authorize it without gRPC communication with Auth service. When running Things in the standalone mode, Auth and Users services can be omitted from the deployment. To run service in a standalone mode, set MF_THINGS_STANDALONE_EMAIL and MF_THINGS_STANDALONE_TOKEN .","title":"Standalone mode"},{"location":"security/#client-configuration","text":"If you wish to secure the gRPC connection to Things and Users services you must define the CAs that you trust. This does not support mutual certificate authentication.","title":"Client Configuration"},{"location":"security/#adapter-configuration","text":"MF_HTTP_ADAPTER_CA_CERTS , MF_MQTT_ADAPTER_CA_CERTS , MF_WS_ADAPTER_CA_CERTS , MF_COAP_ADAPTER_CA_CERTS - the path to a file that contains the CAs in PEM format. If not set, the default connection will be insecure. If it fails to read the file, the adapter will fail to start up.","title":"Adapter Configuration"},{"location":"security/#things_1","text":"MF_THINGS_CA_CERTS - the path to a file that contains the CAs in PEM format. If not set, the default connection will be insecure. If it fails to read the file, the service will fail to start up.","title":"Things"},{"location":"security/#securing-postgresql-connections","text":"By default, Mainflux will connect to Postgres using insecure transport. If a secured connection is required, you can select the SSL mode and set paths to any extra certificates and keys needed. MF_USERS_DB_SSL_MODE the SSL connection mode for Users. MF_USERS_DB_SSL_CERT the path to the certificate file for Users. MF_USERS_DB_SSL_KEY the path to the key file for Users. MF_USERS_DB_SSL_ROOT_CERT the path to the root certificate file for Users. MF_THINGS_DB_SSL_MODE the SSL connection mode for Things. MF_THINGS_DB_SSL_CERT the path to the certificate file for Things. MF_THINGS_DB_SSL_KEY the path to the key file for Things. MF_THINGS_DB_SSL_ROOT_CERT the path to the root certificate file for Things. Supported database connection modes are: disabled (default), required , verify-ca and verify-full .","title":"Securing PostgreSQL Connections"},{"location":"security/#securing-grpc","text":"By default gRPC communication is not secure as Mainflux system is most often run in a private network behind the reverse proxy. However, TLS can be activated and configured.","title":"Securing gRPC"},{"location":"storage/","text":"Storage # Mainflux supports various storage databases in which messages are stored: CassandraDB MongoDB InfluxDB PostgreSQL These storages are activated via docker-compose add-ons. The <project_root>/docker folder contains an addons directory. This directory is used for various services that are not core to the Mainflux platform but could be used for providing additional features. In order to run these services, core services, as well as the network from the core composition, should be already running. Writers # Writers provide an implementation of various message writers . Message writers are services that consume Mainflux messages, transform them to desired format and store them in specific data store. The path of the configuration file can be set using the following environment variables: MF_CASSANDRA_WRITER_CONFIG_PATH , MF_POSTGRES_WRITER_CONFIG_PATH , MF_INFLUX_WRITER_CONFIG_PATH and MF_MONGO_WRITER_CONFIG_PATH . Subscriber config # Each writer can filter messages based on subjects list that is set in config.toml configuration file. If you want to listen on all subjects, just set the field subjects in the [subscriber] section as [\"channels.>\"] , otherwise pass the list of subjects. Here is an example: [subscriber] subjects = [\"channels.*.messages.bedroom.temperature\",\"channels.*.messages.bedroom.humidity\"] Regarding the Subtopics Section in the messaging page, the example channels/<channel_id>/messages/bedroom/temperature can be filtered as \"channels.*.bedroom.temperature\" . The formatting of this filtering list is determined by the NATS format ( Subject-Based Messaging & Wildcards ). Transformer config # There are two types of transformers: SenML and JSON. The transformer type is set in configuration file. For SenML transformer, supported message payload formats are SenML+CBOR and SenML+JSON. They are configurable over content_type field in the [transformer] section and expect application/senml+json or application/senml+cbor formats. Here is an example: [transformer] format = \"senml\" content_type = \"application/senml+json\" Usually, the payload of the IoT message contains message time. It can be in different formats (like base time and record time in the case of SenML) and the message field can be under the arbitrary key. Usually, we would want to map that time to the Mainflux Message field Created and for that reason, we need to configure the Transformer to be able to read the field, parse it using proper format and location (if devices time is different than the service time), and map it to Mainflux Message. For JSON transformer you can configure time_fields in the [transformer] section to use arbitrary fields from the JSON message payload as timestamp. time_fields is represented by an array of objects with fields field_name , field_format and location that represent respectively the name of the JSON key to use as timestamp, the time format to use for the field value and the time location. Here is an example: [transformer] format = \"json\" time_fields = [{ field_name = \"seconds_key\", field_format = \"unix\", location = \"UTC\"}, { field_name = \"millis_key\", field_format = \"unix_ms\", location = \"UTC\"}, { field_name = \"micros_key\", field_format = \"unix_us\", location = \"UTC\"}, { field_name = \"nanos_key\", field_format = \"unix_ns\", location = \"UTC\"}] JSON transformer can be used for any JSON payload. For the messages that contain _JSON array as the root element_, JSON Transformer does normalization of the data: it creates a separate JSON message for each JSON object in the root. In order to be processed and stored properly, JSON messages need to contain message format information. For the sake of simplicity, nested JSON objects are flatten to a single JSON object in InfluxDB, using composite keys separated by the `/` separator. This implies that the separator character (`/`) _is not allowed in the JSON object key_ while using InfluxDB. Apart from InfluxDB, separator character (`/`) usage in the JSON object key is permitted, since other [Writer](storage.md#writers) types do not flat the nested JSON objects. For example, the following JSON object: ```json { \"name\": \"name\", \"id\":8659456789564231564, \"in\": 3.145, \"alarm\": true, \"ts\": 1571259850000, \"d\": { \"tmp\": 2.564, \"hmd\": 87, \"loc\": { \"x\": 1, \"y\": 2 } } } for InfluxDB will be transformed to: { \"name\": \"name\", \"id\":8659456789564231564, \"in\": 3.145, \"alarm\": true, \"ts\": 1571259850000, \"d/tmp\": 2.564, \"d/hmd\": 87, \"d/loc/x\": 1, \"d/loc/y\": 2 } while for other Writers it will preserve its original format. The message format is stored in the subtopic . It's the last part of the subtopic. In the example: http://localhost:8185/channels/<channelID>/messages/home/temperature/myFormat the message format is myFormat . It can be any valid subtopic name, JSON transformer is format-agnostic. The format is used by the JSON message consumers so that they can process the message properly. If the format is not present (i.e. message subtopic is empty), JSON Transformer will report an error. Message writers will store the message(s) in the table/collection/measurement (depending on the underlying database) with the name of the format (which in the example is myFormat ). Mainflux writers will try to save any format received (whether it will be successful depends on the writer implementation and the underlying database), but it's recommended that publishers don't send different formats to the same subtopic. InfluxDB, InfluxDB Writer and Grafana # From the project root execute the following command: docker-compose -f docker/addons/influxdb-writer/docker-compose.yml up -d This will install and start: InfluxDB - time series database InfluxDB writer - message repository implementation for InfluxDB Grafana - tool for database exploration and data visualization and analytics Those new services will take some additional ports: 8086 by InfluxDB 8900 by InfluxDB writer service 3001 by Grafana To access Grafana, navigate to http://localhost:3001 and login with: admin , password: admin Cassandra and Cassandra Writer # ./docker/addons/cassandra-writer/init.sh Please note that Cassandra may not be suitable for your testing environment because of its high system requirements. MongoDB and MongoDB Writer # docker-compose -f docker/addons/mongodb-writer/docker-compose.yml up -d MongoDB default port (27017) is exposed, so you can use various tools for database inspection and data visualization. PostgreSQL and PostgreSQL Writer # docker-compose -f docker/addons/postgres-writer/docker-compose.yml up -d Postgres default port (5432) is exposed, so you can use various tools for database inspection and data visualization. Readers # Readers provide an implementation of various message readers . Message readers are services that consume normalized (in SenML format) Mainflux messages from data storage and opens HTTP API for message consumption. Installing corresponding writer before reader is implied. Each of the Reader services exposes the same HTTP API for fetching messages on its default port. To read sent messages on channel with id channel_id you should send GET request to /channels/<channel_id>/messages with thing access token in Authorization header. That thing must be connected to channel with channel_id Response should look like this: HTTP/1.1 200 OK Content-Type: application/json Date: Tue, 18 Sep 2018 18:56:19 GMT Content-Length: 228 { \"messages\": [ { \"Channel\": 1, \"Publisher\": 2, \"Protocol\": \"mqtt\", \"Name\": \"name:voltage\", \"Unit\": \"V\", \"Value\": 5.6, \"Time\": 48.56 }, { \"Channel\": 1, \"Publisher\": 2, \"Protocol\": \"mqtt\", \"Name\": \"name:temperature\", \"Unit\": \"C\", \"Value\": 24.3, \"Time\": 48.56 } ] } Note that you will receive only those messages that were sent by authorization token's owner. You can specify offset and limit parameters in order to fetch specific group of messages. An example of HTTP request looks like: curl -s -S -i -H \"Authorization: Thing <thing_key>\" http://localhost:<service_port>/channels/<channel_id>/messages?offset=0&limit=5&format=<subtopic> If you don't provide offset and limit parameters, default values will be used instead: 0 for offset and 10 for limit . The format parameter indicates the last subtopic of the message. As indicated under the Writers section, the message format is stored in the subtopic as the last part of the subtopic. In the example: http://localhost:8185/channels/<channelID>/messages/home/temperature/myFormat the message format is myFormat and the value for format=<subtopic> is format=myFormat . InfluxDB Reader # To start InfluxDB reader, execute the following command: docker-compose -f docker/addons/influxdb-reader/docker-compose.yml up -d Cassandra Reader # To start Cassandra reader, execute the following command: docker-compose -f docker/addons/cassandra-reader/docker-compose.yml up -d MongoDB Reader # To start MongoDB reader, execute the following command: docker-compose -f docker/addons/mongodb-reader/docker-compose.yml up -d PostgreSQL Reader # To start PostgreSQL reader, execute the following command: docker-compose -f docker/addons/postgres-reader/docker-compose.yml up -d","title":"Storage"},{"location":"storage/#storage","text":"Mainflux supports various storage databases in which messages are stored: CassandraDB MongoDB InfluxDB PostgreSQL These storages are activated via docker-compose add-ons. The <project_root>/docker folder contains an addons directory. This directory is used for various services that are not core to the Mainflux platform but could be used for providing additional features. In order to run these services, core services, as well as the network from the core composition, should be already running.","title":"Storage"},{"location":"storage/#writers","text":"Writers provide an implementation of various message writers . Message writers are services that consume Mainflux messages, transform them to desired format and store them in specific data store. The path of the configuration file can be set using the following environment variables: MF_CASSANDRA_WRITER_CONFIG_PATH , MF_POSTGRES_WRITER_CONFIG_PATH , MF_INFLUX_WRITER_CONFIG_PATH and MF_MONGO_WRITER_CONFIG_PATH .","title":"Writers"},{"location":"storage/#subscriber-config","text":"Each writer can filter messages based on subjects list that is set in config.toml configuration file. If you want to listen on all subjects, just set the field subjects in the [subscriber] section as [\"channels.>\"] , otherwise pass the list of subjects. Here is an example: [subscriber] subjects = [\"channels.*.messages.bedroom.temperature\",\"channels.*.messages.bedroom.humidity\"] Regarding the Subtopics Section in the messaging page, the example channels/<channel_id>/messages/bedroom/temperature can be filtered as \"channels.*.bedroom.temperature\" . The formatting of this filtering list is determined by the NATS format ( Subject-Based Messaging & Wildcards ).","title":"Subscriber config"},{"location":"storage/#transformer-config","text":"There are two types of transformers: SenML and JSON. The transformer type is set in configuration file. For SenML transformer, supported message payload formats are SenML+CBOR and SenML+JSON. They are configurable over content_type field in the [transformer] section and expect application/senml+json or application/senml+cbor formats. Here is an example: [transformer] format = \"senml\" content_type = \"application/senml+json\" Usually, the payload of the IoT message contains message time. It can be in different formats (like base time and record time in the case of SenML) and the message field can be under the arbitrary key. Usually, we would want to map that time to the Mainflux Message field Created and for that reason, we need to configure the Transformer to be able to read the field, parse it using proper format and location (if devices time is different than the service time), and map it to Mainflux Message. For JSON transformer you can configure time_fields in the [transformer] section to use arbitrary fields from the JSON message payload as timestamp. time_fields is represented by an array of objects with fields field_name , field_format and location that represent respectively the name of the JSON key to use as timestamp, the time format to use for the field value and the time location. Here is an example: [transformer] format = \"json\" time_fields = [{ field_name = \"seconds_key\", field_format = \"unix\", location = \"UTC\"}, { field_name = \"millis_key\", field_format = \"unix_ms\", location = \"UTC\"}, { field_name = \"micros_key\", field_format = \"unix_us\", location = \"UTC\"}, { field_name = \"nanos_key\", field_format = \"unix_ns\", location = \"UTC\"}] JSON transformer can be used for any JSON payload. For the messages that contain _JSON array as the root element_, JSON Transformer does normalization of the data: it creates a separate JSON message for each JSON object in the root. In order to be processed and stored properly, JSON messages need to contain message format information. For the sake of simplicity, nested JSON objects are flatten to a single JSON object in InfluxDB, using composite keys separated by the `/` separator. This implies that the separator character (`/`) _is not allowed in the JSON object key_ while using InfluxDB. Apart from InfluxDB, separator character (`/`) usage in the JSON object key is permitted, since other [Writer](storage.md#writers) types do not flat the nested JSON objects. For example, the following JSON object: ```json { \"name\": \"name\", \"id\":8659456789564231564, \"in\": 3.145, \"alarm\": true, \"ts\": 1571259850000, \"d\": { \"tmp\": 2.564, \"hmd\": 87, \"loc\": { \"x\": 1, \"y\": 2 } } } for InfluxDB will be transformed to: { \"name\": \"name\", \"id\":8659456789564231564, \"in\": 3.145, \"alarm\": true, \"ts\": 1571259850000, \"d/tmp\": 2.564, \"d/hmd\": 87, \"d/loc/x\": 1, \"d/loc/y\": 2 } while for other Writers it will preserve its original format. The message format is stored in the subtopic . It's the last part of the subtopic. In the example: http://localhost:8185/channels/<channelID>/messages/home/temperature/myFormat the message format is myFormat . It can be any valid subtopic name, JSON transformer is format-agnostic. The format is used by the JSON message consumers so that they can process the message properly. If the format is not present (i.e. message subtopic is empty), JSON Transformer will report an error. Message writers will store the message(s) in the table/collection/measurement (depending on the underlying database) with the name of the format (which in the example is myFormat ). Mainflux writers will try to save any format received (whether it will be successful depends on the writer implementation and the underlying database), but it's recommended that publishers don't send different formats to the same subtopic.","title":"Transformer config"},{"location":"storage/#influxdb-influxdb-writer-and-grafana","text":"From the project root execute the following command: docker-compose -f docker/addons/influxdb-writer/docker-compose.yml up -d This will install and start: InfluxDB - time series database InfluxDB writer - message repository implementation for InfluxDB Grafana - tool for database exploration and data visualization and analytics Those new services will take some additional ports: 8086 by InfluxDB 8900 by InfluxDB writer service 3001 by Grafana To access Grafana, navigate to http://localhost:3001 and login with: admin , password: admin","title":"InfluxDB, InfluxDB Writer and Grafana"},{"location":"storage/#cassandra-and-cassandra-writer","text":"./docker/addons/cassandra-writer/init.sh Please note that Cassandra may not be suitable for your testing environment because of its high system requirements.","title":"Cassandra and Cassandra Writer"},{"location":"storage/#mongodb-and-mongodb-writer","text":"docker-compose -f docker/addons/mongodb-writer/docker-compose.yml up -d MongoDB default port (27017) is exposed, so you can use various tools for database inspection and data visualization.","title":"MongoDB and MongoDB Writer"},{"location":"storage/#postgresql-and-postgresql-writer","text":"docker-compose -f docker/addons/postgres-writer/docker-compose.yml up -d Postgres default port (5432) is exposed, so you can use various tools for database inspection and data visualization.","title":"PostgreSQL and PostgreSQL Writer"},{"location":"storage/#readers","text":"Readers provide an implementation of various message readers . Message readers are services that consume normalized (in SenML format) Mainflux messages from data storage and opens HTTP API for message consumption. Installing corresponding writer before reader is implied. Each of the Reader services exposes the same HTTP API for fetching messages on its default port. To read sent messages on channel with id channel_id you should send GET request to /channels/<channel_id>/messages with thing access token in Authorization header. That thing must be connected to channel with channel_id Response should look like this: HTTP/1.1 200 OK Content-Type: application/json Date: Tue, 18 Sep 2018 18:56:19 GMT Content-Length: 228 { \"messages\": [ { \"Channel\": 1, \"Publisher\": 2, \"Protocol\": \"mqtt\", \"Name\": \"name:voltage\", \"Unit\": \"V\", \"Value\": 5.6, \"Time\": 48.56 }, { \"Channel\": 1, \"Publisher\": 2, \"Protocol\": \"mqtt\", \"Name\": \"name:temperature\", \"Unit\": \"C\", \"Value\": 24.3, \"Time\": 48.56 } ] } Note that you will receive only those messages that were sent by authorization token's owner. You can specify offset and limit parameters in order to fetch specific group of messages. An example of HTTP request looks like: curl -s -S -i -H \"Authorization: Thing <thing_key>\" http://localhost:<service_port>/channels/<channel_id>/messages?offset=0&limit=5&format=<subtopic> If you don't provide offset and limit parameters, default values will be used instead: 0 for offset and 10 for limit . The format parameter indicates the last subtopic of the message. As indicated under the Writers section, the message format is stored in the subtopic as the last part of the subtopic. In the example: http://localhost:8185/channels/<channelID>/messages/home/temperature/myFormat the message format is myFormat and the value for format=<subtopic> is format=myFormat .","title":"Readers"},{"location":"storage/#influxdb-reader","text":"To start InfluxDB reader, execute the following command: docker-compose -f docker/addons/influxdb-reader/docker-compose.yml up -d","title":"InfluxDB Reader"},{"location":"storage/#cassandra-reader","text":"To start Cassandra reader, execute the following command: docker-compose -f docker/addons/cassandra-reader/docker-compose.yml up -d","title":"Cassandra Reader"},{"location":"storage/#mongodb-reader","text":"To start MongoDB reader, execute the following command: docker-compose -f docker/addons/mongodb-reader/docker-compose.yml up -d","title":"MongoDB Reader"},{"location":"storage/#postgresql-reader","text":"To start PostgreSQL reader, execute the following command: docker-compose -f docker/addons/postgres-reader/docker-compose.yml up -d","title":"PostgreSQL Reader"},{"location":"tracing/","text":"Tracing # Distributed tracing is a method of profiling and monitoring applications. It can provide valuable insight when optimizing and debugging an application. Mainflux includes the Jaeger open tracing framework as a service with its stack by default. Launch # The Jaeger service will launch with the rest of the Mainflux services. All services can be launched using: make run The Jaeger UI can then be accessed at http://localhost:16686 from a browser. Details about the UI can be found in Jaeger's official documentation . Configure # The Jaeger service can be disabled by using the scale flag with docker-compose up and setting the jaeger container to 0. --scale jaeger=0 make rungw will run all of the Mainflux services except for the Jaeger service. This is currently the only difference from make run . The make rungw command runs Mainflux for gateway devices. There could potentially be more differences running with this command in the future. Jaeger uses 5 ports within the Mainflux framework. These ports can be edited in the .env file. Variable Description Default MF_JAEGER_PORT Agent port for compact jaeger.thrift protocol 6831 MF_JAEGER_FRONTEND UI port 16686 MF_JAEGER_COLLECTOR Collector for jaeger.thrift directly from clients 14268 MF_JAEGER_CONFIGS Configuration server 5778 MF_JAEGER_URL Jaeger access from within Mainflux jaeger:6831 Example # As an example for using Jaeger, we can look at the traces generated after provisioning the system. Make sure to have ran the provisioning script that is part of the Getting Started step. Before getting started with Jaeger, there are a few terms that are important to define. A trace can be thought of as one transaction within the system. A trace is made up of one or more spans . These are the individual steps that must be taken for a trace to perform its action. A span has tags and logs associated with it. Tags are key-value pairs that provide information such as a database type or http method. Tags are useful when filtering traces in the Jaeger UI. Logs are structured messages used at specific points in the trace's transaction. These are typically used to indicate an error. When first navigating to the Jaeger UI, it will present a search page with an empty results section. There are multiple fields to search from including service, operation, tags and time frames. Clicking Find Traces will fill the results section with traces containing the selected fields. The top of the results page includes a scatter plot of the traces and their durations. This can be very useful for finding a trace with a prolonged runtime. Clicking on one of the points will open the trace page of that trace. Below the graph is a list of all the traces with a summary of its information. Each trace shows a unique identifier, the overall runtime, the spans it is composed of and when it was ran. Clicking on one of the traces will open the trace page of that trace. The trace page provides a more detailed breakdown of the individual span calls. The top of the page shows a chart breaking down what spans the trace is spending its time in. Below the chart are the individual spans and their details. Expanding the spans shows any tags associated with that span and process information. This is also where any errors or logs seen while running the span will be reported. This is just a brief overview of the possibilities of Jaeger and its UI. For more information, check out Jaeger's official documentation .","title":"Tracing"},{"location":"tracing/#tracing","text":"Distributed tracing is a method of profiling and monitoring applications. It can provide valuable insight when optimizing and debugging an application. Mainflux includes the Jaeger open tracing framework as a service with its stack by default.","title":"Tracing"},{"location":"tracing/#launch","text":"The Jaeger service will launch with the rest of the Mainflux services. All services can be launched using: make run The Jaeger UI can then be accessed at http://localhost:16686 from a browser. Details about the UI can be found in Jaeger's official documentation .","title":"Launch"},{"location":"tracing/#configure","text":"The Jaeger service can be disabled by using the scale flag with docker-compose up and setting the jaeger container to 0. --scale jaeger=0 make rungw will run all of the Mainflux services except for the Jaeger service. This is currently the only difference from make run . The make rungw command runs Mainflux for gateway devices. There could potentially be more differences running with this command in the future. Jaeger uses 5 ports within the Mainflux framework. These ports can be edited in the .env file. Variable Description Default MF_JAEGER_PORT Agent port for compact jaeger.thrift protocol 6831 MF_JAEGER_FRONTEND UI port 16686 MF_JAEGER_COLLECTOR Collector for jaeger.thrift directly from clients 14268 MF_JAEGER_CONFIGS Configuration server 5778 MF_JAEGER_URL Jaeger access from within Mainflux jaeger:6831","title":"Configure"},{"location":"tracing/#example","text":"As an example for using Jaeger, we can look at the traces generated after provisioning the system. Make sure to have ran the provisioning script that is part of the Getting Started step. Before getting started with Jaeger, there are a few terms that are important to define. A trace can be thought of as one transaction within the system. A trace is made up of one or more spans . These are the individual steps that must be taken for a trace to perform its action. A span has tags and logs associated with it. Tags are key-value pairs that provide information such as a database type or http method. Tags are useful when filtering traces in the Jaeger UI. Logs are structured messages used at specific points in the trace's transaction. These are typically used to indicate an error. When first navigating to the Jaeger UI, it will present a search page with an empty results section. There are multiple fields to search from including service, operation, tags and time frames. Clicking Find Traces will fill the results section with traces containing the selected fields. The top of the results page includes a scatter plot of the traces and their durations. This can be very useful for finding a trace with a prolonged runtime. Clicking on one of the points will open the trace page of that trace. Below the graph is a list of all the traces with a summary of its information. Each trace shows a unique identifier, the overall runtime, the spans it is composed of and when it was ran. Clicking on one of the traces will open the trace page of that trace. The trace page provides a more detailed breakdown of the individual span calls. The top of the page shows a chart breaking down what spans the trace is spending its time in. Below the chart are the individual spans and their details. Expanding the spans shows any tags associated with that span and process information. This is also where any errors or logs seen while running the span will be reported. This is just a brief overview of the possibilities of Jaeger and its UI. For more information, check out Jaeger's official documentation .","title":"Example"},{"location":"twins/","text":"Twins Service # Mainflux twins service is built on top of the Mainflux platform. In order to fully understand what follows, be sure to get acquainted with overall Mainflux architecture . What is Digital Twin # Twin refers to a digital representation of a real world data system consisting of possibly multiple data sources/producers and/or destinations/consumers (data agents). For example, an industrial machine can use multiple protocols such as MQTT, OPC-UA, a regularly updated machine hosted CSV file etc. to send measurement data - such as flowrate, material temperature, etc. - and state metadata - such as engine and chassis temperature, engine rotations per seconds, identity of the current human operator, etc. - as well as to receive control, i.e. actuation messages - such as, turn on/off light, increment/decrement borer speed, change blades, etc. Digital twin is an abstract - and usually less detailed - digital replica of a real world system such as the industrial machine we have just described. It is used to create and store information about system's state at any given moment, to compare system state over a given period of time - so-called diffs or deltas - as well as to control agents composing the system. Mainflux Digital Twin # Any data producer or data consumer - which we refer to here collectively as data agent - or an interrelated system of data agents, can be represented by means of possibly multiple Mainflux things, channels and subtopics . For example, an OPC-UA server can be represented as a Mainflux thing and its nodes can be represented as multiple Mainflux channels or multiple subtopics of a single Mainflux channel. What is more, you can invert the representation: you can represent server as a channel and node as things. Mainflux platform is meant to empower you with the freedom of expression so you can make a digital representation of any data agent according to your needs. Although this works well, satisfies the requirements of a wide variety of use cases and corresponds to the intended use of Mainlfux IoT platform, this setup can be insufficient in two important ways. Firstly, different things, channels, and their connections - i.e. Mainflux representations of different data agent structures - are unrelated to each other, i.e. they do not form a meaningful whole and, as a consequence, they do not represent a single unified system . Secondly, the semantic aspect, i.e. the meaning of different things and channels is not transparent and defined by the sole use of Mainflux platform entities (channels and things). Certainly, we can try to describe things and channels connections and relations as well as their meaning - i.e. their role, position, function in the overall system - by means of their metadata . Although this might work well - with a proviso of a lot of additional effort of writing the relatively complex code to create and parse metadata - it is not a practical approach and we still don't get - at least not out of the box - a readable and useful overview of the system as a whole. Also, this approach does not enable us to answer a simple but very important question, i.e. what was the detailed state of a complete system at a certain moment in time. To overcome these problems, Mainflux comes with a digital twin service . The twins service is built on top of the Mainflux platform and relies on its architecture and entities, more precisely, on Mainflux users, things and channels. The primary task of the twin service is to handle Mainflux digital twins. Mainflux digital twin consists of three parts: General data about twin itself, i.e. twin's metadata , History of twin's definitions , including current definition, History of twin's states , including current state. Mainflux Platform and Mainflux Twins Service # Mainflux Twins service depends on the Mainflux IoT platform. The following diagram shows the place of the twins service in the overall Mainflux architecture : You use an HTTP client to communicate with the twins service. Every request sent to the twins service is authenticated by users service. Twins service handles CRUD requests and creates, retrieves, updates and deletes twins. The CRUD operations depend on the database to persist and fetch already saved twins. Twins service listens to NATS server and intercepts messages passing via NATS broker. Every Mainflux message contains information about subchannel and topic used to send a message. Twins service compares this info with attribute definitions of twins persisted in the database, fetches the corresponding twins and updates their respective states. Before we dwell into twin's anatomy, it is important to realize that in order to use Mainflux twin service, you have to provision Mainflux things and channels and you have to connect things and channels beforehand. As you go, you can modify your things, channels and connections and you can modify your digital twin to reflect these modifications, but you have to have at least a minimal setup in order to use the twin service. Twin's Anatomy # Twin's general information stores twin's owner email - owner is represented by Mainflux user -, twin's ID (unique) and name (not necessarily unique), twin's creation and update dates as well as twin's revision number. The latter refers to the sequential number of twin's definition. The twin's definition is meant to be a semantic representation of system's data sources and consumers (data agents). Each data data agent is represented by means of attribute . Attribute consists of data agent's name, Mainflux channel and subtopic over which it communicates. Nota bene: each attribute is uniquely defined by the combination of channel and subtopic and we cannot have two or more attributes with the same channel and subtopic in the same definition. Attributes have a state persistence flag that determines whether the messages communicated by its corresponding channel and subtopic trigger the creation of a new twin state. Twin states are persisted in the separate collection of the same database. Currently, twins service uses the MongoDB. InfluxDB support for twins and states persistence is on the roadmap. When we define our digital twin, its JSON representation might look like this: { \"owner\": \"john.doe@email.net\", \"id\": \"a838e608-1c1b-4fea-9c34-def877473a89\", \"name\": \"grinding machine 2\", \"revision\": 2, \"created\": \"2020-05-05T08:41:39.142Z\", \"updated\": \"2020-05-05T08:49:12.638Z\", \"definitions\": [ { \"id\": 0, \"created\": \"2020-05-05T08:41:39.142Z\", \"attributes\": [], \"delta\": 1000000 }, { \"id\": 1, \"created\": \"2020-05-05T08:46:23.207Z\", \"attributes\": [ { \"name\": \"engine temperature\", \"channel\": \"7ef6c61c-f514-402f-af4b-2401b588bfec\", \"subtopic\": \"engine\", \"persist_state\": true }, { \"name\": \"chassis temperature\", \"channel\": \"7ef6c61c-f514-402f-af4b-2401b588bfec\", \"subtopic\": \"chassis\", \"persist_state\": true }, { \"name\": \"rotations per sec\", \"channel\": \"a254032a-8bb6-4973-a2a1-dbf80f181a86\", \"subtopic\": \"\", \"persist_state\": false } ], \"delta\": 1000000 }, { \"id\": 2, \"created\": \"2020-05-05T08:49:12.638Z\", \"attributes\": [ { \"name\": \"engine temperature\", \"channel\": \"7ef6c61c-f514-402f-af4b-2401b588bfec\", \"subtopic\": \"engine\", \"persist_state\": true }, { \"name\": \"chassis temperature\", \"channel\": \"7ef6c61c-f514-402f-af4b-2401b588bfec\", \"subtopic\": \"chassis\", \"persist_state\": true }, { \"name\": \"rotations per sec\", \"channel\": \"a254032a-8bb6-4973-a2a1-dbf80f181a86\", \"subtopic\": \"\", \"persist_state\": false }, { \"name\": \"precision\", \"channel\": \"aed0fbca-0d1d-4b07-834c-c62f31526569\", \"subtopic\": \"\", \"persist_state\": true } ], \"delta\": 1000000 } ] } In the case of the upper twin, we begin with an empty definition, the one with the id 0 - we could have provided the definition immediately - and over the course of time, we add two more definitions, so the total number of revisions is 2 (revision index is zero-based). We decide not to persist the number of rotation per second in our digital twin state. We define it, though, because the definition and its attributes are used not only to define states of a complex data agent system, but also to define the semantic structure of the system. delta is the number of nanoseconds used to determine whether the received attribute value should trigger the generation of the new state or the same state should be updated. The reason for this is to enable state sampling over the regular intervals of time. Discarded values are written to the database of choice by Mainflux writers , so you can always retrieve intermediate values if need be. states are created according to the twin's current definition. A state stores twin's ID - every state belongs to a single twin -, its own ID, twin's definition number, creation date and the actual payload. Payload is a set of key-value pairs where a key corresponds to the attribute name and a value is the actual value of the attribute. All SenML value types are supported. A JSON representation of a partial list of states might look like this: { \"total\": 28, \"offset\": 10, \"limit\": 5, \"states\": [ { \"twin_id\": \"a838e608-1c1b-4fea-9c34-def877473a89\", \"id\": 11, \"definition\": 1, \"created\": \"2020-05-05T08:49:06.167Z\", \"payload\": { \"chassis temperature\": 0.3394171011161684, \"engine temperature\": 0.3814079472715233 } }, { \"twin_id\": \"a838e608-1c1b-4fea-9c34-def877473a89\", \"id\": 12, \"definition\": 1, \"created\": \"2020-05-05T08:49:12.168Z\", \"payload\": { \"chassis temperature\": 1.8116442194724147, \"engine temperature\": 0.3814079472715233 } }, { \"twin_id\": \"a838e608-1c1b-4fea-9c34-def877473a89\", \"id\": 13, \"definition\": 2, \"created\": \"2020-05-05T08:49:18.174Z\", \"payload\": { \"chassis temperature\": 1.8116442194724147, \"engine temperature\": 3.2410616702795814 } }, { \"twin_id\": \"a838e608-1c1b-4fea-9c34-def877473a89\", \"id\": 14, \"definition\": 2, \"created\": \"2020-05-05T08:49:19.145Z\", \"payload\": { \"chassis temperature\": 3.2410616702795814, \"engine temperature\": 3.2410616702795814, \"precision\": 8.922156489392854 } }, { \"twin_id\": \"a838e608-1c1b-4fea-9c34-def877473a89\", \"id\": 15, \"definition\": 2, \"created\": \"2020-05-05T08:49:24.178Z\", \"payload\": { \"chassis temperature\": 0.8694383878692546, \"engine temperature\": 3.2410616702795814, \"precision\": 8.922156489392854 } } ] } As you can see, the first two states correspond to the definition 1 and have only two attributes in the payload. The rest of the states is based on the definition 2 , where we persist three attributes and, as a consequence, its payload consists of three entries. Authentication and Authorization # Twin belongs to a Mainflux user, tenant representing a physical person or an organization. User owns Mainflux things and channels as well as twins. Mainflux user provides authorization and authentication mechanisms to twins service. For more details, please see Authentication with Mainflux keys . In practical terms, we need to create a Mainflux user in order to create a digital twin. Every twin belongs to exactly one user. One user can have unlimited number of digital twins. Twin Operations # For more information about the Twins service HTTP API please refer to the twins service OpenAPI file . Create and Update # Create and update requests use JSON body to initialize and modify, respectively, twin. You can omit every piece of data - every key-value pair - from the JSON. However, you must send at least an empty JSON body. { \"name\": \"twin_name\", \"definition\": { \"attributes\": [ { \"name\": \"temperature\", \"channel\": \"3b57b952-318e-47b5-b0d7-a14f61ecd03b\", \"subtopic\": \"temperature\", \"persist_state\": true }, { \"name\": \"humidity\", \"channel\": \"3b57b952-318e-47b5-b0d7-a14f61ecd03b\", \"subtopic\": \"humidity\", \"persist_state\": false }, { \"name\": \"pressure\", \"channel\": \"7ef6c61c-f514-402f-af4b-2401b588bfec\", \"subtopic\": \"\", \"persist_state\": true } ], \"delta\": 1 } } Create # Create request uses POST HTTP method to create twin: curl -s -S -i -X POST -H \"Content-Type: application/json\" -H \"Authorization: Bearer <user_token>\" http://localhost:8191/twins -d '<twin_data>' If you do not suply the definition, the empty definition of the form { \"id\": 0, \"created\": \"2020-05-05T08:41:39.142Z\", \"attributes\": [], \"delta\": 1000000 } will be created. Update # curl -s -S -i -X PUT -H \"Content-Type: application/json\" -H \"Authorization: Bearer <user_token>\" http://localhost:8191/<twin_id> -d '<twin_data>' View # curl -s -S -i -X GET -H \"Authorization: Bearer <user_token>\" http://localhost:8191/twins/<twin_id> List # curl -s -S -i -X GET -H \"Authorization: Bearer <user_token>\" http://localhost:8191/twins List requests accepts limit and offset query parameters. By default, i.e. without these parameters, list requests fetches only first ten twins (or less, if there are less then ten twins). You can fetch twins [10-29) like this: curl -s -S -i -X GET -H \"Authorization: Bearer <user_token>\" http://localhost:8191/twins?offset=10&limit=20 Delete # curl -s -S -i -X DELETE -H \"Authorization: Bearer <user_token>\" http://localhost:8191/twins/<twin_id> STATES operations # List # curl -s -S -i -X GET -H \"Authorization: Bearer <user_token>\" http://localhost:8191/states/<twin_id> List requests accepts limit and offset query parameters. By default, i.e. without these parameters, list requests fetches only first ten states (or less, if there are less then ten states). You can fetch states [10-29) like this: curl -s -S -i -X GET -H \"Authorization: Bearer <user_token>\" http://localhost:8191/states/<twin_id>?offset=10&limit=20 Notifications # Every twin and states related operation publishes notifications via NATS. To fully understand what follows, please read about Mainflux messaging capabilities and utilities. In order to pick up this notifications, you have to create a Mainflux channel before you start the twins service and inform the twins service about the channel by means of the environment variable, like this: export MF_TWINS_CHANNEL_ID=f6894dfe-a7c9-4eef-a614-637ebeea5b4c The twins service will use this channel to publish notifications related to twins creation, update, retrieval and deletion. It will also publish notifications related to state saving into the database. All notifications will be published on the following NATS subject: channels.<mf_twins_channel_id>.<optional_subtopic> where <optional_subtopic> is one of the following: create.success - on successful twin creation, create.failure - on twin creation failure, update.success - on successful twin update, update.failure - on twin update failure, get.success - on successful twin retrieval, get.failure - on twin retrieval failure, remove.success - on successful twin deletion, remove.failure - on twin deletion failure, save.success - on successful state save save.failure - on state save failure. Normally, you can use NATS wildcards. In order to learn more about Mainflux channel topic composition, please read about subtopics . The point is to be able to subscribe to all subjects or any operation pair subject - e.g. create.success/failure - by means of one connection and read all messages or all operation related messages in the context of the same subscription. Since messages published on NATS are republished on any other protocol supported by Mainflux - HTTP, MQTT, CoAP and WS - you can use any supported protocol client to pick up notifications.","title":"Twins"},{"location":"twins/#twins-service","text":"Mainflux twins service is built on top of the Mainflux platform. In order to fully understand what follows, be sure to get acquainted with overall Mainflux architecture .","title":"Twins Service"},{"location":"twins/#what-is-digital-twin","text":"Twin refers to a digital representation of a real world data system consisting of possibly multiple data sources/producers and/or destinations/consumers (data agents). For example, an industrial machine can use multiple protocols such as MQTT, OPC-UA, a regularly updated machine hosted CSV file etc. to send measurement data - such as flowrate, material temperature, etc. - and state metadata - such as engine and chassis temperature, engine rotations per seconds, identity of the current human operator, etc. - as well as to receive control, i.e. actuation messages - such as, turn on/off light, increment/decrement borer speed, change blades, etc. Digital twin is an abstract - and usually less detailed - digital replica of a real world system such as the industrial machine we have just described. It is used to create and store information about system's state at any given moment, to compare system state over a given period of time - so-called diffs or deltas - as well as to control agents composing the system.","title":"What is Digital Twin"},{"location":"twins/#mainflux-digital-twin","text":"Any data producer or data consumer - which we refer to here collectively as data agent - or an interrelated system of data agents, can be represented by means of possibly multiple Mainflux things, channels and subtopics . For example, an OPC-UA server can be represented as a Mainflux thing and its nodes can be represented as multiple Mainflux channels or multiple subtopics of a single Mainflux channel. What is more, you can invert the representation: you can represent server as a channel and node as things. Mainflux platform is meant to empower you with the freedom of expression so you can make a digital representation of any data agent according to your needs. Although this works well, satisfies the requirements of a wide variety of use cases and corresponds to the intended use of Mainlfux IoT platform, this setup can be insufficient in two important ways. Firstly, different things, channels, and their connections - i.e. Mainflux representations of different data agent structures - are unrelated to each other, i.e. they do not form a meaningful whole and, as a consequence, they do not represent a single unified system . Secondly, the semantic aspect, i.e. the meaning of different things and channels is not transparent and defined by the sole use of Mainflux platform entities (channels and things). Certainly, we can try to describe things and channels connections and relations as well as their meaning - i.e. their role, position, function in the overall system - by means of their metadata . Although this might work well - with a proviso of a lot of additional effort of writing the relatively complex code to create and parse metadata - it is not a practical approach and we still don't get - at least not out of the box - a readable and useful overview of the system as a whole. Also, this approach does not enable us to answer a simple but very important question, i.e. what was the detailed state of a complete system at a certain moment in time. To overcome these problems, Mainflux comes with a digital twin service . The twins service is built on top of the Mainflux platform and relies on its architecture and entities, more precisely, on Mainflux users, things and channels. The primary task of the twin service is to handle Mainflux digital twins. Mainflux digital twin consists of three parts: General data about twin itself, i.e. twin's metadata , History of twin's definitions , including current definition, History of twin's states , including current state.","title":"Mainflux Digital Twin"},{"location":"twins/#mainflux-platform-and-mainflux-twins-service","text":"Mainflux Twins service depends on the Mainflux IoT platform. The following diagram shows the place of the twins service in the overall Mainflux architecture : You use an HTTP client to communicate with the twins service. Every request sent to the twins service is authenticated by users service. Twins service handles CRUD requests and creates, retrieves, updates and deletes twins. The CRUD operations depend on the database to persist and fetch already saved twins. Twins service listens to NATS server and intercepts messages passing via NATS broker. Every Mainflux message contains information about subchannel and topic used to send a message. Twins service compares this info with attribute definitions of twins persisted in the database, fetches the corresponding twins and updates their respective states. Before we dwell into twin's anatomy, it is important to realize that in order to use Mainflux twin service, you have to provision Mainflux things and channels and you have to connect things and channels beforehand. As you go, you can modify your things, channels and connections and you can modify your digital twin to reflect these modifications, but you have to have at least a minimal setup in order to use the twin service.","title":"Mainflux Platform and Mainflux Twins Service"},{"location":"twins/#twins-anatomy","text":"Twin's general information stores twin's owner email - owner is represented by Mainflux user -, twin's ID (unique) and name (not necessarily unique), twin's creation and update dates as well as twin's revision number. The latter refers to the sequential number of twin's definition. The twin's definition is meant to be a semantic representation of system's data sources and consumers (data agents). Each data data agent is represented by means of attribute . Attribute consists of data agent's name, Mainflux channel and subtopic over which it communicates. Nota bene: each attribute is uniquely defined by the combination of channel and subtopic and we cannot have two or more attributes with the same channel and subtopic in the same definition. Attributes have a state persistence flag that determines whether the messages communicated by its corresponding channel and subtopic trigger the creation of a new twin state. Twin states are persisted in the separate collection of the same database. Currently, twins service uses the MongoDB. InfluxDB support for twins and states persistence is on the roadmap. When we define our digital twin, its JSON representation might look like this: { \"owner\": \"john.doe@email.net\", \"id\": \"a838e608-1c1b-4fea-9c34-def877473a89\", \"name\": \"grinding machine 2\", \"revision\": 2, \"created\": \"2020-05-05T08:41:39.142Z\", \"updated\": \"2020-05-05T08:49:12.638Z\", \"definitions\": [ { \"id\": 0, \"created\": \"2020-05-05T08:41:39.142Z\", \"attributes\": [], \"delta\": 1000000 }, { \"id\": 1, \"created\": \"2020-05-05T08:46:23.207Z\", \"attributes\": [ { \"name\": \"engine temperature\", \"channel\": \"7ef6c61c-f514-402f-af4b-2401b588bfec\", \"subtopic\": \"engine\", \"persist_state\": true }, { \"name\": \"chassis temperature\", \"channel\": \"7ef6c61c-f514-402f-af4b-2401b588bfec\", \"subtopic\": \"chassis\", \"persist_state\": true }, { \"name\": \"rotations per sec\", \"channel\": \"a254032a-8bb6-4973-a2a1-dbf80f181a86\", \"subtopic\": \"\", \"persist_state\": false } ], \"delta\": 1000000 }, { \"id\": 2, \"created\": \"2020-05-05T08:49:12.638Z\", \"attributes\": [ { \"name\": \"engine temperature\", \"channel\": \"7ef6c61c-f514-402f-af4b-2401b588bfec\", \"subtopic\": \"engine\", \"persist_state\": true }, { \"name\": \"chassis temperature\", \"channel\": \"7ef6c61c-f514-402f-af4b-2401b588bfec\", \"subtopic\": \"chassis\", \"persist_state\": true }, { \"name\": \"rotations per sec\", \"channel\": \"a254032a-8bb6-4973-a2a1-dbf80f181a86\", \"subtopic\": \"\", \"persist_state\": false }, { \"name\": \"precision\", \"channel\": \"aed0fbca-0d1d-4b07-834c-c62f31526569\", \"subtopic\": \"\", \"persist_state\": true } ], \"delta\": 1000000 } ] } In the case of the upper twin, we begin with an empty definition, the one with the id 0 - we could have provided the definition immediately - and over the course of time, we add two more definitions, so the total number of revisions is 2 (revision index is zero-based). We decide not to persist the number of rotation per second in our digital twin state. We define it, though, because the definition and its attributes are used not only to define states of a complex data agent system, but also to define the semantic structure of the system. delta is the number of nanoseconds used to determine whether the received attribute value should trigger the generation of the new state or the same state should be updated. The reason for this is to enable state sampling over the regular intervals of time. Discarded values are written to the database of choice by Mainflux writers , so you can always retrieve intermediate values if need be. states are created according to the twin's current definition. A state stores twin's ID - every state belongs to a single twin -, its own ID, twin's definition number, creation date and the actual payload. Payload is a set of key-value pairs where a key corresponds to the attribute name and a value is the actual value of the attribute. All SenML value types are supported. A JSON representation of a partial list of states might look like this: { \"total\": 28, \"offset\": 10, \"limit\": 5, \"states\": [ { \"twin_id\": \"a838e608-1c1b-4fea-9c34-def877473a89\", \"id\": 11, \"definition\": 1, \"created\": \"2020-05-05T08:49:06.167Z\", \"payload\": { \"chassis temperature\": 0.3394171011161684, \"engine temperature\": 0.3814079472715233 } }, { \"twin_id\": \"a838e608-1c1b-4fea-9c34-def877473a89\", \"id\": 12, \"definition\": 1, \"created\": \"2020-05-05T08:49:12.168Z\", \"payload\": { \"chassis temperature\": 1.8116442194724147, \"engine temperature\": 0.3814079472715233 } }, { \"twin_id\": \"a838e608-1c1b-4fea-9c34-def877473a89\", \"id\": 13, \"definition\": 2, \"created\": \"2020-05-05T08:49:18.174Z\", \"payload\": { \"chassis temperature\": 1.8116442194724147, \"engine temperature\": 3.2410616702795814 } }, { \"twin_id\": \"a838e608-1c1b-4fea-9c34-def877473a89\", \"id\": 14, \"definition\": 2, \"created\": \"2020-05-05T08:49:19.145Z\", \"payload\": { \"chassis temperature\": 3.2410616702795814, \"engine temperature\": 3.2410616702795814, \"precision\": 8.922156489392854 } }, { \"twin_id\": \"a838e608-1c1b-4fea-9c34-def877473a89\", \"id\": 15, \"definition\": 2, \"created\": \"2020-05-05T08:49:24.178Z\", \"payload\": { \"chassis temperature\": 0.8694383878692546, \"engine temperature\": 3.2410616702795814, \"precision\": 8.922156489392854 } } ] } As you can see, the first two states correspond to the definition 1 and have only two attributes in the payload. The rest of the states is based on the definition 2 , where we persist three attributes and, as a consequence, its payload consists of three entries.","title":"Twin's Anatomy"},{"location":"twins/#authentication-and-authorization","text":"Twin belongs to a Mainflux user, tenant representing a physical person or an organization. User owns Mainflux things and channels as well as twins. Mainflux user provides authorization and authentication mechanisms to twins service. For more details, please see Authentication with Mainflux keys . In practical terms, we need to create a Mainflux user in order to create a digital twin. Every twin belongs to exactly one user. One user can have unlimited number of digital twins.","title":"Authentication and Authorization"},{"location":"twins/#twin-operations","text":"For more information about the Twins service HTTP API please refer to the twins service OpenAPI file .","title":"Twin Operations"},{"location":"twins/#create-and-update","text":"Create and update requests use JSON body to initialize and modify, respectively, twin. You can omit every piece of data - every key-value pair - from the JSON. However, you must send at least an empty JSON body. { \"name\": \"twin_name\", \"definition\": { \"attributes\": [ { \"name\": \"temperature\", \"channel\": \"3b57b952-318e-47b5-b0d7-a14f61ecd03b\", \"subtopic\": \"temperature\", \"persist_state\": true }, { \"name\": \"humidity\", \"channel\": \"3b57b952-318e-47b5-b0d7-a14f61ecd03b\", \"subtopic\": \"humidity\", \"persist_state\": false }, { \"name\": \"pressure\", \"channel\": \"7ef6c61c-f514-402f-af4b-2401b588bfec\", \"subtopic\": \"\", \"persist_state\": true } ], \"delta\": 1 } }","title":"Create and Update"},{"location":"twins/#create","text":"Create request uses POST HTTP method to create twin: curl -s -S -i -X POST -H \"Content-Type: application/json\" -H \"Authorization: Bearer <user_token>\" http://localhost:8191/twins -d '<twin_data>' If you do not suply the definition, the empty definition of the form { \"id\": 0, \"created\": \"2020-05-05T08:41:39.142Z\", \"attributes\": [], \"delta\": 1000000 } will be created.","title":"Create"},{"location":"twins/#update","text":"curl -s -S -i -X PUT -H \"Content-Type: application/json\" -H \"Authorization: Bearer <user_token>\" http://localhost:8191/<twin_id> -d '<twin_data>'","title":"Update"},{"location":"twins/#view","text":"curl -s -S -i -X GET -H \"Authorization: Bearer <user_token>\" http://localhost:8191/twins/<twin_id>","title":"View"},{"location":"twins/#list","text":"curl -s -S -i -X GET -H \"Authorization: Bearer <user_token>\" http://localhost:8191/twins List requests accepts limit and offset query parameters. By default, i.e. without these parameters, list requests fetches only first ten twins (or less, if there are less then ten twins). You can fetch twins [10-29) like this: curl -s -S -i -X GET -H \"Authorization: Bearer <user_token>\" http://localhost:8191/twins?offset=10&limit=20","title":"List"},{"location":"twins/#delete","text":"curl -s -S -i -X DELETE -H \"Authorization: Bearer <user_token>\" http://localhost:8191/twins/<twin_id>","title":"Delete"},{"location":"twins/#states-operations","text":"","title":"STATES operations"},{"location":"twins/#list_1","text":"curl -s -S -i -X GET -H \"Authorization: Bearer <user_token>\" http://localhost:8191/states/<twin_id> List requests accepts limit and offset query parameters. By default, i.e. without these parameters, list requests fetches only first ten states (or less, if there are less then ten states). You can fetch states [10-29) like this: curl -s -S -i -X GET -H \"Authorization: Bearer <user_token>\" http://localhost:8191/states/<twin_id>?offset=10&limit=20","title":"List"},{"location":"twins/#notifications","text":"Every twin and states related operation publishes notifications via NATS. To fully understand what follows, please read about Mainflux messaging capabilities and utilities. In order to pick up this notifications, you have to create a Mainflux channel before you start the twins service and inform the twins service about the channel by means of the environment variable, like this: export MF_TWINS_CHANNEL_ID=f6894dfe-a7c9-4eef-a614-637ebeea5b4c The twins service will use this channel to publish notifications related to twins creation, update, retrieval and deletion. It will also publish notifications related to state saving into the database. All notifications will be published on the following NATS subject: channels.<mf_twins_channel_id>.<optional_subtopic> where <optional_subtopic> is one of the following: create.success - on successful twin creation, create.failure - on twin creation failure, update.success - on successful twin update, update.failure - on twin update failure, get.success - on successful twin retrieval, get.failure - on twin retrieval failure, remove.success - on successful twin deletion, remove.failure - on twin deletion failure, save.success - on successful state save save.failure - on state save failure. Normally, you can use NATS wildcards. In order to learn more about Mainflux channel topic composition, please read about subtopics . The point is to be able to subscribe to all subjects or any operation pair subject - e.g. create.success/failure - by means of one connection and read all messages or all operation related messages in the context of the same subscription. Since messages published on NATS are republished on any other protocol supported by Mainflux - HTTP, MQTT, CoAP and WS - you can use any supported protocol client to pick up notifications.","title":"Notifications"}]}